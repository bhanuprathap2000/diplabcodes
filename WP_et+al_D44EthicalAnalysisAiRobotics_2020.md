[[WP_et+al_D44EthicalAnalysisAiRobotics_2020]]

# [D4.4: Ethical Analysis of AI and Robotics Technologies](https://www.sienna-project.eu/digitalAssets/884/c_884668-l_1-k_d4.4_ethical-analysis--ai-and-r--with-acknowledgements.pdf)

## [[WP,: AI]]; [[robotics – Ethical, legal]]; [[social analysis]]

### 2020

## Abstract
This SIENNA deliverable offers a broad ethical analysis of artificial intelligence (AI) and robotics technologies. Its primary aims have been to comprehensively identify and analyse the present and potential future ethical issues in relation to: (1) the AI and robotics subfields, techniques, approaches and methods; (2) ==their physical technological products and procedures that are designed for practical applications==; and (3) the particular uses and applications of these products and procedures. In conducting the ethical analysis, we strove to provide ample clarification, details about nuances, and contextualisation of the ethical issues that were identified, while avoiding the making of moral judgments and proposing of solutions to these issues.
A secondary aim of this report has been to convey the results of SIENNA’s “country studies” of the national academic and popular media debate on the ethical issues in AI and robotics in twelve different EU and non-EU countries, highlighting the similarities and differences between these countries. ==While these country study results have only formed a minor contribution to the overall identification and analysis of the ethical issues== in this report, ==they are expected to play a larger role in future SIENNA deliverables==.

## Key concepts
#claim/artificial_intelligence; #STEM; #finding/robots; #claim/robotics; #method/ethical_issues; #claim/ethics; #method/natural_language_processing; #claim/autonomous_vehicles; #information_technology; #association_for_computing_machinery; #method/internet_of_things; #claim/knowledge_based_systems; #european_union; #method/unmanned_aerial_vehicles; #claim/artificial_general_intelligence; #claim/aerial_vehicle; #lethal_autonomous_weapons; #natural_language_generation; #emerging_technologies; #intelligent_virtual_assistants; #universal_basic_income; #international_association_for_computing_and_philosophy; #neural_machine_translation; #future_of_life_institute; #society_for_philosophy_and_technology; #autonomous_underwater_vehicles; #algorithmic_justice_league; #universal_basic_services; #national_institute_of_standards_and_technology; #return_on_investment; #international_humanitarian_law

## Quote
> We have summarised the content of the SIENNA report on ethical analysis of artificial intelligence and robotics

## Tables
|                                               Version                                               |           Date            |        Description        |     Reason for change     |  Distribution   |
|-----------------------------------------------------------------------------------------------------|---------------------------|---------------------------|---------------------------|-----------------|
| V0.9                                                                                                |August 1st, 2019           |Final draft report for     |-                          |August 1st, 2019 |
|                                                                                                     |external review            |                           |                           |                 |
| V1.0                                                                                                |August 31st, 2019          |Final report for           |Reviews and                |August 31st, 2019|
|                                                                                                     |submission to the EC       |comments                   |                           |                 |
| V1.1                                                                                                |June 17th, 2020            |Final report update        |Corrected                  |June 17th, 2020  |
|                                                                                                     |acknowledgement            |                           |                           |                 |
|                                                                                                     |p.23                       |                           |                           |                 |
|                                                                                                     |5.1. General ethical issues|6.1. Ethical issues with AI|7.1. Ethical issues with AI|                 |
|                                                                                                     |in AI technology           |products                   |applications               |                 |
|                                                                                                     |5.2. General ethical issues|6.2. Ethical issues with   |7.2. Ethical issues with   |                 |
|                                                                                                     |in robotics technology     |robotics products          |robotics applications      |                 |
|                                                                                                     |7.3. Ethical issues for    |                           |                           |                 |
|                                                                                                     |Ethical analysis sections  |different types of users   |                           |                 |
|Figure 1: Structure of the five substantive sections (3–7) of this report.                           |                           |                           |                           |                 |
|        Table 1: List of acronyms/abbreviations.                                                    |                           |                           |                           |                 |
|        Table 2: Glossary of terms.                                                                 |                           |                           |                           |                 |
|        Table 3: Overview of objects and central questions for ethical analysis of SIENNA’s approach|                           |                           |                           |                 |
|                                                                                                     |to ethical analysis.       |                           |                           |                 |
|        Table 4: Summarised findings per country on the country-specific academic debate on ethical |                           |                           |                           |                 |
|                                                                                                     |issues in AI.              |                           |                           |                 |
|        Table 5: Summarised findings per country on the country-specific academic debate on ethical |                           |                           |                           |                 |
|                                                                                                     |issues in robotics.        |                           |                           |                 |
|        Table 6: Summarised findings per country on the country-specific popular media debate on    |                           |                           |                           |                 |
|                                                                                                     |ethical issues in AI.      |                           |                           |                 |
|        Table 7: Summarised findings per country on the country-specific popular media debate on    |                           |                           |                           |                 |
|                                                                                                     |ethical issues in robotics.|                           |                           |                 |
|        Table 8: Overview of ethical issues with major types of AI products.                        |                           |                           |                           |                 |
|        Table 9: Overview of ethical issues with major types of robotics products.                  |                           |                           |                           |                 |
|        Table 10: Overview of ethical issues in major AI application domains.                       |                           |                           |                           |                 |
|        Table 11: Overview of ethical issues in major robotics application domains.                 |                           |                           |                           |                 |
|List of figures                                                                                      |                           |                           |                           |                 |
# Table 1: List of acronyms/abbreviations.
|Glossary of terms|                                  -                                  |                                    -                                    |
|-----------------|---------------------------------------------------------------------|-------------------------------------------------------------------------|
|                 |Term                                                                 |Explanation                                                              |
|                 |Actuator                                                             |A device module or subsystem for performing actions in an environment.   |
|                 |Algorithm                                                            |“[A] precisely-defined sequence of rules telling how to produce specified|
|                 |output information from given input information in a finite number of|                                                                         |
|                 |steps.”1                                                             |                                                                         |
# Table 2: Glossary of terms.
|                                                 -                                                  |     19     |
|----------------------------------------------------------------------------------------------------|------------|
|741716 \| SIENNA \| D4.4                                                                              |            |
|Deliverable report                                                                                  |            |
|1.                                                                                                  |Introduction|
|This SIENNA deliverable offers a broad ethical analysis of artificial intelligence (AI) and robotics|            |
# Table 3: Overview of objects and central questions for ethical analysis of SIENNA’s approach to ethical analysis.
|                                                    -                                                     |24 |
|----------------------------------------------------------------------------------------------------------|---|
|741716 \| SIENNA \| D4.4                                                                                    |   |
|Deliverable report                                                                                        |   |
|In this report, the three levels of ethical analysis are each covered in a separate section: section 5 for|   |
|the technology level, section 6 for the product level, and section 7 for the application level.           |   |
# Table 4: Summarised findings per country on the country-specific academic debate on ethical issues in AI.
|        4.2.2.        |                       Academic debate on ethical issues in robotics                        |
|----------------------|--------------------------------------------------------------------------------------------|
| Country              |Summarised findings                                                                         |
| Brazil               |About half of all articles found address issues related to the education of children. Most  |
|                      |of the articles found were master’s or PhD dissertations. It was noted that the articles    |
|                      |were generally of a fairly low quality, with many drawing trivial conclusions.              |
| China                |In China, there has been significant attention on the ethics of the development and use of  |
|                      |robotics. Some scholars focus on larger, more abstract and theoretical, themes related to   |
|                      |robotics in machine ethics and robot ethics. Others focus more on issues in specific robot  |
|                      |application areas, such as sex robots, medical robots, assistance robots, household robots, |
|                      |and autonomous vehicles. There is debate on ethical issues in relation to dignity, justice, |
|                      |safety, privacy, responsibility (especially the last three), and “harmonious relationships  |
|                      |between humans and machines”. As with research on the ethics of AI, policy implications     |
|                      |are often also focused on.                                                                  |
| France               |The academic debate in France has four strands: One is about the impact that robots will    |
|                      |have on work. A second one is about the nature and purpose of robots within our societies.  |
|                      |A third one is about the risks of using robots and the necessity to foresee a regulatory    |
|                      |framework, either internal (ethics) or external (laws). This last aspect raises the general |
|                      |question about dignity and the relation to our normative background that robots are         |
|                      |already modifying. This discussion is often implicit in the analyses, but it is the main    |
|                      |puzzling aspect that feeds the whole debate.                                                |
| Germany              |In Germany, there is substantial country-specific academic debate on ethical issues with    |
|                      |AI. The ethical issues in relation to AI and robotics are mostly not discussed separately.  |
|                      |The following AI and robotics applications and products are prominently discussed:          |
|                      |applications in healthcare (care robots, healthcare apps, surgical robots), applications in |
|                      |transportation (especially driverless cars), applications in the workplace, and applications|
|                      |in defence. The most important issues relating to these are: privacy and data protection    |
|                      |issues, responsibility and liability issues, changes in the workplace and unemployment,     |
|                      |issues of safety, bias and discrimination (e.g., through facial recognition, algorithmic    |
|                      |decision-making), issues of transparency, issues of control (automated systems              |
|                      |dominating humans), and trust.                                                              |
| Greece               |The academic discussion on ethical issues is scant and very recent (mostly after 2016). The |
|                      |articles address issues in the context of data protection (including drones), intellectual  |
|                      |property, and contract formation, and reference ethical principles such as privacy,         |
|                      |autonomy, justice, safety, and control.                                                     |
|                      |34                                                                                          |
|741716 \| SIENNA \| D4.4|                                                                                            |
# Table 5: Summarised findings per country on the country-specific academic debate on ethical issues in robotics.
|        4.2.3.        |                        Popular media debate on ethical issues in AI                        |
|----------------------|--------------------------------------------------------------------------------------------|
| Country              |Summarised findings                                                                         |
| Brazil               |There exist a lot of media articles that discuss the use of AI to improve education, and the|
|                      |importance of training people in the use of AI so as to better prepare them for the (future)|
|                      |job market. Media articles have more to say about the economic impact of AI development     |
|                      |than the academic articles. They often talk about recent AI advancements and their          |
|                      |economic impacts in general terms.                                                          |
|                      |35                                                                                          |
|741716 \| SIENNA \| D4.4|                                                                                            |
# Table 6: Summarised findings per country on the country-specific popular media debate on ethical issues in AI.
|        4.2.4.        |                       Popular media debate on ethical issues in robotics                        |
|----------------------|-------------------------------------------------------------------------------------------------|
| Country              |Summarised findings                                                                              |
| Brazil               |A lot of media articles discuss the use of robotics to improve education, and the                |
|                      |importance of training people in the use of robotics so as to better prepare them for the        |
|                      |(future) job market. Media articles have more to say about the economic impact of robot          |
|                      |development than the academic articles. They often talk about recent robotics                    |
|                      |advancements and their economic impacts in general terms.                                        |
| China                |The is focus on ethical issues in relation to robotics in general and in relation to specific    |
|                      |applications such as autonomous vehicles and “hotel service robots”. Issues discussed            |
|                      |include privacy, responsibility, job, and human control. Suggestions are frequently offered      |
|                      |to improve laws and regulation, industry norms and standards for robotics, and to                |
|                      |establish ethical values and principles for robotics development. There is also recognition      |
|                      |for the need to engage with stakeholders in robotics development.                                |
| France               |An analysis of the popular media debate was not conducted.                                       |
| Germany              |Much of the German popular media debate focuses on the same topics that are discussed            |
|                      |in the academic debate, especially autonomous vehicles, AI and robotics in the workplace,        |
|                      |and AI and robotics in the defence sector. The most important issues relating to these are:      |
|                      |liability issues (for autonomous cars), automated decision-making by AI and robotic              |
|                      |systems, the potential for mass unemployment and its societal effects, robots making             |
|                      |decisions on life and death, and ethics by design (can we teach robots morality?).               |
| Greece               |The popular media discussion on ethical issues is burgeoning and very recent (mostly after       |
|                      |2018). There is discussion of potential impacts in the military domain (autonomous               |
|                      |robots), and on work and employment. Many texts also discuss the importance of making            |
|                      |sure robotic systems behave ethically.                                                           |
| Netherlands          |There is a broad range of robots that are discussed in the popular media debate.                 |
|                      |Healthcare is a popular topic for robotics in the Netherlands, as there are already robots       |
|                      |used in this field. Emerging robots like household / home robots, self-driving cars and          |
|                      |police robots are often discussed as well. The main issues that are addressed on the short       |
|                      |term or currently experienced are safety and privacy. On the long term, questions about          |
|                      |responsibility and liability are mentioned.                                                      |
| Poland               |Overall, there seems to be little attention for ethical issues in relation to AI in Polish media.|
|                      |The articles that have been analysed often consider robotics technology in general, while        |
|                      |there was some focus on industrial robots, driverless vehicles and drones, and services          |
|                      |and transportation applications. Ethical issues discussed related to the impacts in terms of     |
|                      |unemployment and the quality of work, the impacts on international economic relations,           |
|                      |safety, privacy, psychological implications of the interaction with humanoid robots, and         |
|                      |criminal liability for damages.                                                                  |
| South Africa         |There is a livelier ethical discussion in the popular media than in academia. Much of the        |
|                      |debate is about autonomous weapons and their ethical issues, and about social justice            |
|                      |(relocation of jobs to high-income countries, increasing unemployment in South Africa),          |
|                      |and South Africa as a moral leader and what African values could bring to the regulation         |
|                      |of AI and robotics.                                                                              |
|                      |37                                                                                               |
|741716 \| SIENNA \| D4.4|                                                                                                 |
# Table 7: Summarised findings per country on the country-specific popular media debate on ethical issues in
|                                               robotics.                                               |             -              |            -             |            -             |
|-------------------------------------------------------------------------------------------------------|----------------------------|--------------------------|--------------------------|
|4.3.                                                                                                   |Discussion of findings      |                          |                          |
|Upon preparing the SIENNA country studies task for the ethical analysis of AI and robotics, we hoped   |                            |                          |                          |
|the results would lead to the identification of new ethical issues not found in the broader literature.|                            |                          |                          |
|Unfortunately, however, few unique insights about ethical issues were gleaned. That said, we can still |                            |                          |                          |
|                                                                                                       |5.2.1. Ethical issues with  |5.2.2. Ethical issues with|5.2.3. Ethical issues with|
|                                                                                                       |regard to the aims of AI and|fundamental techniques,   |regard to general         |
|                                                                                                       |its subfields               |methods and approaches    |implications and risks    |
|                                                                                                       |Efficiency and productivity |Robot sensing             |Loss of control           |
|                                                                                                       |improvement                 |                          |                          |
|                                                                                                       |Robot actuation             |Autonomy                  |                          |
|                                                                                                       |Effectiveness improvement   |                          |                          |
# Table 8: Overview of ethical issues with major types of AI products.
|                                                  6.1.1.                                                  |Intelligent agents|
|----------------------------------------------------------------------------------------------------------|------------------|
|Please note that ethical issues with intelligent agents that qualify as robots are discussed under various|                  |
|categories in section 6.2 on robotics products and in section 7.2 on robotics applications.               |                  |
|Intelligent agents are autonomous, artificially created entities364 that perceive their environment       |                  |
|through sensors, act upon that environment using actuators, and direct their activity towards achieving   |                  |
# Table 9: Overview of ethical issues with major types of robotics products.
|                                              6.2.1.                                              |Humanoid robots|
|--------------------------------------------------------------------------------------------------|---------------|
|Humanoid robots refer to a category of robots designed to imitate human beings in appearance,     |               |
|mannerisms, language, emotions, and/or actions. While not limited to these areas of emulation, the|               |
|primary purpose of humanoid robots is to cross the “uncanny valley”516 and encourage humans to    |               |
|interact with robots as though they were interacting with another human being. As such, one of the|               |
# Table 10: Overview of ethical issues in major AI application domains.
|                                                 7.1.1.                                                  |Infrastructure & cities|
|---------------------------------------------------------------------------------------------------------|-----------------------|
|AI technology may be used for infrastructure and cities to create a so-called “smart city”. IBM describes|                       |
|a smart city as a city where its components and its citizens are instrumented, interconnected, and       |                       |
|intelligent.656 Instrumented implies that a city and its citizens are provided with infrastructures and  |                       |
|devices that respond to a network. The information they provide to the network is then available to      |                       |
# Table 11: Overview of ethical issues in major robotics application domains.
|                                                7.2.1.                                                 |Transportation|
|-------------------------------------------------------------------------------------------------------|--------------|
|One of the most common ethical problems encountered in discussions on autonomous vehicles (AVs),       |              |
|especially those sharing the roadways or airspace with human operators, is that of forced choice       |              |
|decisions. Otherwise known as collision ethics or crash ethics, forced choice decisions focus on how to|              |
|program autonomous vehicles to “decide” between two or more unideal choices. For example, if there     |              |


## Key points
- This SIENNA deliverable offers a broad ethical analysis of artificial intelligence (AI) and robotics technologies
- We have summarised the content of the SIENNA report on ethical analysis of artificial intelligence and robotics
- Notable was the significant amount of attention the ethics of defence applications of AI and robotics has been receiving in most countries
- Would it perhaps be preferred to refocus on collaboration rather than on replacement? Or should we focus on using robotics to enhance and supplement human beings in a more rehabilitative stance? And if robots are going to replace humans in caregiving, to what extent is it in favour of the patient to know that they are dealing with a robot? Humanoid robots may be able to deceive children or elderly people by disguising themselves as human caregivers in order to provide better care or to build a better relationship with patients who are sceptical about dealing with robots
- Paris, “Humans to serve the rich, robots to serve the poor,” Medium, August 28, 2016. https://medium.com/radical-urbanist/humans-to-serve-the-rich-robots-to-serve-the-poor-6e2efc95c1b4. In this SIENNA deliverable, we have engaged in an extensive ethical analysis of artificial intelligence and robotics technologies, including their various manifestations and applications
- Notable was the significant amount of attention the ethics of defence applications of AI and robotics in most countries

## Synopsis

### Introduction
This SIENNA deliverable offers a broad ethical analysis of artificial intelligence (AI) and robotics technologies
It identifies and analyses the present and potential future ethical issues in relation to: (1) the AI and robotics subfields, techniques, approaches and methods; (2) their physical technological products and procedures that are designed for practical applications; and (3) the particular uses and applications of these products and procedures.
This deliverable provides an overview of the history and state of the art of the academic debate on ethics of AI and robot ethics, as well as an overview of the current institutional support of these fields.
The report presents a summary of our “country studies” analyses of the national academic and popular media debate on the ethical issues in AI and robotics in twelve different EU and non-EU countries

### Objectives
As part of the SIENNA project, this report engages in an extensive ethical analysis of AI and robotics technologies, including their various manifestations and applications.
It aims to identify and analyse ethical issues in AI and robotics, both present and potential future ethical issues, with a time horizon of twenty years.
The aim of the report is not to make recommendations or present solutions, but only to identify and analyse ethical issues.
The primary aims of this report have been to comprehensively identify and further analyse the most important present and potential future ethical issues in relation to AI and robotics technology, their products, and their applications.
The country study results are expected to contribute more significantly to future SIENNA deliverables.

### Methods
The methodology for the ethical analysis of AI and robotics, carried out in sections 5, 6 and 7 of the report was developed earlier in the SIENNA project, and is called the “SIENNA approach to ethical analysis”.
It is based on literature review, consultation of experts and stakeholders, and original ethical analysis.
It consists of a six-step process that is visualized in figure 2 at the beginning of section 2.
The article could at least identify the issues in the article as interesting for natives of the country

### Findings
Training algorithms for 100% accuracy is practically unfeasible.[^94]. An OECD-commissioned study across 32 countries finds that about 14% of jobs in OECD countries could be lost because of automation because they are highly automatable (automation probability of over 70%).
The more the elderly become capable of communicating with such machines, the more they might rely on such “relationships”; and the less they might feel the need for actual human conversations which might lead to social isolation
When it comes to public opinion, Wachsmuth (2018) reported that out of 26000 European citizens completing a survey, “more than half (60%) of the respondents stated that the use of robots should be banned in the care of children, elderly, and the disabled.”[936] Sparrow (2016) sees this worry based on two grounds: he thinks that robots are incapable of providing interpersonal relations of recognition and respect that are vital to promoting the well-being of the elderly.[937]

### Discussion
Upon preparing the SIENNA country studies task for the ethical analysis of AI and robotics, we hoped the results would lead to the identification of new ethical issues not found in the broader literature.
We can still draw a number of interesting conclusions about the findings laid out in the previous subsection, and highlights some of the similarities and differences between the debates in the twelve countries under study.
Regarding the academic debates in the twelve countries, the following has been observed.
Notable is the relative lack of country-specific studies in the UK, which may be explained by the international academic orientation of UK institutions

### Conclusion
We have summarised the content of the SIENNA report on ethical analysis of artificial intelligence and robotics.
Micro aerial vehicle Microelectromechanical system Natural language processing Non-player character Research and development Socially assistive robot Socio-economic impact assessment Unmanned aerial vehicle.
In this SIENNA deliverable, we have engaged in an extensive ethical analysis of artificial intelligence and robotics technologies, including their various manifestations and applications.
A wide range of ethical issues were discussed, relating to justice, equality, autonomy, dignity, explainability, transparency, safety, accountability, liability, privacy, and data protection.
This largely reflects the international academic debate.
The national academic debates in the US, Germany and China stood out in being focused on potential broad-scoped solutions to ethical issues, including through laws, standards, and regulation, as well as through ethics by design and implementation of moral reasoning systems in robots and AI systems


## Study subjects

### 20 articles
- The searches for academic articles were conducted through Google Scholar, and the searches for popular media articles were conducted through the regular Google search engine. ==Partners included a limited number of the most relevant articles in a detailed analysis of their ethics content (i.e., at most 20 articles each for the academic and popular media analyses, depending on the number of relevant articles they found)==. The following search terms were suggested to the partners, which they could adapt to their country’s situation (e.g., translate into the language of the country): For robotics: (“robots” OR “robotics” OR “automation” OR “automated” OR “machine” OR “machines” OR “unmanned” OR “driverless” OR “pilotless” OR “drones”) AND (“ethics” OR “ethical” OR “social” OR “legal”) AND <COUNTRY>

### 6 articles
- Topics in the popular media debate are mostly similar to the academic discussion in Spain: ethical issues in relation to autonomous vehicles, autonomous weapons, autonomous decision-making, work/jobs, bias and discrimination, use of data, and privacy. ==The Swedish popular media debate analysis was based on six articles from online scientific and IT news magazines==. In these articles, the following concerns in relation to AI and robotics were raised: governance of implementation of AI technology in Swedish society, workers’ job security, distribution of welfare to future generations, longevity, cybersecurity and cyberwarfare, the human aspect of AI, and existential risks of AI

### 200 senior-level women
- 741716 | SIENNA | D4.4 Deliverable report in the technology sector. ==A 2015 survey of 200 senior-level women in Silicon Valley showed that 84 percent had been told they were “too aggressive” in the office, 66 percent reported being excluded from important events because of their gender, and 60 percent reported unwanted sexual advances in the workplace.919==. Todd (2015) also presents the argument that AI draws less women because the field currently de-emphasizes humanistic and communal goals.[920]

## Findings
- Training algorithms for 100% accuracy is practically unfeasible.[^94]
- An OECD-commissioned study across 32 countries finds that about 14% of jobs in OECD countries could be lost because of automation because they are highly automatable (automation probability of over 70%)
- Regarding ethnicity, Muro, Maxim and Whiton find that in the United States, Hispanic and Black workers are more at risk than white workers (47% and 44% vs. 40%), and Asian workers are less at risk (39%).[235]
- The more the elderly become capable of communicating with such machines, the more they might rely on such “relationships”; and conversely, the less they might feel the need for actual human conversations which might lead to social isolation. When it comes to public opinion, Wachsmuth (2018) reported that out of 26000 European citizens completing a survey, “more than half (60%) of the respondents stated that the use of robots should be banned in the care of children, elderly, and the disabled.”[936] Sparrow (2016) sees this worry based on two grounds: he thinks that robots are incapable of providing interpersonal relations of recognition and respect that are vital to promoting the well-being of the elderly.[937]

##  Builds on previous research
- Much of the work on specifying the impacts had already been conducted as a part of the SIENNA D4.1 ==report.[^8]. Furthermore, in this step, we identified and specified relevant stakeholders== (e.g., decision makers, those involved in benefitting or being harmed by the subject or its impacts) and made plans to engage them
- In this report, the three levels of ethical analysis are each covered in a separate section: section 5 for the technology level, section 6 for the product level, and section 7 for the application level. Methods for the identification and specification present and potential future ethical issues at the three levels of analysis have included: (1) literature review of prior ethics studies in the fields of AI and robotics, (2) stakeholder and expert consultation through workshops and interviews,[^9] and (3) the use list of questions about the technologies that could help identify ethical issues (which are sometimes presented as “checklists”10), e.g., by cross-referencing them with ==the results of our SIENNA D4.1 report on the state of the art of AI and robotics technology==
- The full ==reports of these studies are provided on the SIENNA website.[^33]. In the remainder of this section, we describe the methodology of the SIENNA country studies== (subsection 4.1), we present the summarised findings for each of the country studies (subsection 4.2), and we present a preliminary analysis of the findings, highlighting similarities and differences between the countries (subsection 4.3)
- The task consisted of two parts: (1) a search for, and analysis of the contents of, recent (2000–present) academic articles on the ethics of AI and robotics that are specific to the country under study; and (2) a search for, and analysis of the contents of, recent (2000–present) popular media articles on the ethical, legal and social issues in relation to AI and robotics that are specific to the country under study. ==By “specific to the country under study” we mean that only those articles were included that had been authored by individuals at institutions within the country and were specifically addressing ethical issues with reference to the local context of the country== (e.g., population, geography, economy, or other fundamental characteristics of the country).[^34]
- A present aim of more fundamental research in AI is the development of artificial general intelligence (AGI), and a future aim may be to develop artificial superintelligence. Even if many experts indicate that ==we are not likely to see either of these being realized within the next 20 years (which is SIENNA’s scope for studying the ethical issues in AI and robotics),[^48],[^49] it may still be worthwhile to briefly consider the ethical issues related to these aims==
- Nudging at this scale may well limit our autonomy, by constantly influencing and steering our choices and decisions. ==A second, more sophisticated way of in which AI systems can be used to influence our values and desires is through their use in psychographic modelling==.[^143]

## Differs from previous work
- First of all, it is difficult to have algorithmic accountability when AI systems lack transparency.[^193],[^194] For an AI system or algorithm to be transparent, its purpose, inputs and operations should be knowable by its stakeholders, so that they can understand how its decisions are arrived at. Many AI systems that are currently being developed lack transparency and can be characterised as “black boxes”, where ==we can see input-output relations but we do not know how and why they are produced==

## Contributions
- We have summarised the content of the SIENNA report on ethical analysis of artificial intelligence and robotics. We reviewed the objectives and structure of the report, reviewed its methodology, and summarized its major findings: those concerning past academic and practical activity in ethics of AI and    List of tables    List of figures<br/><br/>List of acronyms and abbreviations    Abbreviation<br/><br/>AI ANN AUV BEAM CAD Cobot EC GPS IoT ITS    Explanation<br/><br/>Artificial intelligence Artificial neural networks Autonomous underwater vehicle Biology, electronics, aesthetics and mechanics Computer aided design Collaborative robot European Commission Global Positioning System Internet of Things Intelligent tutoring system<br/><br/>MAV MEMS NLP NPC R&D SAR SEIA UAV<br/><br/>Micro aerial vehicle Microelectromechanical system Natural language processing Non-player character Research and development Socially assistive robot Socio-economic impact assessment Unmanned aerial vehicle<br/><br/>Glossary of terms

## Future work
- In section 7, we identify and analyse the main ethical issues with regard to AI and robotics applications. In section 8, we conclude with a summary and recommendations for further study.
- More study is required to measure both predictive policing's benefits and its downsides.”[718] This is essential to identify the ethical issues involved in the use of AI by LEAs and to make sure appropriate measures are in place to address them.
- In what follows, we provide a summary of our findings and give a brief outline of how this report will be used for further work in the context of SIENNA.
- Having summarised the most important findings of this deliverable, let us conclude by briefly looking at further work in the context of SIENNA. As stated earlier, the aim of the report has not been to make recommendations or present solutions, but only to identify and analyse ethical issues. The report charts the ethical issues that should be taken into account in the development, use and regulation of AI and robotics technologies along their full breadth. In SIENNA, the findings presented here provide an important basis for our report (SIENNA D4.7, which is due in 2020), in which we aim to present an ethical framework for AI and robotics that contains recommendations and solutions for ethical issues. This will bring us one step closer to realising the project’s aims of developing a set of.

## Data and code
- The information they provide to the network is then available to the other devices, making them interconnected
- https://datasociety.net/wpcontent/uploads/2018/04/Data_Society_Algorithmic_Accountability_Primer_FINAL.pdf
- https://www.pwc.com/hu/hu/kiadvanyok/assets/pdf/impact_of_automation_on_jobs.pdf


## References
[^Roth_2014_a]: Aaron Roth, "The algorithmic foundations of differential privacy," Foundations and Trends® in Theoretical Computer Science, Vol. 9, No. 3–4, 2014, pp. 211-407. [[Roth_AlgorithmicFoundationsDifferentialPrivacy_2014]] [OA](https://engine.scholarcy.com/oa_version?query=Roth%2C%20Aaron%20The%20algorithmic%20foundations%20of%20differential%20privacy%2C%202014) [GScholar](https://scholar.google.co.uk/scholar?q=Roth%2C%20Aaron%20The%20algorithmic%20foundations%20of%20differential%20privacy%2C%202014) [Scite](https://engine.scholarcy.com/scite_url?query=Roth%2C%20Aaron%20The%20algorithmic%20foundations%20of%20differential%20privacy%2C%202014)

[^Computer_2017_a]: Computer Science, Vol. 113, 2017. [[Computer__2017]] [OA](https://scholar.google.co.uk/scholar?q=Computer%20Science%20Vol%20113%202017) [GScholar](https://scholar.google.co.uk/scholar?q=Computer%20Science%20Vol%20113%202017) 

[^Acemoglu_2019_a]: Acemoglu, Daron, “Why Universal Basic Income Is a Bad Idea,” Project Syndicate, June 7, 2019. [[Acemoglu_UniversalBasicIncomeIsBad_2019]] [OA](https://engine.scholarcy.com/oa_version?query=Acemoglu%2C%20Daron%20Why%20Universal%20Basic%20Income%20Is%20a%20Bad%20Idea%2C%202019-06-07) [GScholar](https://scholar.google.co.uk/scholar?q=Acemoglu%2C%20Daron%20Why%20Universal%20Basic%20Income%20Is%20a%20Bad%20Idea%2C%202019-06-07) [Scite](https://engine.scholarcy.com/scite_url?query=Acemoglu%2C%20Daron%20Why%20Universal%20Basic%20Income%20Is%20a%20Bad%20Idea%2C%202019-06-07)

[^Acemoglu_2018_a]: https://www.project-syndicate.org/commentary/why-universal-basic-income-is-a-bad-idea-bydaron-acemoglu-2019-06 Acemoglu, Daron, and Restrepo, Pascual, “Artificial Intelligence, Automation and Work”, National Bureau of Economic Research (NBER), January 2018, p.4. [[Acemoglu_ArtificialIntelligenceAutomationWork_2018]] [OA](https://www.project-syndicate.org/commentary/why-universal-basic-income-is-a-bad-idea-bydaron-acemoglu-2019-06)  [Scite](https://engine.scholarcy.com/scite_url?query=Acemoglu%2C%20Daron%20Restrepo%2C%20Pascual%20Artificial%20Intelligence%2C%20Automation%20and%20Work%202018-01)

[^Adam_1998_a]: Adam, Alison, Artificial Knowing: Gender and the Thinking Machine, Florence, KY, USA: Routledge, 1998. [[Adam_ArtificialKnowingGenderThinkingMachine_1998]] [OA](https://scholar.google.co.uk/scholar?q=Adam%2C%20Alison%20Artificial%20Knowing%3A%20Gender%20and%20the%20Thinking%20Machine%201998) [GScholar](https://scholar.google.co.uk/scholar?q=Adam%2C%20Alison%20Artificial%20Knowing%3A%20Gender%20and%20the%20Thinking%20Machine%201998) 

[^Addison_et+al_2019_a]: Addison, A., C. Bartneck, and K. Yogeeswaran, “Robots Can Be More Than Black And White,” Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society - AIES 19, 2019. doi: 10.1145/3306618.3314272 [[Addison_et+al_RobotsCanBeMoreThan_2019]] [OA](https://doi.org/10.1145/3306618.3314272)  [Scite](https://scite.ai/reports/10.1145/3306618.3314272)

[^Aetheon_2018_a]: Aetheon. “Tug Informational Graphics”, 2018. Accessed December 2018. aethon.com/infographics Ahmed, Kaoutar Ben, Mohammed Bouhorma, and Mohamed Ben Ahmed, "Age of big data and smart cities: privacy trade-off," arXiv preprint arXiv:1411.0087, 2014. Akhavan et al., “Exploring the Relationship between Ethics, Knowledge Creation and Organizational Performance”, 44. [[Aetheon_InformationalGraphics_2018]] [OA](https://export.arxiv.org/pdf/1411.0087)  

[^Akhil_2018_a]: Akhil, A., “Researchers Aim to Build Eco-Friendly Robots with Biodegradable Materials”, Sastra Robotics, July 2018. Al-Naji, Ali, Perera, Asanka & Chahl, Javaan, “Remote Monitoring of Cardiorespiratory Signals from a Hovering Unmanned Aerial Vehicle”, BioMedical Engineering Online 16(101), August 2017. AlDairi, Anwaar, and Lo’ai Tawalbeh, "Cyber security attacks on smart cities and associated mobile technologies," Procedia Computer Science, Vol. 109, 2017, pp. 1086-1091. [[Akhil_ResearchersAimBuildEcofriendlyRobots_2018]] [OA](https://engine.scholarcy.com/oa_version?query=Akhil%2C%20A.%20Researchers%20Aim%20to%20Build%20Eco-Friendly%20Robots%20with%20Biodegradable%20Materials%202018-07) [GScholar](https://scholar.google.co.uk/scholar?q=Akhil%2C%20A.%20Researchers%20Aim%20to%20Build%20Eco-Friendly%20Robots%20with%20Biodegradable%20Materials%202018-07) [Scite](https://engine.scholarcy.com/scite_url?query=Akhil%2C%20A.%20Researchers%20Aim%20to%20Build%20Eco-Friendly%20Robots%20with%20Biodegradable%20Materials%202018-07)

[^Alesich_2017_a]: Alesich, Simone, and Michael Rigby, “Gendered Robots: Implications for Our Humanoid Future,” IEEE Technology and Society Magazine, Vol. 36, No. 2, 2017, pp. 50-59. [[Alesich_GenderedRobotsImplicationsOurHumanoid_2017]] [OA](https://engine.scholarcy.com/oa_version?query=Alesich%2C%20Simone%20Rigby%2C%20Michael%20Gendered%20Robots%3A%20Implications%20for%20Our%20Humanoid%20Future%2C%202017) [GScholar](https://scholar.google.co.uk/scholar?q=Alesich%2C%20Simone%20Rigby%2C%20Michael%20Gendered%20Robots%3A%20Implications%20for%20Our%20Humanoid%20Future%2C%202017) [Scite](https://engine.scholarcy.com/scite_url?query=Alesich%2C%20Simone%20Rigby%2C%20Michael%20Gendered%20Robots%3A%20Implications%20for%20Our%20Humanoid%20Future%2C%202017)

[^Bentley_et+al_2014_a]: Alexander R. Bentley, O'Brien, M., J. & Brock, W. A., (2014). Mapping collective behavior in the bigdata era. Behavioral and Brain Sciences 37 (1):63-76. [[Bentley_et+al_MappingCollectiveBehaviorBigdata_2014]] [OA](https://engine.scholarcy.com/oa_version?query=Bentley%2C%20Alexander%20R.%20O%27Brien%2C%20M.%20J.%20Brock%2C%20W.A.%20Mapping%20collective%20behavior%20in%20the%20bigdata%20era%202014) [GScholar](https://scholar.google.co.uk/scholar?q=Bentley%2C%20Alexander%20R.%20O%27Brien%2C%20M.%20J.%20Brock%2C%20W.A.%20Mapping%20collective%20behavior%20in%20the%20bigdata%20era%202014) [Scite](https://engine.scholarcy.com/scite_url?query=Bentley%2C%20Alexander%20R.%20O%27Brien%2C%20M.%20J.%20Brock%2C%20W.A.%20Mapping%20collective%20behavior%20in%20the%20bigdata%20era%202014)

[^Allen_2009_a]: Allen, Colin, and Wendell Wallach, Moral Machines: Teaching Robots Right from Wrong, London, U.K.: Oxford University Press, 2009. [[Allen_MoralMachinesTeachingRobotsRight_2009]] [OA](https://scholar.google.co.uk/scholar?q=Allen%2C%20Colin%20Wallach%2C%20Wendell%20Moral%20Machines%3A%20Teaching%20Robots%20Right%20from%20Wrong%202009) [GScholar](https://scholar.google.co.uk/scholar?q=Allen%2C%20Colin%20Wallach%2C%20Wendell%20Moral%20Machines%3A%20Teaching%20Robots%20Right%20from%20Wrong%202009) 

[^Allen_et+al_2000_a]: Allen, Colin, Gary Varner, and Jason Zinser, “Prolegomena to Any Future Artificial Moral Agent,” Journal of Experimental & Theoretical Artificial Intelligence, Vol. 12, No. 3, 2000, pp. 251– 261. [[Allen_et+al_ProlegomenaAnyFutureArtificialMoral_2000]] [OA](https://engine.scholarcy.com/oa_version?query=Allen%2C%20Colin%20Varner%2C%20Gary%20Zinser%2C%20Jason%20Prolegomena%20to%20Any%20Future%20Artificial%20Moral%20Agent%2C%202000) [GScholar](https://scholar.google.co.uk/scholar?q=Allen%2C%20Colin%20Varner%2C%20Gary%20Zinser%2C%20Jason%20Prolegomena%20to%20Any%20Future%20Artificial%20Moral%20Agent%2C%202000) [Scite](https://engine.scholarcy.com/scite_url?query=Allen%2C%20Colin%20Varner%2C%20Gary%20Zinser%2C%20Jason%20Prolegomena%20to%20Any%20Future%20Artificial%20Moral%20Agent%2C%202000)

[^Allen_et+al_2006_a]: Allen, Colin, Wendell Wallach, and Iva Smit, “Why Machine Ethics?,” IEEE Intelligent Systems, Vol. 21, No. 4, 2006, pp. 12–17. [[Allen_et+al_MachineEthics_2006]] [OA](https://engine.scholarcy.com/oa_version?query=Allen%2C%20Colin%20Wallach%2C%20Wendell%20Smit%2C%20Iva%20Why%20Machine%20Ethics%3F%2C%202006) [GScholar](https://scholar.google.co.uk/scholar?q=Allen%2C%20Colin%20Wallach%2C%20Wendell%20Smit%2C%20Iva%20Why%20Machine%20Ethics%3F%2C%202006) [Scite](https://engine.scholarcy.com/scite_url?query=Allen%2C%20Colin%20Wallach%2C%20Wendell%20Smit%2C%20Iva%20Why%20Machine%20Ethics%3F%2C%202006)

[^Amini_et+al_2019_a]: Amini, Alexander, Ava Soleimany, Wilko Schwarting, Sangeeta Bhatia, and Daniela Rus, "Uncovering and Mitigating Algorithmic Bias through Learned Latent Structure," Proceedings of the 2019 AAAI/ACM Conference on Artificial Intelligence, Ethics, and Society (AIES), 27-28 January, 2019 Honolulu, Hawaii, United States, AAAI/ACM, 2019.) Amiribesheli, Mohsen, Asma Benmansour, and Abdelhamid Bouchachia, "A review of smart homes in healthcare," Journal of Ambient Intelligence and Humanized Computing, Vol. 6, No. 4, 2015, pp. 495-517., p. 495 [[Amini_et+al_UncoveringMitigatingAlgorithmicBiasThrough_2019]] [OA](https://engine.scholarcy.com/oa_version?query=Amini%2C%20Alexander%20Soleimany%2C%20Ava%20Schwarting%2C%20Wilko%20Bhatia%2C%20Sangeeta%20Uncovering%20and%20Mitigating%20Algorithmic%20Bias%20through%20Learned%20Latent%20Structure%2C%202019-01) [GScholar](https://scholar.google.co.uk/scholar?q=Amini%2C%20Alexander%20Soleimany%2C%20Ava%20Schwarting%2C%20Wilko%20Bhatia%2C%20Sangeeta%20Uncovering%20and%20Mitigating%20Algorithmic%20Bias%20through%20Learned%20Latent%20Structure%2C%202019-01) [Scite](https://engine.scholarcy.com/scite_url?query=Amini%2C%20Alexander%20Soleimany%2C%20Ava%20Schwarting%2C%20Wilko%20Bhatia%2C%20Sangeeta%20Uncovering%20and%20Mitigating%20Algorithmic%20Bias%20through%20Learned%20Latent%20Structure%2C%202019-01)

[^Datta_et+al_2015_a]: Amit Datta, Tschantz M. C. & Datta, A. (2015). Automated Experiments on Ad Privacy Settings A Tale of Opacity, Choice, and Discrimination. Proceedings on Privacy Enhancing Technologies 2015 (1), 92–112. [[Datta_et+al_AutomatedExperimentsAdPrivacySettings_2015]] [OA](https://engine.scholarcy.com/oa_version?query=Datta%2C%20Amit%20C.%2C%20Tschantz%20M.%20Datta%2C%20A.%20Automated%20Experiments%20on%20Ad%20Privacy%20Settings%20A%20Tale%20of%20Opacity%2C%20Choice%2C%20and%20Discrimination%202015) [GScholar](https://scholar.google.co.uk/scholar?q=Datta%2C%20Amit%20C.%2C%20Tschantz%20M.%20Datta%2C%20A.%20Automated%20Experiments%20on%20Ad%20Privacy%20Settings%20A%20Tale%20of%20Opacity%2C%20Choice%2C%20and%20Discrimination%202015) [Scite](https://engine.scholarcy.com/scite_url?query=Datta%2C%20Amit%20C.%2C%20Tschantz%20M.%20Datta%2C%20A.%20Automated%20Experiments%20on%20Ad%20Privacy%20Settings%20A%20Tale%20of%20Opacity%2C%20Choice%2C%20and%20Discrimination%202015)

[^International_2015_a]: Amnesty International, “Autonomous Weapons Systems: Five Key Human Rights Issues for Consideration”, 2015. https://www.amnesty.org/en/documents/act30/1401/2015/en/ [[International_AutonomousWeaponsSystemsFiveKey_2015]] [OA](https://www.amnesty.org/en/documents/act30/1401/2015/en/)  

[^Ananny_2016_a]: Ananny, Mike, and Kate Crawford, “Seeing without Knowing: Limitations of the Transparency Ideal and Its Application to Algorithmic Accountability,” New Media & Society, Vol. 20, No. 3, 2016, pp. 973–989. [[Ananny_SeeingWithoutKnowingLimitationsTransparency_2016]] [OA](https://engine.scholarcy.com/oa_version?query=Ananny%2C%20Mike%20Crawford%2C%20Kate%20Seeing%20without%20Knowing%3A%20Limitations%20of%20the%20Transparency%20Ideal%20and%20Its%20Application%20to%20Algorithmic%20Accountability%2C%202016) [GScholar](https://scholar.google.co.uk/scholar?q=Ananny%2C%20Mike%20Crawford%2C%20Kate%20Seeing%20without%20Knowing%3A%20Limitations%20of%20the%20Transparency%20Ideal%20and%20Its%20Application%20to%20Algorithmic%20Accountability%2C%202016) [Scite](https://engine.scholarcy.com/scite_url?query=Ananny%2C%20Mike%20Crawford%2C%20Kate%20Seeing%20without%20Knowing%3A%20Limitations%20of%20the%20Transparency%20Ideal%20and%20Its%20Application%20to%20Algorithmic%20Accountability%2C%202016)

[^Anderson_2011_a]: Anderson, Susan Leigh, “Machine metaethics,” in Machine Ethics, M. Anderson and S. Anderson, Eds., New York, NY, USA: Cambridge University Press, 2011, pp. 21–27. [[Anderson_machineMetaethicsMachineEthics_2011]] [OA](https://scholar.google.co.uk/scholar?q=Anderson%2C%20Susan%20Leigh%20%E2%80%9CMachine%20metaethics%2C%E2%80%9D%20in%20Machine%20Ethics%202011) [GScholar](https://scholar.google.co.uk/scholar?q=Anderson%2C%20Susan%20Leigh%20%E2%80%9CMachine%20metaethics%2C%E2%80%9D%20in%20Machine%20Ethics%202011) 

[^Data_2004_a]: Data,” Customer Needs and Solutions, Vol. 5, p. 28–37. Andreas Matthias (2004) The responsibility gap: Ascribing responsibility for the actions of learning automata. Ethics and Information Technology 6(3): 175–183 Andrew D. Selbst, ‘Disparate Impact in Big Data Policing’, Georgia Law Review, Vol. 52, No. 1, 2017, p. [[Data_ResponsibilityAscribingResponsibilityActions_2004]] [OA](https://engine.scholarcy.com/oa_version?query=Data%2C%20%E2%80%9D%20Customer%20Needs%20Solutions%20The%20responsibility%20gap%3A%20Ascribing%20responsibility%20for%20the%20actions%20of%20learning%20automata%202004) [GScholar](https://scholar.google.co.uk/scholar?q=Data%2C%20%E2%80%9D%20Customer%20Needs%20Solutions%20The%20responsibility%20gap%3A%20Ascribing%20responsibility%20for%20the%20actions%20of%20learning%20automata%202004) [Scite](https://engine.scholarcy.com/scite_url?query=Data%2C%20%E2%80%9D%20Customer%20Needs%20Solutions%20The%20responsibility%20gap%3A%20Ascribing%20responsibility%20for%20the%20actions%20of%20learning%20automata%202004)

[^109]: Concept Paper of the 2019 OSCE Annual Police Experts Meeting Artificial Intelligence and Law Enforcement: An Ally or an Adversary?, 23-24 September 2019, Vienna: https://polis.osce.org/2019APEM Angwin, Julia, Larson, Jeff, Mattu, Surya, and Kirchner, Lauren, “Machine Bias,” ProPublica, May 2016.https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing Antoinette Rouvroy; Rouvroy, Antoinette, and Berns, Thomas, “Algorithmic Governmentality and Prospects of Emancipation,” Réseaux, Vol.177, No.1, 2013. [[Angwin_et+al_ConceptPaper2019OsceAnnual_2013]] [OA](https://polis.osce.org/2019APEM)  [Scite](https://engine.scholarcy.com/scite_url?query=Concept%20Paper%20of%20the%202019%20OSCE%20Annual%20Police%20Experts%20Meeting%20Artificial%20Intelligence%20and%20Law%20Enforcement%20An%20Ally%20or%20an%20Adversary%202324%20September%202019%20Vienna%20httpspolisosceorg2019APEM%20Angwin%20Julia%20Larson%20Jeff%20Mattu%20Surya%20and%20Kirchner%20Lauren%20Machine%20Bias%20ProPublica%20May%202016httpswwwpropublicaorgarticlemachinebiasriskassessmentsincriminalsentencing%20Antoinette%20Rouvroy%20Rouvroy%20Antoinette%20and%20Berns%20Thomas%20Algorithmic%20Governmentality%20and%20Prospects%20of%20Emancipation%20R%C3%A9seaux%20Vol177%20No1%202013)

[^Aparna_2013_a]: Aparna, Kale. Bodhale, Umesh “Overview Of Sensors For Robotics”, International Journal of Engineering Research and Technology (IJERT) Volume 02, Issue 03 (March 2013). Applin, Sally, “Autonomous Vehicles Ethics, Stock or Custom?”, IEEE Consumer Electronics 6(3), June 2017. [[Aparna_UmeshoverviewOfSensorsFor_2013]] [OA](https://engine.scholarcy.com/oa_version?query=Aparna%2C%20Kale%20Bodhale%20Umesh%20%E2%80%9COverview%20Of%20Sensors%20For%20Robotics%E2%80%9D%202013-03-03) [GScholar](https://scholar.google.co.uk/scholar?q=Aparna%2C%20Kale%20Bodhale%20Umesh%20%E2%80%9COverview%20Of%20Sensors%20For%20Robotics%E2%80%9D%202013-03-03) [Scite](https://engine.scholarcy.com/scite_url?query=Aparna%2C%20Kale%20Bodhale%20Umesh%20%E2%80%9COverview%20Of%20Sensors%20For%20Robotics%E2%80%9D%202013-03-03)

[^Araya_1995_a]: Araya, Agustin A., “Questioning Ubiquitous Computing,” Proceedings of the 1995 ACM 23rd Annual Conference on Computer Science - CSC 95, 1995. [[Araya_QuestioningUbiquitousComputing_1995]] [OA](https://engine.scholarcy.com/oa_version?query=Araya%2C%20Agustin%20A.%20Questioning%20Ubiquitous%20Computing%2C%201995) [GScholar](https://scholar.google.co.uk/scholar?q=Araya%2C%20Agustin%20A.%20Questioning%20Ubiquitous%20Computing%2C%201995) [Scite](https://engine.scholarcy.com/scite_url?query=Araya%2C%20Agustin%20A.%20Questioning%20Ubiquitous%20Computing%2C%201995)

[^Arkin_2009_a]: Arkin, Ronald, Governing Lethal Behavior in Autonomous Robots. Boca Raton, Chapman and Hall/CRC Press, 2009. [[Arkin_GoverningLethalBehaviorAutonomousRobots_2009]] [OA](https://scholar.google.co.uk/scholar?q=Arkin%2C%20Ronald%20Governing%20Lethal%20Behavior%20in%20Autonomous%20Robots%202009) [GScholar](https://scholar.google.co.uk/scholar?q=Arkin%2C%20Ronald%20Governing%20Lethal%20Behavior%20in%20Autonomous%20Robots%202009) 

[^Arnold_2017_a]: Arnold, Thomas & Scheutz, Matthais, “The Tactile Ethics of Soft Robotics: Designing Wisely for Human-Robot Interaction”, Soft Robotics 4(2), 2017. [[Arnold_TactileEthicsSoftRoboticsDesigning_2017]] [OA](https://engine.scholarcy.com/oa_version?query=Arnold%2C%20Thomas%20Scheutz%2C%20Matthais%20The%20Tactile%20Ethics%20of%20Soft%20Robotics%3A%20Designing%20Wisely%20for%20Human-Robot%20Interaction%202017) [GScholar](https://scholar.google.co.uk/scholar?q=Arnold%2C%20Thomas%20Scheutz%2C%20Matthais%20The%20Tactile%20Ethics%20of%20Soft%20Robotics%3A%20Designing%20Wisely%20for%20Human-Robot%20Interaction%202017) [Scite](https://engine.scholarcy.com/scite_url?query=Arnold%2C%20Thomas%20Scheutz%2C%20Matthais%20The%20Tactile%20Ethics%20of%20Soft%20Robotics%3A%20Designing%20Wisely%20for%20Human-Robot%20Interaction%202017)

[^Arruda_2017_a]: Arruda, Andrew, “An Ethical Obligation to Use Artificial Intelligence: An Examination of the Use of Artificial Intelligence in Law and the Model Rules of Professional Responsibility,” American Journal of Trial Advocacy, Vol. 40, 2017, pp. 443–58. [[Arruda_EthicalObligationUseArtificialIntelligence_2017]] [OA](https://engine.scholarcy.com/oa_version?query=Arruda%2C%20Andrew%20An%20Ethical%20Obligation%20to%20Use%20Artificial%20Intelligence%3A%20An%20Examination%20of%20the%20Use%20of%20Artificial%20Intelligence%20in%20Law%20and%20the%20Model%20Rules%20of%20Professional%20Responsibility%2C%202017) [GScholar](https://scholar.google.co.uk/scholar?q=Arruda%2C%20Andrew%20An%20Ethical%20Obligation%20to%20Use%20Artificial%20Intelligence%3A%20An%20Examination%20of%20the%20Use%20of%20Artificial%20Intelligence%20in%20Law%20and%20the%20Model%20Rules%20of%20Professional%20Responsibility%2C%202017) [Scite](https://engine.scholarcy.com/scite_url?query=Arruda%2C%20Andrew%20An%20Ethical%20Obligation%20to%20Use%20Artificial%20Intelligence%3A%20An%20Examination%20of%20the%20Use%20of%20Artificial%20Intelligence%20in%20Law%20and%20the%20Model%20Rules%20of%20Professional%20Responsibility%2C%202017)

[^Arruda_2018_a]: Arruda, Andrew, “The world’s first AI legal assistant”, TED Talk, November 2018. https://www.ted.com/talks/andrew_arruda_the_world_s_first_ai_legal_assistant Asaro, Peter M., ‘AI Ethics in Predictive Policing. From Models of Threat to an Ethics of Care’, IEEE Technology and Society Magazine, June 2019, pp.44–46. [[Arruda_WorldFirstAiLegalAssistant_2018]] [OA](https://www.ted.com/talks/andrew_arruda_the_world_s_first_ai_legal_assistant)  [Scite](https://engine.scholarcy.com/scite_url?query=Arruda%2C%20Andrew%20The%20world%E2%80%99s%20first%20AI%20legal%20assistant%202018-06)

[^Asaro_2019_a]: Asaro, Peter, “Algorithms of Violence: Critical Social Perspectives on Autonomous Weapons”, Social Research, Vol. 86, No. 2, 2019, p. 539. [[Asaro_AlgorithmsViolenceCriticalSocialPerspectives_2019]] [OA](https://engine.scholarcy.com/oa_version?query=Asaro%2C%20Peter%20Algorithms%20of%20Violence%3A%20Critical%20Social%20Perspectives%20on%20Autonomous%20Weapons%202019) [GScholar](https://scholar.google.co.uk/scholar?q=Asaro%2C%20Peter%20Algorithms%20of%20Violence%3A%20Critical%20Social%20Perspectives%20on%20Autonomous%20Weapons%202019) [Scite](https://engine.scholarcy.com/scite_url?query=Asaro%2C%20Peter%20Algorithms%20of%20Violence%3A%20Critical%20Social%20Perspectives%20on%20Autonomous%20Weapons%202019)

[^Asaro_2012_a]: Asaro, Peter, “On banning autonomous weapon systems: human rights, automation, and the dehumanization of lethal decision-making”, International Review of the Red Cross, Vo. 94, 2012, pp 687-709. [[Asaro_BanningAutonomousWeaponSystemsHuman_2012]] [OA](https://engine.scholarcy.com/oa_version?query=Asaro%2C%20Peter%20On%20banning%20autonomous%20weapon%20systems%3A%20human%20rights%2C%20automation%2C%20and%20the%20dehumanization%20of%20lethal%20decision-making%202012) [GScholar](https://scholar.google.co.uk/scholar?q=Asaro%2C%20Peter%20On%20banning%20autonomous%20weapon%20systems%3A%20human%20rights%2C%20automation%2C%20and%20the%20dehumanization%20of%20lethal%20decision-making%202012) [Scite](https://engine.scholarcy.com/scite_url?query=Asaro%2C%20Peter%20On%20banning%20autonomous%20weapon%20systems%3A%20human%20rights%2C%20automation%2C%20and%20the%20dehumanization%20of%20lethal%20decision-making%202012)

[^Asaro_2019_b]: Asaro, Peter, “What Is an Artificial Intelligence Arms Race Anyway”, I/S: Journal of Law and Policy for the Information Society, Vol. 15, 2019, pp. 45-64. [[Asaro_WhatIsArtificialIntelligenceArms_2019]] [OA](https://engine.scholarcy.com/oa_version?query=Asaro%2C%20Peter%20What%20Is%20an%20Artificial%20Intelligence%20Arms%20Race%20Anyway%202019) [GScholar](https://scholar.google.co.uk/scholar?q=Asaro%2C%20Peter%20What%20Is%20an%20Artificial%20Intelligence%20Arms%20Race%20Anyway%202019) [Scite](https://engine.scholarcy.com/scite_url?query=Asaro%2C%20Peter%20What%20Is%20an%20Artificial%20Intelligence%20Arms%20Race%20Anyway%202019)

[^Asaro_2018_a]: Asaro, Peter, “Why the world needs to regulate autonomous weapons, and soon”, Bulletin of the Atomic Scientists, 27 April 2018. https://thebulletin.org/2018/04/why-the-world-needs-toregulate-autonomous-weapons-and-soon/ [[Asaro_WorldNeedsRegulateAutonomousWeapons_2018]] [OA](https://thebulletin.org/2018/04/why-the-world-needs-toregulate-autonomous-weapons-and-soon/)  [Scite](https://engine.scholarcy.com/scite_url?query=Asaro%2C%20Peter%20Why%20the%20world%20needs%20to%20regulate%20autonomous%20weapons%2C%20and%20soon%202018-04-27)

[^Asaro_2013_a]: Asaro, Peter. W., “The labor of surveillance and bureaucratized killing: new subjectivities of military drone operators”, Social semiotics, Vol. 23, No. 2, 2013, p. 220. [[Asaro_LaborSurveillanceBureaucratizedKillingSubjectivities_2013]] [OA](https://engine.scholarcy.com/oa_version?query=Asaro%2C%20Peter%20W.%20The%20labor%20of%20surveillance%20and%20bureaucratized%20killing%3A%20new%20subjectivities%20of%20military%20drone%20operators%202013) [GScholar](https://scholar.google.co.uk/scholar?q=Asaro%2C%20Peter%20W.%20The%20labor%20of%20surveillance%20and%20bureaucratized%20killing%3A%20new%20subjectivities%20of%20military%20drone%20operators%202013) [Scite](https://engine.scholarcy.com/scite_url?query=Asaro%2C%20Peter%20W.%20The%20labor%20of%20surveillance%20and%20bureaucratized%20killing%3A%20new%20subjectivities%20of%20military%20drone%20operators%202013)

[^Athalye_et+al_2018_a]: Athalye, Anish, Logan Engstrom, Andrew Ilyas, and Kevin Kwok, “Synthesizing Robust Adversarial Examples,” Cornell University arXiv.org, 2018. https://arxiv.org/abs/1707.07397 [[Athalye_et+al_SynthesizingRobustAdversarialExamples_2018]] [OA](https://arxiv.org/abs/1707.07397)  

[^Avgousti_et+al_2016_a]: Avgousti, Sotiri, et al., “Medical telerobotic systems: current status and future trends”, Biomedical Engineering OnLine, Vol. 15, No. 96, 2016. [[Avgousti_et+al_MedicalTeleroboticSystemsCurrentStatus_2016]] [OA](https://engine.scholarcy.com/oa_version?query=Avgousti%2C%20Sotiri%20Medical%20telerobotic%20systems%3A%20current%20status%20and%20future%20trends%202016) [GScholar](https://scholar.google.co.uk/scholar?q=Avgousti%2C%20Sotiri%20Medical%20telerobotic%20systems%3A%20current%20status%20and%20future%20trends%202016) [Scite](https://engine.scholarcy.com/scite_url?query=Avgousti%2C%20Sotiri%20Medical%20telerobotic%20systems%3A%20current%20status%20and%20future%20trends%202016)

[^Ayadi_et+al_2011_a]: Ayadi, Moataz El, Mohamed S. Kamel, and Fakhri Karray, “Survey on Speech Emotion Recognition: Features, Classification Schemes, and Databases,” Pattern Recognition, Vol. 44, No. 3, 2011, pp. 572–587. [[Ayadi_et+al_SurveySpeechEmotionRecognitionFeatures_2011]] [OA](https://engine.scholarcy.com/oa_version?query=Ayadi%2C%20Moataz%20El%20Kamel%2C%20Mohamed%20S.%20Karray%2C%20Fakhri%20Survey%20on%20Speech%20Emotion%20Recognition%3A%20Features%2C%20Classification%20Schemes%2C%20and%20Databases%2C%202011) [GScholar](https://scholar.google.co.uk/scholar?q=Ayadi%2C%20Moataz%20El%20Kamel%2C%20Mohamed%20S.%20Karray%2C%20Fakhri%20Survey%20on%20Speech%20Emotion%20Recognition%3A%20Features%2C%20Classification%20Schemes%2C%20and%20Databases%2C%202011) [Scite](https://engine.scholarcy.com/scite_url?query=Ayadi%2C%20Moataz%20El%20Kamel%2C%20Mohamed%20S.%20Karray%2C%20Fakhri%20Survey%20on%20Speech%20Emotion%20Recognition%3A%20Features%2C%20Classification%20Schemes%2C%20and%20Databases%2C%202011)

[^Bali_2017_a]: Bali, Meghna, “Companion Robots: What are the Ethical Implications of Intimate Human-Machine Relationships?”, ABC News, August 2017. [[Bali_CompanionRobotsWhatEthicalImplications_2017]] [OA](https://scholar.google.co.uk/scholar?q=Bali%2C%20Meghna%20Companion%20Robots%3A%20What%20are%20the%20Ethical%20Implications%20of%20Intimate%20Human-Machine%20Relationships%3F%202017-08) [GScholar](https://scholar.google.co.uk/scholar?q=Bali%2C%20Meghna%20Companion%20Robots%3A%20What%20are%20the%20Ethical%20Implications%20of%20Intimate%20Human-Machine%20Relationships%3F%202017-08) 

[^Barlow_2018_a]: Barlow, Rich, “Economist Predicts Job Loss to Machines, but Sees Long-Term Hope”, Psys.org Robotics, March 2018. [[Barlow_EconomistPredictsJobLossMachines_2018]] [OA](https://engine.scholarcy.com/oa_version?query=Barlow%2C%20Rich%20Economist%20Predicts%20Job%20Loss%20to%20Machines%2C%20but%20Sees%20Long-Term%20Hope%202018-03) [GScholar](https://scholar.google.co.uk/scholar?q=Barlow%2C%20Rich%20Economist%20Predicts%20Job%20Loss%20to%20Machines%2C%20but%20Sees%20Long-Term%20Hope%202018-03) [Scite](https://engine.scholarcy.com/scite_url?query=Barlow%2C%20Rich%20Economist%20Predicts%20Job%20Loss%20to%20Machines%2C%20but%20Sees%20Long-Term%20Hope%202018-03)

[^Barocas_2017_a]: Barocas, Solon, and Andrew D. Selbst, “Big Datas Disparate Impact,” Calif. L. Rev., Vol. 104, 2016, p. 671.; Brynjolfsson, Erik, and Tom Mitchell, “What Can Machine Learning Do? Workforce Implications,” Science, Vol. 358, No. 6370, 2017, pp. 1530–1534. [[Barocas_DatasDisparateImpact_2017]] [OA](https://engine.scholarcy.com/oa_version?query=Barocas%2C%20Solon%20Selbst%2C%20Andrew%20D.%20Big%20Datas%20Disparate%20Impact%2C%202017) [GScholar](https://scholar.google.co.uk/scholar?q=Barocas%2C%20Solon%20Selbst%2C%20Andrew%20D.%20Big%20Datas%20Disparate%20Impact%2C%202017) [Scite](https://engine.scholarcy.com/scite_url?query=Barocas%2C%20Solon%20Selbst%2C%20Andrew%20D.%20Big%20Datas%20Disparate%20Impact%2C%202017)

[^Manufacturing_1995_a]: Manufacturing Industry”, Machine Vision Applications, Architectures, and Systems Integration IV, 1995. [[Manufacturing_ManufacturingIndustry_1995]] [OA](https://engine.scholarcy.com/oa_version?query=Manufacturing%20Industry%20Machine%20Vision%20Applications%20Architectures%20and%20Systems%20Integration%20IV%201995) [GScholar](https://scholar.google.co.uk/scholar?q=Manufacturing%20Industry%20Machine%20Vision%20Applications%20Architectures%20and%20Systems%20Integration%20IV%201995) [Scite](https://engine.scholarcy.com/scite_url?query=Manufacturing%20Industry%20Machine%20Vision%20Applications%20Architectures%20and%20Systems%20Integration%20IV%201995)

[^Beam_2018_a]: Beam, Andrew L. and Kohane, Isaac S., “Big Data and Machine Learning in Health Care,” Journal of American Medical Association, March 2018, E1–2. [[Beam_DataMachineLearningHealthCare_2018]] [OA](https://engine.scholarcy.com/oa_version?query=Beam%2C%20Andrew%20L.%20Kohane%2C%20Isaac%20S.%20Big%20Data%20and%20Machine%20Learning%20in%20Health%20Care%2C%202018-03) [GScholar](https://scholar.google.co.uk/scholar?q=Beam%2C%20Andrew%20L.%20Kohane%2C%20Isaac%20S.%20Big%20Data%20and%20Machine%20Learning%20in%20Health%20Care%2C%202018-03) [Scite](https://engine.scholarcy.com/scite_url?query=Beam%2C%20Andrew%20L.%20Kohane%2C%20Isaac%20S.%20Big%20Data%20and%20Machine%20Learning%20in%20Health%20Care%2C%202018-03)

[^Beard_2016_a]: Beard, Matthew, “With robots, is a life without work one we'd want to live?,” The Guardian, September 26, 2016. https://www.theguardian.com/sustainable-business/2016/sep/26/withrobots-is-a-life-without-work-one-wed-want-to-live [[Beard_WithRobotsLifeWithoutWork_2016]] [OA](https://www.theguardian.com/sustainable-business/2016/sep/26/withrobots-is-a-life-without-work-one-wed-want-to-live)  [Scite](https://engine.scholarcy.com/scite_url?query=Beard%2C%20Matthew%20With%20robots%2C%20is%20a%20life%20without%20work%20one%20we%27d%20want%20to%20live%3F%2C%202016-09-26)

[^Cardona_2008_a]: Beatriz Cardona (2008) ‘Healthy ageing’ policies and anti-ageing ideologies and practices: On the exercise of responsibility. Medicine, Health Care and Philosophy 11(4): 475–483 Beauchamp, Tom L., and Childress, James F., Principles of Biomedical Ethics, Oxford, Oxford University Press, 2012. We are grateful to Tally Hatzakis for reviewing this section. Bekey, George A., Autonomous Robots: From Biological Inspiration to Implementation and Control, MIT Press, February 2017. [[Cardona_healthyAgeingPoliciesAntiageingIdeologies_2008]] [OA](https://engine.scholarcy.com/oa_version?query=Cardona%2C%20Beatriz%20%E2%80%98Healthy%20ageing%E2%80%99%20policies%20and%20anti-ageing%20ideologies%20and%20practices%3A%20On%20the%20exercise%20of%20responsibility%202008-02) [GScholar](https://scholar.google.co.uk/scholar?q=Cardona%2C%20Beatriz%20%E2%80%98Healthy%20ageing%E2%80%99%20policies%20and%20anti-ageing%20ideologies%20and%20practices%3A%20On%20the%20exercise%20of%20responsibility%202008-02) [Scite](https://engine.scholarcy.com/scite_url?query=Cardona%2C%20Beatriz%20%E2%80%98Healthy%20ageing%E2%80%99%20policies%20and%20anti-ageing%20ideologies%20and%20practices%3A%20On%20the%20exercise%20of%20responsibility%202008-02)

[^Bendel_2018_a]: Bendel, Oliver, “Co-Robots from an Ethical Perspective”, Business Information Systems and Technology 4.0, March 2018. [[Bendel_robotsFromEthicalPerspective_2018]] [OA](https://engine.scholarcy.com/oa_version?query=Bendel%2C%20Oliver%20Co-Robots%20from%20an%20Ethical%20Perspective%202018-03) [GScholar](https://scholar.google.co.uk/scholar?q=Bendel%2C%20Oliver%20Co-Robots%20from%20an%20Ethical%20Perspective%202018-03) [Scite](https://engine.scholarcy.com/scite_url?query=Bendel%2C%20Oliver%20Co-Robots%20from%20an%20Ethical%20Perspective%202018-03)

[^Berghel_2018_a]: Berghel, H., ‘Malice Domestic: The Cambridge Analytica Dystopia’, Computer, Vol. 51, No. 5, pp. 8489, May 2018. [[Berghel_MaliceDomesticCambridgeAnalyticaDystopia_2018]] [OA](https://engine.scholarcy.com/oa_version?query=Berghel%2C%20H.%20Malice%20Domestic%3A%20The%20Cambridge%20Analytica%20Dystopia%202018-05) [GScholar](https://scholar.google.co.uk/scholar?q=Berghel%2C%20H.%20Malice%20Domestic%3A%20The%20Cambridge%20Analytica%20Dystopia%202018-05) [Scite](https://engine.scholarcy.com/scite_url?query=Berghel%2C%20H.%20Malice%20Domestic%3A%20The%20Cambridge%20Analytica%20Dystopia%202018-05)

[^Berlin_1969_a]: Berlin, Isaiah, Two Concepts of Liberty, 1969, in Berlin, Isaiah, Four Essays on Liberty, Oxford University Press, Oxford, p. 118-172. [[Berlin_ConceptsLiberty1969BerlinIsaiah_1969]] [OA](https://scholar.google.co.uk/scholar?q=Berlin%20Isaiah%20Two%20Concepts%20of%20Liberty%201969%20in%20Berlin%20Isaiah%20Four%20Essays%20on%20Liberty%20Oxford%20University%20Press%20Oxford%20p%20118172) [GScholar](https://scholar.google.co.uk/scholar?q=Berlin%20Isaiah%20Two%20Concepts%20of%20Liberty%201969%20in%20Berlin%20Isaiah%20Four%20Essays%20on%20Liberty%20Oxford%20University%20Press%20Oxford%20p%20118172) 

[^Bernard_2018_a]: Bernard, Pascal, “Is AI a threat to Democracy?,” Towards data Science, May 21, 2019. https://towardsdatascience.com/is-ai-a-threat-to-democracy-4bef3e5fcfdd Bhoge, Anand, “Smart Robotics: Revolution is Motto, Efficiency is Aim”, Robotics Tomorrow, August 2018. [[Bernard_isAiThreatDemocracyTowards_2018]] [OA](https://towardsdatascience.com/is-ai-a-threat-to-democracy-4bef3e5fcfdd)  [Scite](https://engine.scholarcy.com/scite_url?query=Bernard%20Pascal%20Is%20AI%20a%20threat%20to%20Democracy%20Towards%20data%20Science%20May%2021%202019%20httpstowardsdatasciencecomisaiathreattodemocracy4bef3e5fcfdd%20Bhoge%20Anand%20Smart%20Robotics%20Revolution%20is%20Motto%20Efficiency%20is%20Aim%20Robotics%20Tomorrow%20August%202018)

[^Bigda_2018_a]: Bigda, Jordan, “The Legal Profession: From Humans to Robots,” Journal of High Technology Law, Vol. 18, 2018, pp. 396–428. Seedrs in “Six ways the legal sector is using AI right now” (13 December 2018) identifies six different aspects of this first type of use of AI in law. https://www.lawsociety.org.uk/news/stories/six-ways-the-legal-sector-is-using-ai [[Bigda_LegalProfessionFromHumansRobots_2018]] [OA](https://www.lawsociety.org.uk/news/stories/six-ways-the-legal-sector-is-using-ai)  [Scite](https://engine.scholarcy.com/scite_url?query=Bigda%2C%20Jordan%20The%20Legal%20Profession%3A%20From%20Humans%20to%20Robots%2C%202018-12-13)

[^Binns_2017_a]: Binns, R. (2017). Algorithmic Accountability and Public Reason. Philosophy and Technology, 1-14. [[Binns_AlgorithmicAccountabilityPublicReason_2017]] [OA](https://engine.scholarcy.com/oa_version?query=Binns%2C%20R.%20Algorithmic%20Accountability%20and%20Public%20Reason%202017) [GScholar](https://scholar.google.co.uk/scholar?q=Binns%2C%20R.%20Algorithmic%20Accountability%20and%20Public%20Reason%202017) [Scite](https://engine.scholarcy.com/scite_url?query=Binns%2C%20R.%20Algorithmic%20Accountability%20and%20Public%20Reason%202017)

[^Lucidi_et+al_2018_a]: Bisconti Lucidi, Piercosma & Nardi, Daniele, “Companion Robots: The Hallucinatory Danger of Human-Robot Interactions”, Association for the Advancement of Artificial Intelligence, 2018. [[Lucidi_et+al_CompanionRobotsHallucinatoryDangerHumanrobot_2018]] [OA](https://engine.scholarcy.com/oa_version?query=Lucidi%2C%20Bisconti%20Piercosma%20Nardi%2C%20Daniele%20Companion%20Robots%3A%20The%20Hallucinatory%20Danger%20of%20Human-Robot%20Interactions%202018) [GScholar](https://scholar.google.co.uk/scholar?q=Lucidi%2C%20Bisconti%20Piercosma%20Nardi%2C%20Daniele%20Companion%20Robots%3A%20The%20Hallucinatory%20Danger%20of%20Human-Robot%20Interactions%202018) [Scite](https://engine.scholarcy.com/scite_url?query=Lucidi%2C%20Bisconti%20Piercosma%20Nardi%2C%20Daniele%20Companion%20Robots%3A%20The%20Hallucinatory%20Danger%20of%20Human-Robot%20Interactions%202018)

[^Bissolotti_et+al_2018_a]: Bissolotti, Luciano, Nicoli, Federico & Picozzi, Mario, “Domestic Use of the Exoskeleton for Gait Training in Patients with Spinal Cord Injuries: Ethical Dilemmas in Clinical Practice”, Frontiers in Neuroscience, February 2018. [[Bissolotti_et+al_DomesticUseExoskeletonGaitTraining_2018]] [OA](https://engine.scholarcy.com/oa_version?query=Bissolotti%2C%20Luciano%20Nicoli%2C%20Federico%20Picozzi%2C%20Mario%20Domestic%20Use%20of%20the%20Exoskeleton%20for%20Gait%20Training%20in%20Patients%20with%20Spinal%20Cord%20Injuries%3A%20Ethical%20Dilemmas%20in%20Clinical%20Practice%202018-02) [GScholar](https://scholar.google.co.uk/scholar?q=Bissolotti%2C%20Luciano%20Nicoli%2C%20Federico%20Picozzi%2C%20Mario%20Domestic%20Use%20of%20the%20Exoskeleton%20for%20Gait%20Training%20in%20Patients%20with%20Spinal%20Cord%20Injuries%3A%20Ethical%20Dilemmas%20in%20Clinical%20Practice%202018-02) [Scite](https://engine.scholarcy.com/scite_url?query=Bissolotti%2C%20Luciano%20Nicoli%2C%20Federico%20Picozzi%2C%20Mario%20Domestic%20Use%20of%20the%20Exoskeleton%20for%20Gait%20Training%20in%20Patients%20with%20Spinal%20Cord%20Injuries%3A%20Ethical%20Dilemmas%20in%20Clinical%20Practice%202018-02)

[^Blodgett_et+al_2016_a]: Blodgett, Su Lin, Lisa Green, and Brendan O’Connor, “Demographic Dialectal Variation in Social Media: A Case Study of African-American English,” Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, 2016. [[Blodgett_et+al_DemographicDialectalVariationSocialMedia_2016]] [OA](https://engine.scholarcy.com/oa_version?query=Blodgett%2C%20Su%20Lin%20Green%2C%20Lisa%20O%E2%80%99Connor%2C%20Brendan%20Demographic%20Dialectal%20Variation%20in%20Social%20Media%3A%20A%20Case%20Study%20of%20African-American%20English%2C%202016) [GScholar](https://scholar.google.co.uk/scholar?q=Blodgett%2C%20Su%20Lin%20Green%2C%20Lisa%20O%E2%80%99Connor%2C%20Brendan%20Demographic%20Dialectal%20Variation%20in%20Social%20Media%3A%20A%20Case%20Study%20of%20African-American%20English%2C%202016) [Scite](https://engine.scholarcy.com/scite_url?query=Blodgett%2C%20Su%20Lin%20Green%2C%20Lisa%20O%E2%80%99Connor%2C%20Brendan%20Demographic%20Dialectal%20Variation%20in%20Social%20Media%3A%20A%20Case%20Study%20of%20African-American%20English%2C%202016)

[^Blodgett_et+al_2016_b]: Blodgett, Su Lin, Lisa Green, and Brendan O’Connor, “Demographic Dialectal Variation in Social Media: A Case Study of African-American English,” Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, 2016., p. 1 [[Blodgett_et+al_DemographicDialectalVariationSocialMedia_2016]] [OA](https://engine.scholarcy.com/oa_version?query=Blodgett%2C%20Su%20Lin%20Green%2C%20Lisa%20O%E2%80%99Connor%2C%20Brendan%20Demographic%20Dialectal%20Variation%20in%20Social%20Media%3A%20A%20Case%20Study%20of%20African-American%20English%2C%202016) [GScholar](https://scholar.google.co.uk/scholar?q=Blodgett%2C%20Su%20Lin%20Green%2C%20Lisa%20O%E2%80%99Connor%2C%20Brendan%20Demographic%20Dialectal%20Variation%20in%20Social%20Media%3A%20A%20Case%20Study%20of%20African-American%20English%2C%202016) [Scite](https://engine.scholarcy.com/scite_url?query=Blodgett%2C%20Su%20Lin%20Green%2C%20Lisa%20O%E2%80%99Connor%2C%20Brendan%20Demographic%20Dialectal%20Variation%20in%20Social%20Media%3A%20A%20Case%20Study%20of%20African-American%20English%2C%202016)

[^Bohn_et+al_2005_a]: Bohn, J., V. Coroamă, M. Langheinrich, F. Mattern, and M. Rohs, “Social, Economic, and Ethical Implications of Ambient Intelligence and Ubiquitous Computing,” Ambient Intelligence, 2005, pp. 5–29. [[Bohn_et+al_SocialEconomicEthicalImplicationsAmbient_2005]] [OA](https://engine.scholarcy.com/oa_version?query=Bohn%2C%20J.%20Coroam%C4%83%2C%20V.%20Langheinrich%2C%20M.%20Mattern%2C%20F.%20Social%2C%20Economic%2C%20and%20Ethical%20Implications%20of%20Ambient%20Intelligence%20and%20Ubiquitous%20Computing%2C%202005) [GScholar](https://scholar.google.co.uk/scholar?q=Bohn%2C%20J.%20Coroam%C4%83%2C%20V.%20Langheinrich%2C%20M.%20Mattern%2C%20F.%20Social%2C%20Economic%2C%20and%20Ethical%20Implications%20of%20Ambient%20Intelligence%20and%20Ubiquitous%20Computing%2C%202005) [Scite](https://engine.scholarcy.com/scite_url?query=Bohn%2C%20J.%20Coroam%C4%83%2C%20V.%20Langheinrich%2C%20M.%20Mattern%2C%20F.%20Social%2C%20Economic%2C%20and%20Ethical%20Implications%20of%20Ambient%20Intelligence%20and%20Ubiquitous%20Computing%2C%202005)

[^Bolukbasi_et+al_2016_a]: Bolukbasi, T., Chang, K.-W., Zou, J., Saligrama, V., and Kalai, A., ‘Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings’, 30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain, 2016. http://papers.nips.cc/paper/6227-manis-to-computer-programmer-as-woman-is-to-homemaker-debiasing-word-embeddings. [[Bolukbasi_et+al_ComputerProgrammerWomanHomemakerDebiasing_2016]] [OA](http://papers.nips.cc/paper/6227-manis-to-computer-programmer-as-woman-is-to-homemaker-debiasing-word-embeddings)  [Scite](https://engine.scholarcy.com/scite_url?query=Bolukbasi%2C%20T.%20Chang%2C%20K.-W.%20Zou%2C%20J.%20Saligrama%2C%20V.%20Man%20is%20to%20Computer%20Programmer%20as%20Woman%20is%20to%20Homemaker%3F%20Debiasing%20Word%20Embeddings%202016)

[^Bolukbasi_et+al_2016_b]: Bolukbasi, Tolga, Kai-Wei Chang, James Y. Zou, Venkatesh Saligrama, and Adam T. Kalai, "Man is to computer programmer as woman is to homemaker? debiasing word embeddings," In Advances in neural information processing systems, pp. 4349-4357, 2016. [[Bolukbasi_et+al_ComputerProgrammerWomanHomemakerDebiasing_2016]] [OA](https://engine.scholarcy.com/oa_version?query=Bolukbasi%2C%20Tolga%20Chang%2C%20Kai-Wei%20Zou%2C%20James%20Y.%20Saligrama%2C%20Venkatesh%20Man%20is%20to%20computer%20programmer%20as%20woman%20is%20to%20homemaker%3F%20debiasing%20word%20embeddings%2C%202016) [GScholar](https://scholar.google.co.uk/scholar?q=Bolukbasi%2C%20Tolga%20Chang%2C%20Kai-Wei%20Zou%2C%20James%20Y.%20Saligrama%2C%20Venkatesh%20Man%20is%20to%20computer%20programmer%20as%20woman%20is%20to%20homemaker%3F%20debiasing%20word%20embeddings%2C%202016) [Scite](https://engine.scholarcy.com/scite_url?query=Bolukbasi%2C%20Tolga%20Chang%2C%20Kai-Wei%20Zou%2C%20James%20Y.%20Saligrama%2C%20Venkatesh%20Man%20is%20to%20computer%20programmer%20as%20woman%20is%20to%20homemaker%3F%20debiasing%20word%20embeddings%2C%202016)

[^Bonnefon_et+al_2015_a]: Bonnefon, Jean-François, Shariff, Azim & Rahwan, Iyad, “Autonomous Vehicles Need Experimental Ethics: Are We Ready for Utilitarian Cars?” 2015. [[Bonnefon_et+al_AutonomousVehiclesNeedExperimentalEthics_2015]] [OA](https://scholar.google.co.uk/scholar?q=Bonnefon%2C%20Jean-Fran%C3%A7ois%20Shariff%2C%20Azim%20Rahwan%2C%20Iyad%20Autonomous%20Vehicles%20Need%20Experimental%20Ethics%3A%20Are%20We%20Ready%20for%20Utilitarian%20Cars%3F%202015) [GScholar](https://scholar.google.co.uk/scholar?q=Bonnefon%2C%20Jean-Fran%C3%A7ois%20Shariff%2C%20Azim%20Rahwan%2C%20Iyad%20Autonomous%20Vehicles%20Need%20Experimental%20Ethics%3A%20Are%20We%20Ready%20for%20Utilitarian%20Cars%3F%202015) 

[^Booth_et+al_2017_a]: Booth, Serena, Tompkin, James & Pfister, Hanspeter et al., “Piggybacking Robots: Human-Robot Overtrust in University Dormitory Security”, Human Robot Interaction, March 2017. [[Booth_et+al_PiggybackingRobotsHumanrobotOvertrustUniversity_2017]] [OA](https://engine.scholarcy.com/oa_version?query=Booth%2C%20Serena%20Tompkin%2C%20James%20Pfister%2C%20Hanspeter%20Piggybacking%20Robots%3A%20Human-Robot%20Overtrust%20in%20University%20Dormitory%20Security%202017-03) [GScholar](https://scholar.google.co.uk/scholar?q=Booth%2C%20Serena%20Tompkin%2C%20James%20Pfister%2C%20Hanspeter%20Piggybacking%20Robots%3A%20Human-Robot%20Overtrust%20in%20University%20Dormitory%20Security%202017-03) [Scite](https://engine.scholarcy.com/scite_url?query=Booth%2C%20Serena%20Tompkin%2C%20James%20Pfister%2C%20Hanspeter%20Piggybacking%20Robots%3A%20Human-Robot%20Overtrust%20in%20University%20Dormitory%20Security%202017-03)

[^Borgesius_2018_a]: Borgesius, Frederik Zuiderveen, Discrimination, Artificial Intelligence and Algorithmic DecisionMaking, 2018. https://rm.coe.int/discrimination-artificial-intelligence-and-algorithmic-decisionmaking/1680925d73 [[Borgesius_DiscriminationArtificialIntelligenceAlgorithmicDecisionmaking_2018]] [OA](https://rm.coe.int/discrimination-artificial-intelligence-and-algorithmic-decisionmaking/1680925d73)  

[^Borzaga_et+al_2019_a]: Borzaga, Carlo, Gianluca Salvatori, and Riccardo Bodini, “Social and Solidarity Economy and the Future of Work,” Journal of Entrepreneurship and Innovation in Emerging Economies, Vol. 5, No. 1, 2019, pp. 37-57. https://www.ilo.org/global/topics/cooperatives/publications/WCMS_573160/lang--en/index.htm. [[Borzaga_et+al_SocialSolidarityEconomyFutureWork_2019]] [OA](https://www.ilo.org/global/topics/cooperatives/publications/WCMS_573160/lang--en/index.htm)  [Scite](https://engine.scholarcy.com/scite_url?query=Borzaga%2C%20Carlo%20Salvatori%2C%20Gianluca%20Bodini%2C%20Riccardo%20Social%20and%20Solidarity%20Economy%20and%20the%20Future%20of%20Work%2C%202019)

[^Bostrom_2003_a]: Bostrom, Nick, “Ethical Issues in Advanced Artificial Intelligence,” 2003. https://nickbostrom.com/ethics/ai.html. [[Bostrom_EthicalIssuesAdvancedArtificialIntelligence_2003]] [OA](https://nickbostrom.com/ethics/ai.html)  

[^Bozdag_2015_a]: Bozdag, Engin, and Jeroen Van Den Hoven, “Breaking the Filter Bubble: Democracy and Design,” Ethics and Information Technology, Vol. 17, No. 4, 2015, pp. 249–265. [[Bozdag_BreakingFilterBubbleDemocracyDesign_2015]] [OA](https://engine.scholarcy.com/oa_version?query=Bozdag%2C%20Engin%20Hoven%2C%20Jeroen%20Van%20Den%20Breaking%20the%20Filter%20Bubble%3A%20Democracy%20and%20Design%2C%202015) [GScholar](https://scholar.google.co.uk/scholar?q=Bozdag%2C%20Engin%20Hoven%2C%20Jeroen%20Van%20Den%20Breaking%20the%20Filter%20Bubble%3A%20Democracy%20and%20Design%2C%202015) [Scite](https://engine.scholarcy.com/scite_url?query=Bozdag%2C%20Engin%20Hoven%2C%20Jeroen%20Van%20Den%20Breaking%20the%20Filter%20Bubble%3A%20Democracy%20and%20Design%2C%202015)

[^Braun_2018_a]: Braun, Ashley. “The RangerBot is a New Line of Defense Against Coral-Eating Crown-of-Thorns Starfish”, Smithsonian, August 2018. [[Braun_RangerbotNewLineDefenseAgainst_2018]] [OA](https://engine.scholarcy.com/oa_version?query=Braun%2C%20Ashley%20The%20RangerBot%20is%20a%20New%20Line%20of%20Defense%20Against%20Coral-Eating%20Crown-of-Thorns%20Starfish%202018-08) [GScholar](https://scholar.google.co.uk/scholar?q=Braun%2C%20Ashley%20The%20RangerBot%20is%20a%20New%20Line%20of%20Defense%20Against%20Coral-Eating%20Crown-of-Thorns%20Starfish%202018-08) [Scite](https://engine.scholarcy.com/scite_url?query=Braun%2C%20Ashley%20The%20RangerBot%20is%20a%20New%20Line%20of%20Defense%20Against%20Coral-Eating%20Crown-of-Thorns%20Starfish%202018-08)

[^Braun_et+al_2018_b]: Braun, Trevor, Benjamin CM Fung, Farkhund Iqbal, and Babar Shah. "Security and privacy challenges in smart cities," Sustainable cities and society, Vol. 39, 2018, pp. 499-507., p. 2 [[Braun_et+al_SecurityPrivacyChallengesSmartCities_2018]] [OA](https://engine.scholarcy.com/oa_version?query=Braun%2C%20Trevor%20Fung%2C%20Benjamin%20C.M.%20Iqbal%2C%20Farkhund%20Shah%2C%20Babar%20Security%20and%20privacy%20challenges%20in%20smart%20cities%2C%202018) [GScholar](https://scholar.google.co.uk/scholar?q=Braun%2C%20Trevor%20Fung%2C%20Benjamin%20C.M.%20Iqbal%2C%20Farkhund%20Shah%2C%20Babar%20Security%20and%20privacy%20challenges%20in%20smart%20cities%2C%202018) [Scite](https://engine.scholarcy.com/scite_url?query=Braun%2C%20Trevor%20Fung%2C%20Benjamin%20C.M.%20Iqbal%2C%20Farkhund%20Shah%2C%20Babar%20Security%20and%20privacy%20challenges%20in%20smart%20cities%2C%202018)

[^Brayne_2017_a]: Brayne, Sarah, “Big Data Surveillance: The Case of Policing,” American Sociological Review, Vol 82, No. 5, 2017. [[Brayne_DataSurveillanceCasePolicing_2017]] [OA](https://engine.scholarcy.com/oa_version?query=Brayne%2C%20Sarah%20Big%20Data%20Surveillance%3A%20The%20Case%20of%20Policing%2C%202017) [GScholar](https://scholar.google.co.uk/scholar?q=Brayne%2C%20Sarah%20Big%20Data%20Surveillance%3A%20The%20Case%20of%20Policing%2C%202017) [Scite](https://engine.scholarcy.com/scite_url?query=Brayne%2C%20Sarah%20Big%20Data%20Surveillance%3A%20The%20Case%20of%20Policing%2C%202017)

[^Breazeal_2003_a]: Breazeal, Cynthia, “Toward sociable robots,” Robotics and Autonomous Systems, Vol. 42, 2003, pp. 167–175. [[Breazeal_TowardSociableRobots_2003]] [OA](https://engine.scholarcy.com/oa_version?query=Breazeal%2C%20Cynthia%20Toward%20sociable%20robots%2C%202003) [GScholar](https://scholar.google.co.uk/scholar?q=Breazeal%2C%20Cynthia%20Toward%20sociable%20robots%2C%202003) [Scite](https://engine.scholarcy.com/scite_url?query=Breazeal%2C%20Cynthia%20Toward%20sociable%20robots%2C%202003)

[^Brey_2010_a]: Brey, P. (2010). Values in Technology and Disclosive Computer Ethics. In L. Floridi (Ed.), The Cambridge Handbook of Information and Computer Ethics (pp. 41-58). Cambridge: Cambridge University Press. [[Brey_ValuesTechnologyDisclosiveComputerEthics_2010]] [OA](https://engine.scholarcy.com/oa_version?query=Brey%2C%20P.%20Values%20in%20Technology%20and%20Disclosive%20Computer%20Ethics%202010) [GScholar](https://scholar.google.co.uk/scholar?q=Brey%2C%20P.%20Values%20in%20Technology%20and%20Disclosive%20Computer%20Ethics%202010) [Scite](https://engine.scholarcy.com/scite_url?query=Brey%2C%20P.%20Values%20in%20Technology%20and%20Disclosive%20Computer%20Ethics%202010)

[^Brey_2012_a]: Brey, P.A.E., “Anticipatory Ethics for Emerging Technologies”, Nanoethics, Vol. 6, 2012, pp. 1–13. [[Brey_AnticipatoryEthicsEmergingTechnologies_2012]] [OA](https://engine.scholarcy.com/oa_version?query=Brey%2C%20P.A.E.%20Anticipatory%20Ethics%20for%20Emerging%20Technologies%202012) [GScholar](https://scholar.google.co.uk/scholar?q=Brey%2C%20P.A.E.%20Anticipatory%20Ethics%20for%20Emerging%20Technologies%202012) [Scite](https://engine.scholarcy.com/scite_url?query=Brey%2C%20P.A.E.%20Anticipatory%20Ethics%20for%20Emerging%20Technologies%202012)

[^Brey_2005_a]: Brey, Philip, “Freedom and Privacy in Ambient Intelligence,” Ethics and Information Technology, Vol. 7, No. 3, 2005, pp. 157–166., p. 8. [[Brey_FreedomPrivacyAmbientIntelligence_2005]] [OA](https://engine.scholarcy.com/oa_version?query=Brey%2C%20Philip%20Freedom%20and%20Privacy%20in%20Ambient%20Intelligence%2C%202005) [GScholar](https://scholar.google.co.uk/scholar?q=Brey%2C%20Philip%20Freedom%20and%20Privacy%20in%20Ambient%20Intelligence%2C%202005) [Scite](https://engine.scholarcy.com/scite_url?query=Brey%2C%20Philip%20Freedom%20and%20Privacy%20in%20Ambient%20Intelligence%2C%202005)

[^Brey_2005_b]: Brey, Philip, “Freedom and Privacy in Ambient Intelligence,” Ethics and Information Technology, Vol. 7, No. 3, 2005, pp. 157–166., p. 9. [[Brey_FreedomPrivacyAmbientIntelligence_2005]] [OA](https://engine.scholarcy.com/oa_version?query=Brey%2C%20Philip%20Freedom%20and%20Privacy%20in%20Ambient%20Intelligence%2C%202005) [GScholar](https://scholar.google.co.uk/scholar?q=Brey%2C%20Philip%20Freedom%20and%20Privacy%20in%20Ambient%20Intelligence%2C%202005) [Scite](https://engine.scholarcy.com/scite_url?query=Brey%2C%20Philip%20Freedom%20and%20Privacy%20in%20Ambient%20Intelligence%2C%202005)

[^Brinton_2017_a]: Brinton, Chris, "A framework for explanation of machine learning decisions," In IJCAI-17 Workshop on Explainable AI (XAI), 2017, pp. 14-18., p. 14 [[Brinton_FrameworkExplanationMachineLearningDecisions_2017]] [OA](https://engine.scholarcy.com/oa_version?query=Brinton%2C%20Chris%20A%20framework%20for%20explanation%20of%20machine%20learning%20decisions%2C%202017) [GScholar](https://scholar.google.co.uk/scholar?q=Brinton%2C%20Chris%20A%20framework%20for%20explanation%20of%20machine%20learning%20decisions%2C%202017) [Scite](https://engine.scholarcy.com/scite_url?query=Brinton%2C%20Chris%20A%20framework%20for%20explanation%20of%20machine%20learning%20decisions%2C%202017)

[^Bronson_2018_a]: Bronson, Kelly, and Knezevic, Irenam “Big Data in Food and Agriculture,” Big Data & Society, 2018, p. 2. [[Bronson_IrenambigDataFoodAgriculture_2018]] [OA](https://engine.scholarcy.com/oa_version?query=Bronson%2C%20Kelly%20Knezevic%20Irenam%20%E2%80%9CBig%20Data%20in%20Food%20and%20Agriculture%2C%E2%80%9D%202018) [GScholar](https://scholar.google.co.uk/scholar?q=Bronson%2C%20Kelly%20Knezevic%20Irenam%20%E2%80%9CBig%20Data%20in%20Food%20and%20Agriculture%2C%E2%80%9D%202018) [Scite](https://engine.scholarcy.com/scite_url?query=Bronson%2C%20Kelly%20Knezevic%20Irenam%20%E2%80%9CBig%20Data%20in%20Food%20and%20Agriculture%2C%E2%80%9D%202018)

[^Brooks_2018_a]: Brooks, Victoria, “Samantha’s Suffering: Why Sex Machines Should Have Rights Too”, The Conversation, April 2018. [[Brooks_SamanthaSufferingSexMachinesShould_2018]] [OA](https://engine.scholarcy.com/oa_version?query=Brooks%2C%20Victoria%20Samantha%E2%80%99s%20Suffering%3A%20Why%20Sex%20Machines%20Should%20Have%20Rights%20Too%202018-04) [GScholar](https://scholar.google.co.uk/scholar?q=Brooks%2C%20Victoria%20Samantha%E2%80%99s%20Suffering%3A%20Why%20Sex%20Machines%20Should%20Have%20Rights%20Too%202018-04) [Scite](https://engine.scholarcy.com/scite_url?query=Brooks%2C%20Victoria%20Samantha%E2%80%99s%20Suffering%3A%20Why%20Sex%20Machines%20Should%20Have%20Rights%20Too%202018-04)

[^Brown_2015_a]: Brown, Robert, “Robots Make A Money-Making Assembly Line by Cutting Costs”, Center for The Future of Work, February 2015. [[Brown_robotsMakeAMoneymakingAssembly_2015]] [OA](https://scholar.google.co.uk/scholar?q=Brown%2C%20Robert%20%E2%80%9CRobots%20Make%20A%20Money-Making%20Assembly%20Line%20by%20Cutting%20Costs%E2%80%9D%2C%20Center%20for%20The%20Future%20of%20Work%202015-02) [GScholar](https://scholar.google.co.uk/scholar?q=Brown%2C%20Robert%20%E2%80%9CRobots%20Make%20A%20Money-Making%20Assembly%20Line%20by%20Cutting%20Costs%E2%80%9D%2C%20Center%20for%20The%20Future%20of%20Work%202015-02) 

[^Brundage_et+al_2018_a]: Brundage, M., Avin, S., Clark, J., Toner, H., Eckersley, P., Garfinkel, B., Dafoe, A., Scharre, P., Zeitzoff, T., Filar, B., Anderson, H., Roff, H., Allen, G. C., Steinhardt, J., Flynn, C., Ó hÉigeartaigh, S., Beard, S., Belfield, H., Farquhar, S., Lyle, C., Crootof, R., Evans, O., Page, M., Bryson, J., Yampolskiy, R., and Amodei, D., ‘The Malicious Use of Artificial Intelligence: Forecasting, Prevention, and Mitigation’, 2018. https://www.eff.org/files/2018/02/20/malicious_ai_report_final.pdf. [[Brundage_et+al_MaliciousUseArtificialIntelligenceForecasting_2018]] [OA](https://www.eff.org/files/2018/02/20/malicious_ai_report_final.pdf)  

[^Brundage_2014_a]: Brundage, Miles, “Limitations and risks of machine ethics,” Journal of Experimental & Theoretical Artificial Intelligence, Vol. 26, No. 3, 2014, pp. 355–372. [[Brundage_LimitationsRisksMachineEthics_2014]] [OA](https://engine.scholarcy.com/oa_version?query=Brundage%2C%20Miles%20Limitations%20and%20risks%20of%20machine%20ethics%2C%202014) [GScholar](https://scholar.google.co.uk/scholar?q=Brundage%2C%20Miles%20Limitations%20and%20risks%20of%20machine%20ethics%2C%202014) [Scite](https://engine.scholarcy.com/scite_url?query=Brundage%2C%20Miles%20Limitations%20and%20risks%20of%20machine%20ethics%2C%202014)

[^Implications_2017_a]: Implications,” Science, American Association for the Advancement of Science, December 22, 2017. http://science.sciencemag.org/content/358/6370/1530. Brynjolfsson, Erik, and Tom Mitchell, “What Can Machine Learning Do? Workforce Implications,” Science, Vol.358, No.6370, 2017, pp.1530–1534. [[Implications_AmericanAssociationAdvancementScience_2017]] [OA](http://science.sciencemag.org/content/358/6370/1530)  [Scite](https://engine.scholarcy.com/scite_url?query=Implications%2C%20%E2%80%9D%20Science%20American%20Association%20for%20the%20Advancement%20of%20Science%202017-12-22)

[^Brynjolfsson_2017_a]: Brynjolfsson, Erik, and Tom Mitchell, “What Can Machine Learning Do? Workforce Implications,” Science, Vol. 358, No. 6370, 2017, pp. 1530–1534.; Litvinski, O. (2018). Algorithmic opacity: a narrative revue. [[Brynjolfsson_WhatCanMachineLearningDo_2017]] [OA](https://engine.scholarcy.com/oa_version?query=Brynjolfsson%2C%20Erik%20Mitchell%2C%20Tom%20What%20Can%20Machine%20Learning%20Do%3F%20Workforce%20Implications%2C%202017) [GScholar](https://scholar.google.co.uk/scholar?q=Brynjolfsson%2C%20Erik%20Mitchell%2C%20Tom%20What%20Can%20Machine%20Learning%20Do%3F%20Workforce%20Implications%2C%202017) [Scite](https://engine.scholarcy.com/scite_url?query=Brynjolfsson%2C%20Erik%20Mitchell%2C%20Tom%20What%20Can%20Machine%20Learning%20Do%3F%20Workforce%20Implications%2C%202017)

[^Bryson_2010_a]: Bryson, J. J. (2010). Robots should be slaves. In Y. Wilks (Ed.), Close engagements with artificial companions: Key social, psychological, ethical and design issues (pp. 63–74). Amsterdam: John Benjamins. Bullington, Joseph, “Affective computing and emotion recognition systems: The future of biometric surveillance?,” Information Security Curriculum Development Conference ‘05, 2005. [[Bryson_RobotsShouldSlaves_2010]] [OA](https://engine.scholarcy.com/oa_version?query=Bryson%2C%20J.J.%20Robots%20should%20be%20slaves%202010) [GScholar](https://scholar.google.co.uk/scholar?q=Bryson%2C%20J.J.%20Robots%20should%20be%20slaves%202010) [Scite](https://engine.scholarcy.com/scite_url?query=Bryson%2C%20J.J.%20Robots%20should%20be%20slaves%202010)

[^Bundy_2006_a]: Bundy, Alan, and Fiona Mcneill, “Representation as a Fluent: An AI Challenge for the Next Half Century,” IEEE Intelligent Systems, Vol. 21, No. 3, 2006, pp. 85–87. https://ieeexplore.ieee.org/abstract/document/1637360 [[Bundy_RepresentationFluentAiChallengeNext_2006]] [OA](https://ieeexplore.ieee.org/abstract/document/1637360)  [Scite](https://engine.scholarcy.com/scite_url?query=Bundy%2C%20Alan%20Mcneill%2C%20Fiona%20Representation%20as%20a%20Fluent%3A%20An%20AI%20Challenge%20for%20the%20Next%20Half%20Century%2C%202006)

[^Buolamwini_2018_a]: Buolamwini, J., and Gebru, T., ‘Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification’, Proceedings of the 1st Conference on Fairness, Accountability, and Transparency, PMLR, Vol. 81, pp. 77-91, 2018. [[Buolamwini_GenderShadesIntersectionalAccuracyDisparities_2018]] [OA](https://engine.scholarcy.com/oa_version?query=Buolamwini%2C%20J.%20Gebru%2C%20T.%20Gender%20Shades%3A%20Intersectional%20Accuracy%20Disparities%20in%20Commercial%20Gender%20Classification%202018) [GScholar](https://scholar.google.co.uk/scholar?q=Buolamwini%2C%20J.%20Gebru%2C%20T.%20Gender%20Shades%3A%20Intersectional%20Accuracy%20Disparities%20in%20Commercial%20Gender%20Classification%202018) [Scite](https://engine.scholarcy.com/scite_url?query=Buolamwini%2C%20J.%20Gebru%2C%20T.%20Gender%20Shades%3A%20Intersectional%20Accuracy%20Disparities%20in%20Commercial%20Gender%20Classification%202018)

[^Buoncompagni_et+al_2018_a]: Buoncompagni, Luca, Capitanelli, Alessio, & Carfi, Alessandro et al., “From Collaborative Robots to Work Mates: A New Perspective on Human-Robot Cooperation”, ERCIM News, July 2018. [[Buoncompagni_et+al_FromCollaborativeRobotsWorkMates_2018]] [OA](https://engine.scholarcy.com/oa_version?query=Buoncompagni%2C%20Luca%20Capitanelli%2C%20Alessio%20Carfi%2C%20Alessandro%20From%20Collaborative%20Robots%20to%20Work%20Mates%3A%20A%20New%20Perspective%20on%20Human-Robot%20Cooperation%202018-07) [GScholar](https://scholar.google.co.uk/scholar?q=Buoncompagni%2C%20Luca%20Capitanelli%2C%20Alessio%20Carfi%2C%20Alessandro%20From%20Collaborative%20Robots%20to%20Work%20Mates%3A%20A%20New%20Perspective%20on%20Human-Robot%20Cooperation%202018-07) [Scite](https://engine.scholarcy.com/scite_url?query=Buoncompagni%2C%20Luca%20Capitanelli%2C%20Alessio%20Carfi%2C%20Alessandro%20From%20Collaborative%20Robots%20to%20Work%20Mates%3A%20A%20New%20Perspective%20on%20Human-Robot%20Cooperation%202018-07)

[^Burgess_2018_a]: Burgess, Matt, “Now DeepMind's AI can spot eye disease just as well as your doctor”, Wired, August 13, 2018. https://www.wired.co.uk/article/deepmind-moorfields-ai-eye-nhs Burke, “Knowledge-Based Recommender Systems”. Burr, Chistopher, Nello Cristianini, and James Ladyman, “An Analysis of the Interaction Between Intelligent Software Agents and Human Users,” Minds and Machines, Vol.28, No.4, 2018, pp.735–774. [[Burgess_DeepmindAiSpotDiseaseJust_2018]] [OA](https://www.wired.co.uk/article/deepmind-moorfields-ai-eye-nhs)  [Scite](https://engine.scholarcy.com/scite_url?query=Burgess%2C%20Matt%20Now%20DeepMind%27s%20AI%20can%20spot%20eye%20disease%20just%20as%20well%20as%20your%20doctor%202018-08-13)

[^Burrell_2016_a]: Burrell, J., ‘How the Machine ‘Thinks’: Understanding Opacity in Machin Learning Algorithms’, Big Data & Society, Vol. 3, No. 1, June 2016. [[Burrell_MachinethinksUnderstandingOpacityMachin_2016]] [OA](https://engine.scholarcy.com/oa_version?query=Burrell%2C%20J.%20How%20the%20Machine%20%E2%80%98Thinks%E2%80%99%3A%20Understanding%20Opacity%20in%20Machin%20Learning%20Algorithms%202016-06) [GScholar](https://scholar.google.co.uk/scholar?q=Burrell%2C%20J.%20How%20the%20Machine%20%E2%80%98Thinks%E2%80%99%3A%20Understanding%20Opacity%20in%20Machin%20Learning%20Algorithms%202016-06) [Scite](https://engine.scholarcy.com/scite_url?query=Burrell%2C%20J.%20How%20the%20Machine%20%E2%80%98Thinks%E2%80%99%3A%20Understanding%20Opacity%20in%20Machin%20Learning%20Algorithms%202016-06)

[^Burrell_2015_a]: Burrell, Jenna, “How the Machine Thinks: Understanding Opacity in Machine Learning Algorithms,” Big Data & Society, Vol. 3, No. 1, 2015, p. 2053951715622512. [[Burrell_MachineThinksUnderstandingOpacityMachine_2015]] [OA](https://engine.scholarcy.com/oa_version?query=Burrell%2C%20Jenna%20How%20the%20Machine%20Thinks%3A%20Understanding%20Opacity%20in%20Machine%20Learning%20Algorithms%2C%202015) [GScholar](https://scholar.google.co.uk/scholar?q=Burrell%2C%20Jenna%20How%20the%20Machine%20Thinks%3A%20Understanding%20Opacity%20in%20Machine%20Learning%20Algorithms%2C%202015) [Scite](https://engine.scholarcy.com/scite_url?query=Burrell%2C%20Jenna%20How%20the%20Machine%20Thinks%3A%20Understanding%20Opacity%20in%20Machine%20Learning%20Algorithms%2C%202015)

[^Cammozzo_2011_a]: Cammozzo, A., ‘Face Recognition and Privacy Enhancing Techniques’, in Bissett, A., Bynum, T. W., Light, A., Lauener, A., and Rogerson, S., ETHICOMP 2011: The Social Impact of Social Computing, Sheffield, UK, Sheffield Hallam University, pp. 101- 109, 2011. [[Cammozzo_FaceRecognitionPrivacyEnhancingTechniques_2011]] [OA](https://engine.scholarcy.com/oa_version?query=Cammozzo%2C%20A.%20Face%20Recognition%20and%20Privacy%20Enhancing%20Techniques%202011) [GScholar](https://scholar.google.co.uk/scholar?q=Cammozzo%2C%20A.%20Face%20Recognition%20and%20Privacy%20Enhancing%20Techniques%202011) [Scite](https://engine.scholarcy.com/scite_url?query=Cammozzo%2C%20A.%20Face%20Recognition%20and%20Privacy%20Enhancing%20Techniques%202011)

[^Cangelosi_2018_a]: Cangelosi, Angelo & Schlesinger, Matthew, “From Babies to Robots: The Contribution of Developmental Robotics to Developmental Psychology”, Child Development Perspectives, February 2018. [[Cangelosi_FromBabiesRobotsContributionDevelopmental_2018]] [OA](https://engine.scholarcy.com/oa_version?query=Cangelosi%2C%20Angelo%20Schlesinger%2C%20Matthew%20From%20Babies%20to%20Robots%3A%20The%20Contribution%20of%20Developmental%20Robotics%20to%20Developmental%20Psychology%202018-02) [GScholar](https://scholar.google.co.uk/scholar?q=Cangelosi%2C%20Angelo%20Schlesinger%2C%20Matthew%20From%20Babies%20to%20Robots%3A%20The%20Contribution%20of%20Developmental%20Robotics%20to%20Developmental%20Psychology%202018-02) [Scite](https://engine.scholarcy.com/scite_url?query=Cangelosi%2C%20Angelo%20Schlesinger%2C%20Matthew%20From%20Babies%20to%20Robots%3A%20The%20Contribution%20of%20Developmental%20Robotics%20to%20Developmental%20Psychology%202018-02)

[^Capgemini_2014_a]: Capgemini, “Microbots: Innovation in Healthcare”, 2 Dec 2014. https://www.capgemini.com/2014/12/microbots-innovation-in-healthcare-0/ [[Capgemini_MicrobotsInnovationHealthcare_2014]] [OA](https://www.capgemini.com/2014/12/microbots-innovation-in-healthcare-0/)  

[^Carbonell_2016_a]: Carbonell, Isabelle M., “The Ethics of Big Data in Big Agriculture,” Internet Policy Review, Vol. 5, No. 1, 2016, p. 3. [[Carbonell_EthicsBigDataBigAgriculture_2016]] [OA](https://engine.scholarcy.com/oa_version?query=Carbonell%2C%20Isabelle%20M.%20The%20Ethics%20of%20Big%20Data%20in%20Big%20Agriculture%2C%202016) [GScholar](https://scholar.google.co.uk/scholar?q=Carbonell%2C%20Isabelle%20M.%20The%20Ethics%20of%20Big%20Data%20in%20Big%20Agriculture%2C%202016) [Scite](https://engine.scholarcy.com/scite_url?query=Carbonell%2C%20Isabelle%20M.%20The%20Ethics%20of%20Big%20Data%20in%20Big%20Agriculture%2C%202016)

[^Carlini_et+al_2016_a]: Carlini, Nicholas, Pratyush Mishra, Tavish Vaidya, Yuankai Zhang, Micah Sherr, Clay Shields, David Wagner, and Wenchao Zhou, "Hidden voice commands," 25th {USENIX} Security Symposium ({USENIX} Security 16), pp. 513-530, 2016., p. 513 [[Carlini_et+al_HiddenVoiceCommands_2016]] [OA](https://engine.scholarcy.com/oa_version?query=Carlini%2C%20Nicholas%20Mishra%2C%20Pratyush%20Vaidya%2C%20Tavish%20Zhang%2C%20Yuankai%20Hidden%20voice%20commands%2C%202016) [GScholar](https://scholar.google.co.uk/scholar?q=Carlini%2C%20Nicholas%20Mishra%2C%20Pratyush%20Vaidya%2C%20Tavish%20Zhang%2C%20Yuankai%20Hidden%20voice%20commands%2C%202016) [Scite](https://engine.scholarcy.com/scite_url?query=Carlini%2C%20Nicholas%20Mishra%2C%20Pratyush%20Vaidya%2C%20Tavish%20Zhang%2C%20Yuankai%20Hidden%20voice%20commands%2C%202016)

[^Carlisle_2017_a]: Carlisle, Brian, “Pick and Place for Profit: Using Robot Labor to Save Money”, Robotics Business Review, September 2017. [[Carlisle_PickPlaceProfitUsingRobot_2017]] [OA](https://engine.scholarcy.com/oa_version?query=Carlisle%2C%20Brian%20Pick%20and%20Place%20for%20Profit%3A%20Using%20Robot%20Labor%20to%20Save%20Money%202017-09) [GScholar](https://scholar.google.co.uk/scholar?q=Carlisle%2C%20Brian%20Pick%20and%20Place%20for%20Profit%3A%20Using%20Robot%20Labor%20to%20Save%20Money%202017-09) [Scite](https://engine.scholarcy.com/scite_url?query=Carlisle%2C%20Brian%20Pick%20and%20Place%20for%20Profit%3A%20Using%20Robot%20Labor%20to%20Save%20Money%202017-09)

[^Carnevale_2015_a]: Carnevale, A., “Robots, Disability, and Good Human Life”, Disability Studies Quarterly, Vol. 35, No. 1, 2015. [[Carnevale_RobotsDisabilityGoodHumanLife_2015]] [OA](https://engine.scholarcy.com/oa_version?query=Carnevale%2C%20A.%20Robots%2C%20Disability%2C%20and%20Good%20Human%20Life%202015) [GScholar](https://scholar.google.co.uk/scholar?q=Carnevale%2C%20A.%20Robots%2C%20Disability%2C%20and%20Good%20Human%20Life%202015) [Scite](https://engine.scholarcy.com/scite_url?query=Carnevale%2C%20A.%20Robots%2C%20Disability%2C%20and%20Good%20Human%20Life%202015)

[^Carolan_2019_a]: Carolan 2015 quoted in Mark Ryan, “Ethics of Using AI and Big Data in Agriculture: The Case of a Large Agriculture Multinational,” ORBIT Journal, Vol. 2, No. 2, 2019, p. 6. https://doi.org/10.29297/orbit.v2i2.109 [[Carolan_Carolan2015QuotedMarkRyan_2019]] [OA](https://doi.org/10.29297/orbit.v2i2.109)  [Scite](https://scite.ai/reports/10.29297/orbit.v2i2.109)

[^Castelvecchi_0000_a]: Castelvecchi, D., ‘The Black Box of AI’, Nature, Vol. 538, No. 7623, pp. 20-23, 6 [[Castelvecchi_BlackBoxAi_0000]] [OA](https://engine.scholarcy.com/oa_version?query=Castelvecchi%2C%20D.%20The%20Black%20Box%20of%20AI) [GScholar](https://scholar.google.co.uk/scholar?q=Castelvecchi%2C%20D.%20The%20Black%20Box%20of%20AI) [Scite](https://engine.scholarcy.com/scite_url?query=Castelvecchi%2C%20D.%20The%20Black%20Box%20of%20AI)

[^Ethics,”_2019_b]: Ethics,” Proceedings of the IEEE, Vol. 107, No. 3, 2019, pp. 562–574. [[Ethics,”_Ethics_2019]] [OA](https://engine.scholarcy.com/oa_version?query=Ethics%20Proceedings%20of%20the%20IEEE%20Vol%20107%20No%203%202019%20pp%20562574) [GScholar](https://scholar.google.co.uk/scholar?q=Ethics%20Proceedings%20of%20the%20IEEE%20Vol%20107%20No%203%202019%20pp%20562574) [Scite](https://engine.scholarcy.com/scite_url?query=Ethics%20Proceedings%20of%20the%20IEEE%20Vol%20107%20No%203%202019%20pp%20562574)

[^Care_2018_a]: Care — Addressing Ethical Challenges,” New England Journal of Medicine, Vol. 378, No. 11, March 2018, p. 3. [[Care_CareAddressingEthicalChallenges_2018]] [OA](https://engine.scholarcy.com/oa_version?query=Care%20%20Addressing%20Ethical%20Challenges%20New%20England%20Journal%20of%20Medicine%20Vol%20378%20No%2011%20March%202018%20p%203) [GScholar](https://scholar.google.co.uk/scholar?q=Care%20%20Addressing%20Ethical%20Challenges%20New%20England%20Journal%20of%20Medicine%20Vol%20378%20No%2011%20March%202018%20p%203) [Scite](https://engine.scholarcy.com/scite_url?query=Care%20%20Addressing%20Ethical%20Challenges%20New%20England%20Journal%20of%20Medicine%20Vol%20378%20No%2011%20March%202018%20p%203)

[^Charisi_2017_a]: Charisi, Vicky, Louise Dennis, Michael Fisher, Robert Lieck, Andreas Matthias, Marija Slavkovik, Janina Loh (Sombetzki), Alan F.T. Winfield, and Roman Yampolskiy, “Towards moral autonomous systems.” Cornell University arXiv.org, 2017. https://arxiv.org/abs/1703.04741 Chatila, Raja, “Inclusion of Humanoid Robots in Human Society: Ethical Issues”, Humanoid Robots: A Reference, October 2017. [[Charisi_TowardsMoralAutonomousSystems_2017]] [OA](https://arxiv.org/abs/1703.04741)  

[^Chesney_2018_a]: Chesney, B., and Citron, D., ‘Deep Fakes: A Looming Challenge for Privacy, Democracy, and National Security’, SSRN, 2018. https://ssrn.com/abstract=3213954 Choudhary, Harding, and Tiwari, “Data Mining in Manufacturing”. Christman, John, “Autonomy in Moral and Political Philosophy,” Stanford Encyclopedia of Philosophy, Edward N. Zalta, January 9, 2015.https://plato.stanford.edu/entries/autonomy-moral/ Chung, Hyunji, Michaela Iorga, Jeffrey Voas, and Sangjin Lee, "Alexa, can I trust you?," Computer, Vol 50, no.9, 2017, pp.100-104. Civil Rights Groups, Predictive Policing Today: A Shared Statement of Civil Rights Concerns, 31 August 2016 <https://www.aclu.org/other/statement-concern-about-predictive-policing-aclu-and-16civil-rights-privacy-racial-justice> [accessed 1 July 2019]. Clarke, Roger, Introduction to Dataveillance and Information Privacy, and Definitions of Terms, 1997.http://www.rogerclarke.com/DV/Intro.htm [[Chesney_DeepFakesLoomingChallengePrivacy_2018]] [OA](https://ssrn.com/abstract=3213954)  [Scite](https://engine.scholarcy.com/scite_url?query=Chesney%2C%20B.%20Citron%2C%20D.%20Deep%20Fakes%3A%20A%20Looming%20Challenge%20for%20Privacy%2C%20Democracy%2C%20and%20National%20Security%202018-08-31)

[^Coeckelbergh_2015_a]: Coeckelbergh, Mark, “Care robots and the future of ICT-mediated elderly care: a response to doom scenarios,” AI & Society, Vol. 31, No. 4, 2015, pp. 455–462. [[Coeckelbergh_CareRobotsFutureIctmediatedElderly_2015]] [OA](https://engine.scholarcy.com/oa_version?query=Coeckelbergh%2C%20Mark%20Care%20robots%20and%20the%20future%20of%20ICT-mediated%20elderly%20care%3A%20a%20response%20to%20doom%20scenarios%2C%202015) [GScholar](https://scholar.google.co.uk/scholar?q=Coeckelbergh%2C%20Mark%20Care%20robots%20and%20the%20future%20of%20ICT-mediated%20elderly%20care%3A%20a%20response%20to%20doom%20scenarios%2C%202015) [Scite](https://engine.scholarcy.com/scite_url?query=Coeckelbergh%2C%20Mark%20Care%20robots%20and%20the%20future%20of%20ICT-mediated%20elderly%20care%3A%20a%20response%20to%20doom%20scenarios%2C%202015)

[^Coeckelbergh_2010_a]: Coeckelbergh, Mark, “Health Care, Capabilities, and AI Assistive Technologies,” Ethical Theory and Moral Practice, Vol. 13, 2010. [[Coeckelbergh_HealthCareCapabilitiesAiAssistive_2010]] [OA](https://engine.scholarcy.com/oa_version?query=Coeckelbergh%2C%20Mark%20Health%20Care%2C%20Capabilities%2C%20and%20AI%20Assistive%20Technologies%2C%202010) [GScholar](https://scholar.google.co.uk/scholar?q=Coeckelbergh%2C%20Mark%20Health%20Care%2C%20Capabilities%2C%20and%20AI%20Assistive%20Technologies%2C%202010) [Scite](https://engine.scholarcy.com/scite_url?query=Coeckelbergh%2C%20Mark%20Health%20Care%2C%20Capabilities%2C%20and%20AI%20Assistive%20Technologies%2C%202010)

[^Cohen_2019_a]: Cohen, Noam, “Will California’s New Bot Law Strengthen Democracy?,” The New Yorker, 2 July 2019. https://www.newyorker.com/tech/annals-of-technology/will-californias-new-bot-law-strengthendemocracy [[Cohen_WillCaliforniaNewBotLaw_2019]] [OA](https://www.newyorker.com/tech/annals-of-technology/will-californias-new-bot-law-strengthendemocracy)  [Scite](https://engine.scholarcy.com/scite_url?query=Cohen%2C%20Noam%20Will%20California%E2%80%99s%20New%20Bot%20Law%20Strengthen%20Democracy%3F%2C%202019-07-02)

[^Collectif_2018_a]: Collectif, Cerna. "Research Ethics in Machine Learning," PhD diss., CERNA; ALLISTENE, 2018. Conitzer et al., “Moral Decision Making Frameworks for Artificial Intelligence”, 4834. [[Collectif_ResearchEthicsMachineLearning_2018]] [OA](https://engine.scholarcy.com/oa_version?query=Collectif%2C%20Cerna%20Research%20Ethics%20in%20Machine%20Learning%2C%202018) [GScholar](https://scholar.google.co.uk/scholar?q=Collectif%2C%20Cerna%20Research%20Ethics%20in%20Machine%20Learning%2C%202018) [Scite](https://engine.scholarcy.com/scite_url?query=Collectif%2C%20Cerna%20Research%20Ethics%20in%20Machine%20Learning%2C%202018)

[^Coughlin_2017_a]: Coughlin, Joseph F., “The 'Internet of Things' Will Take Nudge Theory Too Far,” Big Think, March 27, 2017. https://bigthink.com/disruptive-demographics/the-internet-of-things-big-data-when-anudge-becomes-a-noodge. Council of Europe, European Social Charter (Revised), 3 May 1996, ETS 163.https://rm.coe.int/168007cf93. [[Coughlin_internetThingsWillTakeNudge_2017]] [OA](https://bigthink.com/disruptive-demographics/the-internet-of-things-big-data-when-anudge-becomes-a-noodge)  [Scite](https://engine.scholarcy.com/scite_url?query=Coughlin%2C%20Joseph%20F.%20The%20%27Internet%20of%20Things%27%20Will%20Take%20Nudge%20Theory%20Too%20Far%2C%202017-03-27)

[^Cowie_2015_a]: Cowie, Roddie, “Ethical issues in affective computing,” In The Oxford handbook of affective computing, Oxford Library of Psychology, 2015. [[Cowie_EthicalIssuesAffectiveComputing_2015]] [OA](https://engine.scholarcy.com/oa_version?query=Cowie%2C%20Roddie%20Ethical%20issues%20in%20affective%20computing%2C%202015) [GScholar](https://scholar.google.co.uk/scholar?q=Cowie%2C%20Roddie%20Ethical%20issues%20in%20affective%20computing%2C%202015) [Scite](https://engine.scholarcy.com/scite_url?query=Cowie%2C%20Roddie%20Ethical%20issues%20in%20affective%20computing%2C%202015)

[^Coyle_et+al_2018_a]: Coyle, Stephen, Majidi, Carmel, LeDuc, Philip & Hsia, Jimmy, “Bio-Inspired Soft Robotics: Material Selection, Actuation, and Design”, Extreme Mechanics 22, July 2018. [[Coyle_et+al_inspiredSoftRoboticsMaterialSelection_2018]] [OA](https://engine.scholarcy.com/oa_version?query=Coyle%2C%20Stephen%20Majidi%2C%20Carmel%20LeDuc%2C%20Philip%20Hsia%2C%20Jimmy%20Bio-Inspired%20Soft%20Robotics%3A%20Material%20Selection%2C%20Actuation%2C%20and%20Design%202018-07) [GScholar](https://scholar.google.co.uk/scholar?q=Coyle%2C%20Stephen%20Majidi%2C%20Carmel%20LeDuc%2C%20Philip%20Hsia%2C%20Jimmy%20Bio-Inspired%20Soft%20Robotics%3A%20Material%20Selection%2C%20Actuation%2C%20and%20Design%202018-07) [Scite](https://engine.scholarcy.com/scite_url?query=Coyle%2C%20Stephen%20Majidi%2C%20Carmel%20LeDuc%2C%20Philip%20Hsia%2C%20Jimmy%20Bio-Inspired%20Soft%20Robotics%3A%20Material%20Selection%2C%20Actuation%2C%20and%20Design%202018-07)

[^Crawford_2016_a]: Crawford, K., ‘A.I.'s White Guy Problem’, The New York Times, p. SR11, June 26, 2016. https://www.nytimes.com/2016/06/26/opinion/sunday/artificial-intelligences-white-guyproblem.html. [[Crawford_IWhiteGuyProblem_2016]] [OA](https://www.nytimes.com/2016/06/26/opinion/sunday/artificial-intelligences-white-guyproblem.html)  [Scite](https://engine.scholarcy.com/scite_url?query=Crawford%2C%20K.%20A.%20I.%27s%20White%20Guy%20Problem%202016-06-26)

[^Crawford_2019_a]: Crawford, Matthew B., “Algorithmic Governance and Political Legitimacy,” American Affairs, Vol. III, No. 2, 2019. [[Crawford_AlgorithmicGovernancePoliticalLegitimacy_2019]] [OA](https://engine.scholarcy.com/oa_version?query=Crawford%2C%20Matthew%20B.%20Algorithmic%20Governance%20and%20Political%20Legitimacy%2C%202019) [GScholar](https://scholar.google.co.uk/scholar?q=Crawford%2C%20Matthew%20B.%20Algorithmic%20Governance%20and%20Political%20Legitimacy%2C%202019) [Scite](https://engine.scholarcy.com/scite_url?query=Crawford%2C%20Matthew%20B.%20Algorithmic%20Governance%20and%20Political%20Legitimacy%2C%202019)

[^Crouch_2012_a]: Crouch, Will, “The most important unsolved problems in ethics,” 2012. http://blog.practicalethics.ox.ac.uk/2012/10/the-most-important-unsolvedproblems-in-ethics-orhow-to-be-a-high-impact-philosopher-part-iii/ Çürüklü, Baran, Dodig-Crnkovic, Gordana, & Akan, Batu, “Towards Industrial Robots with Human-like Moral Responsibilities”, Human-Robot Interaction (HRI), 2010 5th ACM/IEEE International Conference, April 2010. [[Crouch_MostImportantUnsolvedProblemsEthics_2012]] [OA](http://blog.practicalethics.ox.ac.uk/2012/10/the-most-important-unsolvedproblems-in-ethics-orhow-to-be-a-high-impact-philosopher-part-iii/Çürüklü)  [Scite](https://engine.scholarcy.com/scite_url?query=Crouch%2C%20Will%20The%20most%20important%20unsolved%20problems%20in%20ethics%2C%202012-04)

[^Cushman_et+al_2012_a]: Cushman, Fiery, Liane Young, and Joshua Greene, “Multi-system moral psychology,” In The moral psychology handbook, J. M. Doris & the Moral Psychology Research Group, Eds., New York, NY: Oxford University Press, 2012, pp. 47–71. [[Cushman_et+al_multisystemMoralPsychologyInThe_2012]] [OA](https://scholar.google.co.uk/scholar?q=Cushman%2C%20Fiery%20Young%2C%20Liane%20Greene%2C%20Joshua%20%E2%80%9CMulti-system%20moral%20psychology%2C%E2%80%9D%20In%20The%20moral%20psychology%20handbook%2C%20J.%20M.%20Doris%20%26%20the%20Moral%20Psychology%20Research%20Group%202012) [GScholar](https://scholar.google.co.uk/scholar?q=Cushman%2C%20Fiery%20Young%2C%20Liane%20Greene%2C%20Joshua%20%E2%80%9CMulti-system%20moral%20psychology%2C%E2%80%9D%20In%20The%20moral%20psychology%20handbook%2C%20J.%20M.%20Doris%20%26%20the%20Moral%20Psychology%20Research%20Group%202012) 

[^Custers_2016_a]: Custers, Bart. "Drones Here, There and Everywhere Introduction and Overview." In The Future of Drone Use, pp. 3-20. TMC Asser Press, The Hague, 2016. [[Custers_dronesHereThereEverywhereIntroduction_2016]] [OA](https://scholar.google.co.uk/scholar?q=Custers%2C%20Bart%20%22Drones%20Here%2C%20There%20and%20Everywhere%20Introduction%20and%20Overview.%22%20In%20The%20Future%20of%20Drone%20Use%202016) [GScholar](https://scholar.google.co.uk/scholar?q=Custers%2C%20Bart%20%22Drones%20Here%2C%20There%20and%20Everywhere%20Introduction%20and%20Overview.%22%20In%20The%20Future%20of%20Drone%20Use%202016) 

[^Daerden_2002_a]: Daerden, Frank, and Dirk Lefeber. "Pneumatic artificial muscles: actuators for robotics and automation." European journal of mechanical and environmental engineering 47, no. 1 (2002): 1121. [[Daerden_PneumaticArtificialMusclesActuatorsRobotics_2002]] [OA](https://engine.scholarcy.com/oa_version?query=Daerden%2C%20Frank%20Lefeber%2C%20Dirk%20Pneumatic%20artificial%20muscles%3A%20actuators%20for%20robotics%20and%20automation.%202002) [GScholar](https://scholar.google.co.uk/scholar?q=Daerden%2C%20Frank%20Lefeber%2C%20Dirk%20Pneumatic%20artificial%20muscles%3A%20actuators%20for%20robotics%20and%20automation.%202002) [Scite](https://engine.scholarcy.com/scite_url?query=Daerden%2C%20Frank%20Lefeber%2C%20Dirk%20Pneumatic%20artificial%20muscles%3A%20actuators%20for%20robotics%20and%20automation.%202002)

[^Dalkir_2016_a]: Dalkir, Knowledge Management in Theory and Practice, 217–244. Dan Gettinger and Arthur Holland Michel, Law Enforcement Robots Datasheet, Center for the Study of the Drone, Bard College, 2016; Peter Asaro, ‘“Hands Up, Don’t Shoot!” HRI and the Automation of Police Use of Force’, Journal of Human-Robot Interaction, Vol. 5, No. 3, 2016, pp. 55–56. [[Dalkir_KnowledgeManagementTheoryPractice_2016]] [OA](https://engine.scholarcy.com/oa_version?query=Dalkir%20Knowledge%20Management%20in%20Theory%20and%20Practice%202016) [GScholar](https://scholar.google.co.uk/scholar?q=Dalkir%20Knowledge%20Management%20in%20Theory%20and%20Practice%202016) [Scite](https://engine.scholarcy.com/scite_url?query=Dalkir%20Knowledge%20Management%20in%20Theory%20and%20Practice%202016)

[^Danaher_2014_a]: Danaher, John, “Robotic Rape and Robotic Child Sexual Abuse: Should They be Criminalised?”, Criminal Law and Philosophy, December 2014. [[Danaher_RoboticRapeRoboticChildSexual_2014]] [OA](https://engine.scholarcy.com/oa_version?query=Danaher%2C%20John%20Robotic%20Rape%20and%20Robotic%20Child%20Sexual%20Abuse%3A%20Should%20They%20be%20Criminalised%3F%202014-12) [GScholar](https://scholar.google.co.uk/scholar?q=Danaher%2C%20John%20Robotic%20Rape%20and%20Robotic%20Child%20Sexual%20Abuse%3A%20Should%20They%20be%20Criminalised%3F%202014-12) [Scite](https://engine.scholarcy.com/scite_url?query=Danaher%2C%20John%20Robotic%20Rape%20and%20Robotic%20Child%20Sexual%20Abuse%3A%20Should%20They%20be%20Criminalised%3F%202014-12)

[^Danks_2017_a]: Danks, David & London, Alex, “Regulating Autonomous Systems: Beyond Standards”, IEEE Intelligent Systems, 2017. [[Danks_RegulatingAutonomousSystemsBeyondStandards_2017]] [OA](https://engine.scholarcy.com/oa_version?query=Danks%2C%20David%20London%2C%20Alex%20Regulating%20Autonomous%20Systems%3A%20Beyond%20Standards%202017) [GScholar](https://scholar.google.co.uk/scholar?q=Danks%2C%20David%20London%2C%20Alex%20Regulating%20Autonomous%20Systems%3A%20Beyond%20Standards%202017) [Scite](https://engine.scholarcy.com/scite_url?query=Danks%2C%20David%20London%2C%20Alex%20Regulating%20Autonomous%20Systems%3A%20Beyond%20Standards%202017)

[^Dastin_2018_a]: Dastin, Jeffrey, “Amazon scraps secret AI recruiting tool that showed bias against women,” Reuters, 10 October 2018. Retrieves at https://www.reuters.com/article/us-amazon-com-jobsautomation-in...-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G. [[Dastin_AmazonScrapsSecretAiRecruiting_2018]] [OA](https://www.reuters.com/article/us-amazon-com-jobsautomation-in...-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G)  [Scite](https://engine.scholarcy.com/scite_url?query=Dastin%2C%20Jeffrey%20Amazon%20scraps%20secret%20AI%20recruiting%20tool%20that%20showed%20bias%20against%20women%2C%202018-10-10)

[^Datta_et+al_2015_b]: Datta, A., Tschantz M. C. & Datta, A. (2015). Automated Experiments on Ad Privacy Settings A Tale of Opacity, Choice, and Discrimination. Proceedings on Privacy Enhancing Technologies, 2015 (1), 92–112. [[Datta_et+al_AutomatedExperimentsAdPrivacySettings_2015]] [OA](https://engine.scholarcy.com/oa_version?query=Datta%2C%20A.%20C.%2C%20Tschantz%20M.%20Datta%2C%20A.%20Automated%20Experiments%20on%20Ad%20Privacy%20Settings%20A%20Tale%20of%20Opacity%2C%20Choice%2C%20and%20Discrimination%202015) [GScholar](https://scholar.google.co.uk/scholar?q=Datta%2C%20A.%20C.%2C%20Tschantz%20M.%20Datta%2C%20A.%20Automated%20Experiments%20on%20Ad%20Privacy%20Settings%20A%20Tale%20of%20Opacity%2C%20Choice%2C%20and%20Discrimination%202015) [Scite](https://engine.scholarcy.com/scite_url?query=Datta%2C%20A.%20C.%2C%20Tschantz%20M.%20Datta%2C%20A.%20Automated%20Experiments%20on%20Ad%20Privacy%20Settings%20A%20Tale%20of%20Opacity%2C%20Choice%2C%20and%20Discrimination%202015)

[^Datta_et+al_2015_c]: Datta, A., Tschantz, M. C., & Datta, A. (2015). Automated experiments on ad privacy settings. Proceedings on privacy enhancing technologies, 2015(1), 92-112. [[Datta_et+al_AutomatedExperimentsPrivacySettingsProceedings_2015]] [OA](https://engine.scholarcy.com/oa_version?query=Datta%2C%20A.%20Tschantz%2C%20M.C.%20Datta%2C%20A.%20Automated%20experiments%20on%20ad%20privacy%20settings.%20Proceedings%20on%20privacy%20enhancing%202015) [GScholar](https://scholar.google.co.uk/scholar?q=Datta%2C%20A.%20Tschantz%2C%20M.C.%20Datta%2C%20A.%20Automated%20experiments%20on%20ad%20privacy%20settings.%20Proceedings%20on%20privacy%20enhancing%202015) [Scite](https://engine.scholarcy.com/scite_url?query=Datta%2C%20A.%20Tschantz%2C%20M.C.%20Datta%2C%20A.%20Automated%20experiments%20on%20ad%20privacy%20settings.%20Proceedings%20on%20privacy%20enhancing%202015)

[^Daugherty_2019_a]: Daugherty, Paul R., and H. James Wilson, “How Humans and AI Are Working Together in 1,500 Companies,” Harvard Business Review, April 4, 2019. https://hbr.org/2018/07/collaborativeintelligence-humans-and-ai-are-joining-forces. [[Daugherty_HumansAiAreWorkingTogether_2019]] [OA](https://hbr.org/2018/07/collaborativeintelligence-humans-and-ai-are-joining-forces)  [Scite](https://engine.scholarcy.com/scite_url?query=Daugherty%2C%20Paul%20R.%20Wilson%2C%20H.James%20How%20Humans%20and%20AI%20Are%20Working%20Together%20in%201%2C500%20Companies%2C%202019-04-04)

[^Graham_2016_a]: David A. Graham, “The Dallas Shooting and the Advent of Killer Police Robots”, The Atlantic, 8 July 2016. https://www.theatlantic.com/news/archive/2016/07/dallas-police-robot/490478/ [[Graham_DallasShootingAdventKillerPolice_2016]] [OA](https://www.theatlantic.com/news/archive/2016/07/dallas-police-robot/490478/)  [Scite](https://engine.scholarcy.com/scite_url?query=Graham%2C%20David%20A.%20The%20Dallas%20Shooting%20and%20the%20Advent%20of%20Killer%20Police%20Robots%202016-07-08)

[^Gr_et+al_2012_a]: David Grémillet et al., “Robots in Ecology: Welcome to the Machine,” Open Journal of Ecology, Vol. 2, No. 2, 2012, p. 54 [[Gr_et+al_RobotsEcologyWelcomeMachine_2012]] [OA](https://engine.scholarcy.com/oa_version?query=Gr%C3%A9millet%2C%20David%20Robots%20in%20Ecology%3A%20Welcome%20to%20the%20Machine%2C%202012) [GScholar](https://scholar.google.co.uk/scholar?q=Gr%C3%A9millet%2C%20David%20Robots%20in%20Ecology%3A%20Welcome%20to%20the%20Machine%2C%202012) [Scite](https://engine.scholarcy.com/scite_url?query=Gr%C3%A9millet%2C%20David%20Robots%20in%20Ecology%3A%20Welcome%20to%20the%20Machine%2C%202012)

[^Decew_2018_a]: DeCew, Judith, “Privacy,” In Edward N. Zalta (ed.), The Stanford Encyclopedia of Philosophy (Spring 2018 Edition), 2018. https://plato.stanford.edu/archives/spr2018/entries/privacy [[Decew_Privacy_2018]] [OA](https://plato.stanford.edu/archives/spr2018/entries/privacy)  [Scite](https://engine.scholarcy.com/scite_url?query=DeCew%2C%20Judith%20Privacy%2C%202018)

[^Decker_et+al_2017_a]: Decker, Michael, Fischer, Martin & Ott, Ingrid, “Service Robotics and Human Labor: A First Technology Assessment of Substitution and Cooperation”, Elselvier Robotics and Autonomous Systems 87, January 2017. [[Decker_et+al_ServiceRoboticsHumanLaborFirst_2017]] [OA](https://engine.scholarcy.com/oa_version?query=Decker%2C%20Michael%20Fischer%2C%20Martin%20Ott%2C%20Ingrid%20Service%20Robotics%20and%20Human%20Labor%3A%20A%20First%20Technology%20Assessment%20of%20Substitution%20and%20Cooperation%202017-01-87) [GScholar](https://scholar.google.co.uk/scholar?q=Decker%2C%20Michael%20Fischer%2C%20Martin%20Ott%2C%20Ingrid%20Service%20Robotics%20and%20Human%20Labor%3A%20A%20First%20Technology%20Assessment%20of%20Substitution%20and%20Cooperation%202017-01-87) [Scite](https://engine.scholarcy.com/scite_url?query=Decker%2C%20Michael%20Fischer%2C%20Martin%20Ott%2C%20Ingrid%20Service%20Robotics%20and%20Human%20Labor%3A%20A%20First%20Technology%20Assessment%20of%20Substitution%20and%20Cooperation%202017-01-87)

[^Dempsey_2015_a]: Dempsey, Caitlin, “Drones and GIS: A Look at the Legal and Ethical Issues”, GIS Lounge, September 2015. [[Dempsey_DronesGisLookLegalEthical_2015]] [OA](https://engine.scholarcy.com/oa_version?query=Dempsey%2C%20Caitlin%20Drones%20and%20GIS%3A%20A%20Look%20at%20the%20Legal%20and%20Ethical%20Issues%202015-09) [GScholar](https://scholar.google.co.uk/scholar?q=Dempsey%2C%20Caitlin%20Drones%20and%20GIS%3A%20A%20Look%20at%20the%20Legal%20and%20Ethical%20Issues%202015-09) [Scite](https://engine.scholarcy.com/scite_url?query=Dempsey%2C%20Caitlin%20Drones%20and%20GIS%3A%20A%20Look%20at%20the%20Legal%20and%20Ethical%20Issues%202015-09)

[^Denicolai_et+al_2017_a]: Denicolai, Lorzenzo, Grimaldi & Palmieri, Silvia, “Videos, Educational Robotics and Puppets: An Experimental Integration of Languages”, Universita Degli Studi Di Torino, 2017. [[Denicolai_et+al_VideosEducationalRoboticsPuppetsExperimental_2017]] [OA](https://engine.scholarcy.com/oa_version?query=Denicolai%2C%20Lorzenzo%20Grimaldi%20Palmieri%2C%20Silvia%20Videos%2C%20Educational%20Robotics%20and%20Puppets%3A%20An%20Experimental%20Integration%20of%20Languages%202017) [GScholar](https://scholar.google.co.uk/scholar?q=Denicolai%2C%20Lorzenzo%20Grimaldi%20Palmieri%2C%20Silvia%20Videos%2C%20Educational%20Robotics%20and%20Puppets%3A%20An%20Experimental%20Integration%20of%20Languages%202017) [Scite](https://engine.scholarcy.com/scite_url?query=Denicolai%2C%20Lorzenzo%20Grimaldi%20Palmieri%2C%20Silvia%20Videos%2C%20Educational%20Robotics%20and%20Puppets%3A%20An%20Experimental%20Integration%20of%20Languages%202017)

[^Dennis_et+al_2016_a]: Dennis, Louise, Michael Fisher, Marija Slavkovik, and Matt Webster, “Formal Verification of Ethical Choices in Autonomous Systems,” Robotics and Autonomous Systems, Vol. 77, 2016, pp. 1–14., p. 1 [[Dennis_et+al_FormalVerificationEthicalChoicesAutonomous_2016]] [OA](https://engine.scholarcy.com/oa_version?query=Dennis%2C%20Louise%20Fisher%2C%20Michael%20Slavkovik%2C%20Marija%20Webster%2C%20Matt%20Formal%20Verification%20of%20Ethical%20Choices%20in%20Autonomous%20Systems%2C%202016) [GScholar](https://scholar.google.co.uk/scholar?q=Dennis%2C%20Louise%20Fisher%2C%20Michael%20Slavkovik%2C%20Marija%20Webster%2C%20Matt%20Formal%20Verification%20of%20Ethical%20Choices%20in%20Autonomous%20Systems%2C%202016) [Scite](https://engine.scholarcy.com/scite_url?query=Dennis%2C%20Louise%20Fisher%2C%20Michael%20Slavkovik%2C%20Marija%20Webster%2C%20Matt%20Formal%20Verification%20of%20Ethical%20Choices%20in%20Autonomous%20Systems%2C%202016)

[^Dhir_et+al_2018_a]: Dhir, Amandeep, Yossatorn, Yossiri, Kaur, Puneet, & Chen, Sufen, “Online Social Media Fatigue and Psychological Wellbeing- A Study of Comulsive Use, Fear of Mission Out, Fatigue, Anxiety and Depression”, Elselvier International Journal of Information Management, June 2018. [[Dhir_et+al_OnlineSocialMediaFatiguePsychological_2018]] [OA](https://engine.scholarcy.com/oa_version?query=Dhir%2C%20Amandeep%20Yossatorn%2C%20Yossiri%20Kaur%2C%20Puneet%20Chen%2C%20Sufen%20Online%20Social%20Media%20Fatigue%20and%20Psychological%20Wellbeing-%20A%20Study%20of%20Comulsive%20Use%2C%20Fear%20of%20Mission%20Out%2C%20Fatigue%2C%20Anxiety%20and%20Depression%202018-06) [GScholar](https://scholar.google.co.uk/scholar?q=Dhir%2C%20Amandeep%20Yossatorn%2C%20Yossiri%20Kaur%2C%20Puneet%20Chen%2C%20Sufen%20Online%20Social%20Media%20Fatigue%20and%20Psychological%20Wellbeing-%20A%20Study%20of%20Comulsive%20Use%2C%20Fear%20of%20Mission%20Out%2C%20Fatigue%2C%20Anxiety%20and%20Depression%202018-06) [Scite](https://engine.scholarcy.com/scite_url?query=Dhir%2C%20Amandeep%20Yossatorn%2C%20Yossiri%20Kaur%2C%20Puneet%20Chen%2C%20Sufen%20Online%20Social%20Media%20Fatigue%20and%20Psychological%20Wellbeing-%20A%20Study%20of%20Comulsive%20Use%2C%20Fear%20of%20Mission%20Out%2C%20Fatigue%2C%20Anxiety%20and%20Depression%202018-06)

[^Diakopoulos_2016_a]: Diakopoulos, Nicholas, "Accountability in algorithmic decision making," Communications of the ACM, Vol. 59, No. 2, 2016, pp. 56-62. [[Diakopoulos_AccountabilityAlgorithmicDecisionMaking_2016]] [OA](https://engine.scholarcy.com/oa_version?query=Diakopoulos%2C%20Nicholas%20Accountability%20in%20algorithmic%20decision%20making%2C%202016) [GScholar](https://scholar.google.co.uk/scholar?q=Diakopoulos%2C%20Nicholas%20Accountability%20in%20algorithmic%20decision%20making%2C%202016) [Scite](https://engine.scholarcy.com/scite_url?query=Diakopoulos%2C%20Nicholas%20Accountability%20in%20algorithmic%20decision%20making%2C%202016)

[^Diakopoulos_2015_a]: Diakopoulos, Nicholas, “Accountability in Algorithmic Decision-Making,” Queue, Vol. 13, No. 9, 2015, pp. 126–149. [[Diakopoulos_AccountabilityAlgorithmicDecisionmaking_2015]] [OA](https://engine.scholarcy.com/oa_version?query=Diakopoulos%2C%20Nicholas%20Accountability%20in%20Algorithmic%20Decision-Making%2C%202015) [GScholar](https://scholar.google.co.uk/scholar?q=Diakopoulos%2C%20Nicholas%20Accountability%20in%20Algorithmic%20Decision-Making%2C%202015) [Scite](https://engine.scholarcy.com/scite_url?query=Diakopoulos%2C%20Nicholas%20Accountability%20in%20Algorithmic%20Decision-Making%2C%202015)

[^Dillard_2018_a]: Dillard and Yuthas, “Ethics Research in AIS”. Dobbe, R., Dean, S., Gilbert, T., and Kohli, N., ‘A Broader View on Bias in Automated Decision-Making: Reflecting on Epistemology and Dynamics’, 2018 Workshop on Fairness, Accountability and Transparency in Machine Learning during ICML 2018, Stockholm, Sweden, 2018. https://arxiv.org/abs/1807.00553. Donovan, Joan, Robyn Caplan, Jeanna Matthews, “Algorithmic accountability: A primer. Data & Society Tech Algorithm Briefing: How Algorithms Perpetuate Racial Bias and Inequality,” Washington, DC, USA, 2018.https://datasociety.net/wpcontent/uploads/2018/04/Data_Society_Algorithmic_Accountability_Primer_FINAL.pdf [[Dillard_EthicsResearchAis_2018]] [OA](https://arxiv.org/abs/1807.00553)  

[^Doran_et+al_2017_a]: Doran, Derek, Sarah Schulz, and Tarek R. Besold, "What does explainable AI really mean? A new conceptualization of perspectives," arXiv preprint arXiv:1710.00794, 2017. [[Doran_et+al_WhatDoesExplainableAiReally_2017]] [OA](https://export.arxiv.org/pdf/1710.00794)  

[^Dossett_2018_a]: Dossett, Julian, “Artificial Intelligence: Raising New Ethics Questions in Media and Journalism,” PR Newswire for Journalists, May 9, 2018. https://mediablog.prnewswire.com/2018/05/09/artificialintelligence-ethics-questions/ Dressel, Julia and Farid, Hany, “The Accuracy, Fairness, and Limits of Predicting Recidivism,” Science Advances, Vol.4, 2018, p.1. [[Dossett_artificialIntelligenceRaisingNewEthics_2018]] [OA](https://mediablog.prnewswire.com/2018/05/09/artificialintelligence-ethics-questions/Dressel)  [Scite](https://engine.scholarcy.com/scite_url?query=Dossett%20Julian%20Artificial%20Intelligence%20Raising%20New%20Ethics%20Questions%20in%20Media%20and%20Journalism%20PR%20Newswire%20for%20Journalists%20May%209%202018%20httpsmediablogprnewswirecom20180509artificialintelligenceethicsquestions%20Dressel%20Julia%20and%20Farid%20Hany%20The%20Accuracy%20Fairness%20and%20Limits%20of%20Predicting%20Recidivism%20Science%20Advances%20Vol4%202018%20p1)

[^Driessen_2015_a]: Driessen, Clemens & Heutinck, Leonie F.M., “Cows Desiring to be Milked? Milking Robots and the Coevolution of Ethics and Technology on Dutch Dairy Farms”, Agriculture and Human Values 32(1), March 2015.,,,,,, Droneseed, “Precision Forestry”, accessed December 2018. droneseed.co Duffy, Brian, “Fundamental Issues in Affective Intelligent Social Machines,” The Open Artificial Intelligence Journal, No. 2, 2004, pp. 21–34. [[Driessen_CowsDesiringMilkedMilkingRobots_2015]] [OA](https://engine.scholarcy.com/oa_version?query=Driessen%2C%20Clemens%20Heutinck%2C%20Leonie%20F.M.%20Cows%20Desiring%20to%20be%20Milked%3F%20Milking%20Robots%20and%20the%20Coevolution%20of%20Ethics%20and%20Technology%20on%20Dutch%20Dairy%20Farms%202015-03) [GScholar](https://scholar.google.co.uk/scholar?q=Driessen%2C%20Clemens%20Heutinck%2C%20Leonie%20F.M.%20Cows%20Desiring%20to%20be%20Milked%3F%20Milking%20Robots%20and%20the%20Coevolution%20of%20Ethics%20and%20Technology%20on%20Dutch%20Dairy%20Farms%202015-03) [Scite](https://engine.scholarcy.com/scite_url?query=Driessen%2C%20Clemens%20Heutinck%2C%20Leonie%20F.M.%20Cows%20Desiring%20to%20be%20Milked%3F%20Milking%20Robots%20and%20the%20Coevolution%20of%20Ethics%20and%20Technology%20on%20Dutch%20Dairy%20Farms%202015-03)

[^Dumouchel_2017_a]: Dumouchel, Paul & Damiano, Luisa, Living with Robots, 2017. [[Dumouchel__2017]] [OA](https://scholar.google.co.uk/scholar?q=Dumouchel%20Paul%20%20Damiano%20Luisa%20Living%20with%20Robots%202017) [GScholar](https://scholar.google.co.uk/scholar?q=Dumouchel%20Paul%20%20Damiano%20Luisa%20Living%20with%20Robots%202017) 

[^Dwork_2014_a]: Dwork, Cynthia, and Aaron Roth, "The algorithmic foundations of differential privacy," Foundations and Trends® in Theoretical Computer Science, Vol. 9, No. 3–4, 2014, pp. 211-407. [[Dwork_AlgorithmicFoundationsDifferentialPrivacy_2014]] [OA](https://engine.scholarcy.com/oa_version?query=Dwork%2C%20Cynthia%20Roth%2C%20Aaron%20The%20algorithmic%20foundations%20of%20differential%20privacy%2C%202014) [GScholar](https://scholar.google.co.uk/scholar?q=Dwork%2C%20Cynthia%20Roth%2C%20Aaron%20The%20algorithmic%20foundations%20of%20differential%20privacy%2C%202014) [Scite](https://engine.scholarcy.com/scite_url?query=Dwork%2C%20Cynthia%20Roth%2C%20Aaron%20The%20algorithmic%20foundations%20of%20differential%20privacy%2C%202014)

[^Dworkin_1988_a]: Dworkin, Gerald, The theory and practice of autonomy, Cambridge University Press, New York, 1988., 61f; Arneson, Richard, “Autonomy and Preference Formation,” 1991, in Coleman, Jules L. and Allen Buchanan, eds, In Harm's Way: Essays in Honor of Joel Feinberg, Cambridge University Press, Cambridge, 1994, pp. 42–73. [[Dworkin_TheoryPracticeAutonomy_1988]] [OA](https://engine.scholarcy.com/oa_version?query=Dworkin%2C%20Gerald%20The%20theory%20and%20practice%20of%20autonomy%201988) [GScholar](https://scholar.google.co.uk/scholar?q=Dworkin%2C%20Gerald%20The%20theory%20and%20practice%20of%20autonomy%201988) [Scite](https://engine.scholarcy.com/scite_url?query=Dworkin%2C%20Gerald%20The%20theory%20and%20practice%20of%20autonomy%201988)

[^Easton_2013_a]: Easton, Catherine, “Carry on Automat(r)on: Legal and Ethical Issues Relating to Healthcare Robots”, Tech Law, May 2013. [[Easton_CarryAutomatLegalEthicalIssues_2013]] [OA](https://engine.scholarcy.com/oa_version?query=Easton%2C%20Catherine%20Carry%20on%20Automat%28r%29on%3A%20Legal%20and%20Ethical%20Issues%20Relating%20to%20Healthcare%20Robots%202013-05) [GScholar](https://scholar.google.co.uk/scholar?q=Easton%2C%20Catherine%20Carry%20on%20Automat%28r%29on%3A%20Legal%20and%20Ethical%20Issues%20Relating%20to%20Healthcare%20Robots%202013-05) [Scite](https://engine.scholarcy.com/scite_url?query=Easton%2C%20Catherine%20Carry%20on%20Automat%28r%29on%3A%20Legal%20and%20Ethical%20Issues%20Relating%20to%20Healthcare%20Robots%202013-05)

[^Edlich_2017_a]: Edlich, Alex & Sohoni, Vik, “Burned by the Bots: Why Robotic Autonomation is Stumbling”, McKinsey & Company, May 2017. [[Edlich_BurnedBotsRoboticAutonomationStumbling_2017]] [OA](https://scholar.google.co.uk/scholar?q=Edlich%2C%20Alex%20Sohoni%2C%20Vik%20Burned%20by%20the%20Bots%3A%20Why%20Robotic%20Autonomation%20is%20Stumbling%202017-05) [GScholar](https://scholar.google.co.uk/scholar?q=Edlich%2C%20Alex%20Sohoni%2C%20Vik%20Burned%20by%20the%20Bots%3A%20Why%20Robotic%20Autonomation%20is%20Stumbling%202017-05) 

[^E_2018_a]: EGE, Future of Work, Future of Society, 19 December 2018. https://ec.europa.eu/info/sites/info/files/research_and_innovation/ege/ege_future-ofwork_opinion_122018.pdf [[E_FutureWorkFutureSociety_2018]] [OA](https://ec.europa.eu/info/sites/info/files/research_and_innovation/ege/ege_future-ofwork_opinion_122018.pdf)  

[^Elder_2017_a]: Elder, A., “Robot Friends for Autistic Children. Monopoly Money or Counterfeit Currency?,” In Lin, P., Abney, K. and Jenkins, R. (eds.), Robot Ethics 2.0: From Autonomous Cars to Artificial Intelligence. Oxford University Press, 2017. [[Elder_RobotFriendsAutisticChildrenMonopoly_2017]] [OA](https://engine.scholarcy.com/oa_version?query=Elder%2C%20A.%20Robot%20Friends%20for%20Autistic%20Children.%20Monopoly%20Money%20or%20Counterfeit%20Currency%3F%2C%202017) [GScholar](https://scholar.google.co.uk/scholar?q=Elder%2C%20A.%20Robot%20Friends%20for%20Autistic%20Children.%20Monopoly%20Money%20or%20Counterfeit%20Currency%3F%2C%202017) [Scite](https://engine.scholarcy.com/scite_url?query=Elder%2C%20A.%20Robot%20Friends%20for%20Autistic%20Children.%20Monopoly%20Money%20or%20Counterfeit%20Currency%3F%2C%202017)

[^Joh_2016_a]: Elizabeth E. Joh, “Police Robots Need to Be Regulated to Avoid Potential Risks”, The New York Times, 16 November 2016. https://www.nytimes.com/roomfordebate/2016/07/14/what-ethics-shouldguide-the-use-of-robots-in-policing/police-robots-need-to-be-regulated-to-avoid-potential-risks [[Joh_PoliceRobotsNeedBeRegulated_2016]] [OA](https://www.nytimes.com/roomfordebate/2016/07/14/what-ethics-shouldguide-the-use-of-robots-in-policing/police-robots-need-to-be-regulated-to-avoid-potential-risks)  [Scite](https://engine.scholarcy.com/scite_url?query=Joh%2C%20Elizabeth%20E.%20Police%20Robots%20Need%20to%20Be%20Regulated%20to%20Avoid%20Potential%20Risks%202016-11-16)

[^Joh_2016_a]: Elizabeth E. Joh, “Policing Police Robots”, UCLA Law Review Discourse, Vol. 64, 2016, p. 521. [[Joh_PolicingPoliceRobots_2016]] [OA](https://engine.scholarcy.com/oa_version?query=Joh%2C%20Elizabeth%20E.%20Policing%20Police%20Robots%202016) [GScholar](https://scholar.google.co.uk/scholar?q=Joh%2C%20Elizabeth%20E.%20Policing%20Police%20Robots%202016) [Scite](https://engine.scholarcy.com/scite_url?query=Joh%2C%20Elizabeth%20E.%20Policing%20Police%20Robots%202016)

[^Elmaghraby_2014_a]: Elmaghraby & Losavio, 2014, p. 493 Elmaghraby, Adel S., and Michael M. Losavio, "Cyber security challenges in Smart Cities: Safety, security and privacy," Journal of advanced research, Vol. 5, No. 4, 2014, pp. 491-497., p. 492 Englisch, Joachim, “Digitalisation and the Future of National Tax Systems: Taxing Robots?,” Available at SSRN: https://ssrn.com/abstract=3244670, September 5, 2018. [[Elmaghraby_CyberSecurityChallengesSmartCities_2014]] [OA](https://ssrn.com/abstract=3244670)  [Scite](https://engine.scholarcy.com/scite_url?query=Elmaghraby%20Losavio%20Cyber%20security%20challenges%20in%20Smart%20Cities%3A%20Safety%2C%20security%20and%20privacy%2C%202014-09-05)

[^Eriksson_2016_a]: Eriksson, Alexander, and Neville Stanton, “The chatty co-driver: A linguistics approach to humanautomation-interaction,” In: Contemporary ergonomics and human factors 2016: Proceedings of the international conference on ergonomics & human factors, 2016. [[Eriksson_theChattydriverLinguisticsApproach_2016]] [OA](https://engine.scholarcy.com/oa_version?query=Eriksson%20Alexander%20and%20Neville%20Stanton%20The%20chatty%20codriver%20A%20linguistics%20approach%20to%20humanautomationinteraction%20In%20Contemporary%20ergonomics%20and%20human%20factors%202016%20Proceedings%20of%20the%20international%20conference%20on%20ergonomics%20%20human%20factors%202016) [GScholar](https://scholar.google.co.uk/scholar?q=Eriksson%20Alexander%20and%20Neville%20Stanton%20The%20chatty%20codriver%20A%20linguistics%20approach%20to%20humanautomationinteraction%20In%20Contemporary%20ergonomics%20and%20human%20factors%202016%20Proceedings%20of%20the%20international%20conference%20on%20ergonomics%20%20human%20factors%202016) [Scite](https://engine.scholarcy.com/scite_url?query=Eriksson%20Alexander%20and%20Neville%20Stanton%20The%20chatty%20codriver%20A%20linguistics%20approach%20to%20humanautomationinteraction%20In%20Contemporary%20ergonomics%20and%20human%20factors%202016%20Proceedings%20of%20the%20international%20conference%20on%20ergonomics%20%20human%20factors%202016)

[^Etzioni_2017_a]: Etzioni, Amitai & Etzioni, Oren, “Incorporating Ethics into Artificial Intelligence”, The Journal of Ethics 21(4), December 2017. European Commission for the Efficiency of Justice (CEPEJ), “European Ethical Charter on the Use of Artificial Intelligence in Judicial Systems and Their Environment”, adopted on 3-4 December 2018. European Parliament, Public Undertakings and Services in the European Union. http://www.europarl.europa.eu/workingpapers/econ/w21/sum-2_en.htm [[Etzioni_IncorporatingEthicsIntoArtificialIntelligence_2017]] [OA](http://www.europarl.europa.eu/workingpapers/econ/w21/sum-2_en.htm)  [Scite](https://engine.scholarcy.com/scite_url?query=Etzioni%2C%20Amitai%20Etzioni%2C%20Oren%20Incorporating%20Ethics%20into%20Artificial%20Intelligence%202017-12)

[^Union_2000_a]: European Union, Charter of Fundamental Rights of the European Union, December 18, 2000, 2000/C 364/01. https://www.europarl.europa.eu/charter/pdf/text_en.pdf., art.15.1 Eurostat, “Girls and women under-represented in ICT,” 25 April 2018. Retrieved at https://ec.europa.eu/eurostat/web/products-eurostat-news/-/EDN-20180425-1. [[Union_CharterFundamentalRightsEuropeanUnion_2000]] [OA](https://www.europarl.europa.eu/charter/pdf/text_en.pdf)  [Scite](https://engine.scholarcy.com/scite_url?query=Union%2C%20European%20Charter%20of%20Fundamental%20Rights%20of%20the%20European%20Union%202000-04-18)

[^Evans_et+al_2018_a]: Evans, Chadrick R., et al., “Telemedicine and telerobotics: from science fiction to reality”, Updates in Surgery, Vol. 70, 2018, p. 361. [[Evans_et+al_TelemedicineTeleroboticsFromScienceFiction_2018]] [OA](https://engine.scholarcy.com/oa_version?query=Evans%2C%20Chadrick%20R.%20Telemedicine%20and%20telerobotics%3A%20from%20science%20fiction%20to%20reality%202018) [GScholar](https://scholar.google.co.uk/scholar?q=Evans%2C%20Chadrick%20R.%20Telemedicine%20and%20telerobotics%3A%20from%20science%20fiction%20to%20reality%202018) [Scite](https://engine.scholarcy.com/scite_url?query=Evans%2C%20Chadrick%20R.%20Telemedicine%20and%20telerobotics%3A%20from%20science%20fiction%20to%20reality%202018)

[^Fackler_2017_a]: Fackler, Martin, “Six Years After Fukushima, Robots Finally Find Reactors’ Melted Uranium Fuel”, New York Times, November 2017. [[Fackler_YearsAfterFukushimaRobotsFinally_2017]] [OA](https://engine.scholarcy.com/oa_version?query=Fackler%2C%20Martin%20Six%20Years%20After%20Fukushima%2C%20Robots%20Finally%20Find%20Reactors%E2%80%99%20Melted%20Uranium%20Fuel%202017-11) [GScholar](https://scholar.google.co.uk/scholar?q=Fackler%2C%20Martin%20Six%20Years%20After%20Fukushima%2C%20Robots%20Finally%20Find%20Reactors%E2%80%99%20Melted%20Uranium%20Fuel%202017-11) [Scite](https://engine.scholarcy.com/scite_url?query=Fackler%2C%20Martin%20Six%20Years%20After%20Fukushima%2C%20Robots%20Finally%20Find%20Reactors%E2%80%99%20Melted%20Uranium%20Fuel%202017-11)

[^Faggella_2018_a]: Faggella, Daniel, “Machine Learning in Finanace—Present and Future Applications,” TechEmergence, March 27, 2018. http://techemergence.com/machine-learning-in-finance-applications/ Fagnant, Daniel J.& Kockelman, Kara, “Preparing a Nation for Autonomous Vehicles: Opportunities, Barriers and Policy Recommendations,” 2015. Faniyi et al., “Architecting Self-Aware Software Systems”. Feiner, Lauren, “A woman shared her tragic story of how social media kept targeting her with baby ads after she had a stillbirth,” CNBC, December 12, 2018, Friedman, B., & Nissenbaum, H. (1996). Bias in computer systems. ACM Transactions on Information Systems (TOIS), 14(3), 330-347. [[Faggella_MachineLearningFinanacepresentFutureApplications_2018]] [OA](http://techemergence.com/machine-learning-in-finance-applications/Fagnant)  [Scite](https://engine.scholarcy.com/scite_url?query=Faggella%2C%20Daniel%20Machine%20Learning%20in%20Finanace%E2%80%94Present%20and%20Future%20Applications%2C%202018-03-27)

[^Friedman_1990_a]: Friedman, B., 1990. “Moral Responsibility and Computer Technology,” Paper Presented at the Annual Meeting of the American Educational Research Association, Boston, Massachusetts. [[Friedman_moralResponsibilityComputerTechnologyPaper_1990]] [OA](https://scholar.google.co.uk/scholar?q=Friedman%2C%20B.%20%E2%80%9CMoral%20Responsibility%20and%20Computer%20Technology%2C%E2%80%9D%20Paper%20Presented%20at%20the%20Annual%20Meeting%20of%20the%20American%20Educational%20Research%20Association%201990) [GScholar](https://scholar.google.co.uk/scholar?q=Friedman%2C%20B.%20%E2%80%9CMoral%20Responsibility%20and%20Computer%20Technology%2C%E2%80%9D%20Paper%20Presented%20at%20the%20Annual%20Meeting%20of%20the%20American%20Educational%20Research%20Association%201990) 

[^Fuller_et+al_2017_a]: Fuller, Daniel, Martine Shareck, and Kevin Stanley. "Ethical implications of location and accelerometer measurement in health research studies with mobile sensing devices." Social Science & Medicine 191 (2017): 84-88. [[Fuller_et+al_EthicalImplicationsLocationAccelerometerMeasurement_2017]] [OA](https://engine.scholarcy.com/oa_version?query=Fuller%2C%20Daniel%20Shareck%2C%20Martine%20Stanley%2C%20Kevin%20Ethical%20implications%20of%20location%20and%20accelerometer%20measurement%20in%20health%20research%20studies%20with%20mobile%20sensing%20devices.%202017) [GScholar](https://scholar.google.co.uk/scholar?q=Fuller%2C%20Daniel%20Shareck%2C%20Martine%20Stanley%2C%20Kevin%20Ethical%20implications%20of%20location%20and%20accelerometer%20measurement%20in%20health%20research%20studies%20with%20mobile%20sensing%20devices.%202017) [Scite](https://engine.scholarcy.com/scite_url?query=Fuller%2C%20Daniel%20Shareck%2C%20Martine%20Stanley%2C%20Kevin%20Ethical%20implications%20of%20location%20and%20accelerometer%20measurement%20in%20health%20research%20studies%20with%20mobile%20sensing%20devices.%202017)

[^Gallagher_et+al_2017_a]: Gallagher, J., et al., “Junk News and Bots during the 2017 UK General Election: What Are UK Voters [[Gallagher_et+al_junkNewsBotsDuring2017_2017]] [OA](https://scholar.google.co.uk/scholar?q=Gallagher%20J%20et%20al%20Junk%20News%20and%20Bots%20during%20the%202017%20UK%20General%20Election%20What%20Are%20UK%20Voters) [GScholar](https://scholar.google.co.uk/scholar?q=Gallagher%20J%20et%20al%20Junk%20News%20and%20Bots%20during%20the%202017%20UK%20General%20Election%20What%20Are%20UK%20Voters) 

[^Sharing_2016_a]: Sharing Over Twitter?,” Data Memo, COMPROP-OII, May 2017. https://comprop.oii.ox.ac.uk/research/working-papers/junk-news-and-bots-during-the-2017-ukgeneral-election/ Gallego, Jelor, “The Microbots Will Treat Diseases From Inside Your Body”, 9 Oct 2016.https://futurism.com/meet-the-microbots-that-will-treat-diseases-from-inside-your-body Gams, Matjaz, Irene Yu-Hua Gu, Aki Härmä, Andrés Muñoz, and Vincent Tam, “Artificial Intelligence and Ambient Intelligence,” Journal of Ambient Intelligence and Smart Environments, Vol.11, No.1, 2019, pp.71-86., p.76. [[Sharing_SharingOverTwitterDataMemo_2016]] [OA](https://comprop.oii.ox.ac.uk/research/working-papers/junk-news-and-bots-during-the-2017-ukgeneral-election/Gallego)  [Scite](https://engine.scholarcy.com/scite_url?query=Sharing%20Over%20Twitter%20Data%20Memo%20COMPROPOII%20May%202017%20httpscompropoiioxacukresearchworkingpapersjunknewsandbotsduringthe2017ukgeneralelection%20Gallego%20Jelor%20The%20Microbots%20Will%20Treat%20Diseases%20From%20Inside%20Your%20Body%209%20Oct%202016httpsfuturismcommeetthemicrobotsthatwilltreatdiseasesfrominsideyourbody%20Gams%20Matjaz%20Irene%20YuHua%20Gu%20Aki%20H%C3%A4rm%C3%A4%20Andr%C3%A9s%20Mu%C3%B1oz%20and%20Vincent%20Tam%20Artificial%20Intelligence%20and%20Ambient%20Intelligence%20Journal%20of%20Ambient%20Intelligence%20and%20Smart%20Environments%20Vol11%20No1%202019%20pp7186%20p76)

[^Garg_2016_a]: Garg, Vikas K., and Adam Tauman Kalai, "Meta-Unsupervised-Learning: A supervised approach to unsupervised learning," arXiv preprint arXiv:1612.09030, 2016. [[Garg_MetaunsupervisedlearningSupervisedApproachUnsupervisedLearning_2016]] [OA](https://export.arxiv.org/pdf/1612.09030)  

[^Gavaghan_et+al_2019_a]: Gavaghan, Colin, et al., “Government Use of Artificial Intelligence in New Zealand”, New Zealand Law Foundation, Wellington, pp. 46–47, 2019. Gennari et al., “The Evolution of Protégé”. Gevaert, Caroline, Richard Sliuzas, Claudio Persello, and George Vosselman. "Evaluating the societal impact of using drones to support urban upgrading projects." ISPRS international journal of geoinformation 7, no. 3 (2018): 91. [[Gavaghan_et+al_governmentUseArtificialIntelligenceNew_2019]] [OA](https://scholar.google.co.uk/scholar?q=Gavaghan%2C%20Colin%20%E2%80%9CGovernment%20Use%20of%20Artificial%20Intelligence%20in%20New%20Zealand%E2%80%9D%2C%20New%20Zealand%20Law%20Foundation%202019) [GScholar](https://scholar.google.co.uk/scholar?q=Gavaghan%2C%20Colin%20%E2%80%9CGovernment%20Use%20of%20Artificial%20Intelligence%20in%20New%20Zealand%E2%80%9D%2C%20New%20Zealand%20Law%20Foundation%202019) 

[^Ghosh_2018_a]: Ghosh, Dipayan, “What is microtargeting and what is it doing in our politics?,” Internet Citizen, October 4, 2018. https://blog.mozilla.org/internetcitizen/2018/10/04/microtargeting-dipayanghosh/ Giannelli, Gianna C., Lucia Mangiavacchi, and Luca Piccoli, "GDP and the value of family caretaking:how much does Europe care?," Applied Economics, Vol.44, No.16, 2012, pp.2111-2131. [[Ghosh_WhatMicrotargetingWhatDoingPolitics_2018]] [OA](https://blog.mozilla.org/internetcitizen/2018/10/04/microtargeting-dipayanghosh/Giannelli)  [Scite](https://engine.scholarcy.com/scite_url?query=Ghosh%2C%20Dipayan%20What%20is%20microtargeting%20and%20what%20is%20it%20doing%20in%20our%20politics%3F%2C%202018-10-04)

[^Giuliani_et+al_2010_a]: Giuliani, Manuel, Claus Lenz, Thomas Müller, Markus Rickert, and Alois Knoll. "Design principles for safety in human-robot interaction." International Journal of Social Robotics 2, no. 3 (2010): 253274. Glenn Greenwald, No Place to Hide: Edward Snowden, the NSA and the Surveillance State, Hamis Hamilton, London, 2014. [[Giuliani_et+al_DesignPrinciplesSafetyHumanrobotInteraction_2010]] [OA](https://engine.scholarcy.com/oa_version?query=Giuliani%2C%20Manuel%20Lenz%2C%20Claus%20M%C3%BCller%2C%20Thomas%20Rickert%2C%20Markus%20Design%20principles%20for%20safety%20in%20human-robot%20interaction.%202010) [GScholar](https://scholar.google.co.uk/scholar?q=Giuliani%2C%20Manuel%20Lenz%2C%20Claus%20M%C3%BCller%2C%20Thomas%20Rickert%2C%20Markus%20Design%20principles%20for%20safety%20in%20human-robot%20interaction.%202010) [Scite](https://engine.scholarcy.com/scite_url?query=Giuliani%2C%20Manuel%20Lenz%2C%20Claus%20M%C3%BCller%2C%20Thomas%20Rickert%2C%20Markus%20Design%20principles%20for%20safety%20in%20human-robot%20interaction.%202010)

[^Goldstein_et+al_1999_a]: Goldstein, Jade, Mark Kantrowitz, Vibhu Mittal, and Jaime Carbonell, "Summarizing text documents: sentence selection and evaluation metrics," In SIGIR, Vol. 99, no. 8, pp. 121-128, 1999. Gonzalez-Fierro, Miguel, “10 Ethical Issues of Artificial Intelligence and Robotics”, Github, April 2018. [[Goldstein_et+al_SummarizingTextDocumentsSentenceSelection_1999]] [OA](https://engine.scholarcy.com/oa_version?query=Goldstein%2C%20Jade%20Kantrowitz%2C%20Mark%20Mittal%2C%20Vibhu%20Carbonell%2C%20Jaime%20Summarizing%20text%20documents%3A%20sentence%20selection%20and%20evaluation%20metrics%2C%201999-04) [GScholar](https://scholar.google.co.uk/scholar?q=Goldstein%2C%20Jade%20Kantrowitz%2C%20Mark%20Mittal%2C%20Vibhu%20Carbonell%2C%20Jaime%20Summarizing%20text%20documents%3A%20sentence%20selection%20and%20evaluation%20metrics%2C%201999-04) [Scite](https://engine.scholarcy.com/scite_url?query=Goldstein%2C%20Jade%20Kantrowitz%2C%20Mark%20Mittal%2C%20Vibhu%20Carbonell%2C%20Jaime%20Summarizing%20text%20documents%3A%20sentence%20selection%20and%20evaluation%20metrics%2C%201999-04)

[^Goodall_2014_a]: Goodall, Noah J., “Ethical Decision Making During Automated Vehicle Crashes,” Transportation Research Record: Journal of the Transportation Research Board, 2014. [[Goodall_EthicalDecisionMakingDuringAutomated_2014]] [OA](https://engine.scholarcy.com/oa_version?query=Goodall%2C%20Noah%20J.%20Ethical%20Decision%20Making%20During%20Automated%20Vehicle%20Crashes%2C%202014) [GScholar](https://scholar.google.co.uk/scholar?q=Goodall%2C%20Noah%20J.%20Ethical%20Decision%20Making%20During%20Automated%20Vehicle%20Crashes%2C%202014) [Scite](https://engine.scholarcy.com/scite_url?query=Goodall%2C%20Noah%20J.%20Ethical%20Decision%20Making%20During%20Automated%20Vehicle%20Crashes%2C%202014)

[^Goodall_2017_a]: Goodall, Noah, “From Trolleys to Risk: Models for Ethical Autonomous Driving”, American Journal of Public Healthy, April 2017. [[Goodall_FromTrolleysRiskModelsEthical_2017]] [OA](https://engine.scholarcy.com/oa_version?query=Goodall%2C%20Noah%20From%20Trolleys%20to%20Risk%3A%20Models%20for%20Ethical%20Autonomous%20Driving%202017-04) [GScholar](https://scholar.google.co.uk/scholar?q=Goodall%2C%20Noah%20From%20Trolleys%20to%20Risk%3A%20Models%20for%20Ethical%20Autonomous%20Driving%202017-04) [Scite](https://engine.scholarcy.com/scite_url?query=Goodall%2C%20Noah%20From%20Trolleys%20to%20Risk%3A%20Models%20for%20Ethical%20Autonomous%20Driving%202017-04)

[^Goodman_2017_a]: Goodman, Bryce, and Seth Flaxman, "European Union regulations on algorithmic decision-making and a “right to explanation”," AI Magazine, Vol. 38, No. 3, 2017, pp. 50-57. [[Goodman_EuropeanUnionRegulationsAlgorithmicDecisionmaking_2017]] [OA](https://engine.scholarcy.com/oa_version?query=Goodman%2C%20Bryce%20Flaxman%2C%20Seth%20European%20Union%20regulations%20on%20algorithmic%20decision-making%20and%20a%20%E2%80%9Cright%20to%20explanation%E2%80%9D%2C%202017) [GScholar](https://scholar.google.co.uk/scholar?q=Goodman%2C%20Bryce%20Flaxman%2C%20Seth%20European%20Union%20regulations%20on%20algorithmic%20decision-making%20and%20a%20%E2%80%9Cright%20to%20explanation%E2%80%9D%2C%202017) [Scite](https://engine.scholarcy.com/scite_url?query=Goodman%2C%20Bryce%20Flaxman%2C%20Seth%20European%20Union%20regulations%20on%20algorithmic%20decision-making%20and%20a%20%E2%80%9Cright%20to%20explanation%E2%80%9D%2C%202017)

[^Goodrich_et+al_0000_a]: Goodrich, Michael A., Dan R. Olsen, Jacob W. Crandall, and Thomas J. Palmer. "Experiments in adjustable autonomy." In Proceedings of IJCAI Workshop on autonomy, delegation and control: interacting with intelligent agents, pp. 1624-1629. [[Goodrich_et+al_ExperimentsAdjustableAutonomy_0000]] [OA](https://engine.scholarcy.com/oa_version?query=Goodrich%2C%20Michael%20A.%20Olsen%2C%20Dan%20R.%20Crandall%2C%20Jacob%20W.%20Palmer%2C%20Thomas%20J.%20Experiments%20in%20adjustable%20autonomy.) [GScholar](https://scholar.google.co.uk/scholar?q=Goodrich%2C%20Michael%20A.%20Olsen%2C%20Dan%20R.%20Crandall%2C%20Jacob%20W.%20Palmer%2C%20Thomas%20J.%20Experiments%20in%20adjustable%20autonomy.) [Scite](https://engine.scholarcy.com/scite_url?query=Goodrich%2C%20Michael%20A.%20Olsen%2C%20Dan%20R.%20Crandall%2C%20Jacob%20W.%20Palmer%2C%20Thomas%20J.%20Experiments%20in%20adjustable%20autonomy.)

[^Seattle_2001_a]: Seattle, WA: American Association for Artificial Intelligence Press, 2001. [[Seattle__2001]] [OA](https://scholar.google.co.uk/scholar?q=Seattle%20WA%20American%20Association%20for%20Artificial%20Intelligence%20Press%202001) [GScholar](https://scholar.google.co.uk/scholar?q=Seattle%20WA%20American%20Association%20for%20Artificial%20Intelligence%20Press%202001) 

[^Gordon_2018_a]: Gordon, John-Stewart, “What do we owe to intelligent Robots?,” AI & Society, 2018. [[Gordon_WhatIntelligentRobots_2018]] [OA](https://engine.scholarcy.com/oa_version?query=Gordon%2C%20John-Stewart%20What%20do%20we%20owe%20to%20intelligent%20Robots%3F%2C%202018) [GScholar](https://scholar.google.co.uk/scholar?q=Gordon%2C%20John-Stewart%20What%20do%20we%20owe%20to%20intelligent%20Robots%3F%2C%202018) [Scite](https://engine.scholarcy.com/scite_url?query=Gordon%2C%20John-Stewart%20What%20do%20we%20owe%20to%20intelligent%20Robots%3F%2C%202018)

[^Gorey_2017_a]: Gorey, Colm, “Tiny robots in our blood could soon be used to sniff out and treat cancer”, Silicon Republic, 23 Nov 2017. https://www.siliconrepublic.com/machines/blood-cell-sized-robots-cancer [[Gorey_TinyRobotsBloodCouldSoon_2017]] [OA](https://www.siliconrepublic.com/machines/blood-cell-sized-robots-cancer)  [Scite](https://engine.scholarcy.com/scite_url?query=Gorey%2C%20Colm%20Tiny%20robots%20in%20our%20blood%20could%20soon%20be%20used%20to%20sniff%20out%20and%20treat%20cancer%202017-11)

[^Graveleau_2017_a]: Graveleau, Séverin, “APB: Le gouvernement promet de se conformer aux demandes de la CNIL,” Le Monde, 28 September 2017. https://www.lemonde.fr/campus/article/2017/09/28/mise-endemeure-de-la-cnil-pour-changer-le-fontionnement-d-admission-postbac_5192758_4401467.html [[Graveleau_ApbGouvernementPrometConformerDemandes_2017]] [OA](https://www.lemonde.fr/campus/article/2017/09/28/mise-endemeure-de-la-cnil-pour-changer-le-fontionnement-d-admission-postbac_5192758_4401467.html)  [Scite](https://engine.scholarcy.com/scite_url?query=Graveleau%2C%20S%C3%A9verin%20APB%3A%20Le%20gouvernement%20promet%20de%20se%20conformer%20aux%20demandes%20de%20la%20CNIL%2C%202017-09-28)

[^Greenbaum_2015_a]: Greenbaum, Dov, “Ethical, Legal and Social Concerns Relating to Exoskeletons”, Computers and Society 45(3), September 2015. [[Greenbaum_EthicalLegalSocialConcernsRelating_2015]] [OA](https://engine.scholarcy.com/oa_version?query=Greenbaum%2C%20Dov%20Ethical%2C%20Legal%20and%20Social%20Concerns%20Relating%20to%20Exoskeletons%202015-09) [GScholar](https://scholar.google.co.uk/scholar?q=Greenbaum%2C%20Dov%20Ethical%2C%20Legal%20and%20Social%20Concerns%20Relating%20to%20Exoskeletons%202015-09) [Scite](https://engine.scholarcy.com/scite_url?query=Greenbaum%2C%20Dov%20Ethical%2C%20Legal%20and%20Social%20Concerns%20Relating%20to%20Exoskeletons%202015-09)

[^Gronlun_2019_a]: Gronlun, Kirsten, “State of AI: Artificial Intelligence, the Military and Increasingly Autonomous Weapons”, Future of Life Institute Website, 9 May 2019. https://futureoflife.org/2019/05/09/state-of-ai/ [[Gronlun_StateAiArtificialIntelligenceMilitary_2019]] [OA](https://futureoflife.org/2019/05/09/state-of-ai/)  

[^Gunkel_2017_a]: Gunkel, David, “Mind the gap: responsible robotics and the problem of responsibility,” Ethics of Information Technology, 2017. [[Gunkel_MindResponsibleRoboticsProblem_2017]] [OA](https://engine.scholarcy.com/oa_version?query=Gunkel%2C%20David%20Mind%20the%20gap%3A%20responsible%20robotics%20and%20the%20problem%20of%20responsibility%2C%202017) [GScholar](https://scholar.google.co.uk/scholar?q=Gunkel%2C%20David%20Mind%20the%20gap%3A%20responsible%20robotics%20and%20the%20problem%20of%20responsibility%2C%202017) [Scite](https://engine.scholarcy.com/scite_url?query=Gunkel%2C%20David%20Mind%20the%20gap%3A%20responsible%20robotics%20and%20the%20problem%20of%20responsibility%2C%202017)

[^Gunkel_2018_a]: Gunkel, David, Robot Rights, The MIT Press, 2018. [[Gunkel_RobotRights_2018]] [OA](https://scholar.google.co.uk/scholar?q=Gunkel%2C%20David%20Robot%20Rights%202018) [GScholar](https://scholar.google.co.uk/scholar?q=Gunkel%2C%20David%20Robot%20Rights%202018) 

[^Gunning_2017_a]: Gunning, David, "Explainable artificial intelligence (xai)," Defense Advanced Research Projects Agency (DARPA), nd Web, Vol. 2, 2017. [[Gunning_ExplainableArtificialIntelligence_2017]] [OA](https://engine.scholarcy.com/oa_version?query=Gunning%2C%20David%20Explainable%20artificial%20intelligence%20%28xai%29%2C%202017) [GScholar](https://scholar.google.co.uk/scholar?q=Gunning%2C%20David%20Explainable%20artificial%20intelligence%20%28xai%29%2C%202017) [Scite](https://engine.scholarcy.com/scite_url?query=Gunning%2C%20David%20Explainable%20artificial%20intelligence%20%28xai%29%2C%202017)

[^Hall_0000_a]: Hall, Holly Kathleen, “Deepfake Videos: When Seeing Isn't Believing,” Catholic University Journal of [[Hall_DeepfakeVideosWhenSeeingIsn_0000]] [OA](https://engine.scholarcy.com/oa_version?query=Hall%2C%20Holly%20Kathleen%20Deepfake%20Videos%3A%20When%20Seeing%20Isn%27t%20Believing%2C) [GScholar](https://scholar.google.co.uk/scholar?q=Hall%2C%20Holly%20Kathleen%20Deepfake%20Videos%3A%20When%20Seeing%20Isn%27t%20Believing%2C) [Scite](https://engine.scholarcy.com/scite_url?query=Hall%2C%20Holly%20Kathleen%20Deepfake%20Videos%3A%20When%20Seeing%20Isn%27t%20Believing%2C)

[^Law_2018_a]: Law and Technology, Vol. 27, No. 1, 2018, pp. 51-76. https://scholarship.law.edu/jlt/vol27/iss1/4. Hamann, Heiko, Divband Soorati, Mohammad & Heinrich, Mary Katherine et al., “Flora Robotica— An [[Law_floraRoboticaAn_2018]] [OA](https://scholarship.law.edu/jlt/vol27/iss1/4)  

[^Architectural_2017_b]: Architectural System Combining Living Natural Plants and Distributed Robots”, Cornell University Computer Science & Emerging Technologies, September 2017. [[Architectural_ArchitecturalSystemCombiningLivingNatural_2017]] [OA](https://engine.scholarcy.com/oa_version?query=Architectural%20System%20Combining%20Living%20Natural%20Plants%20and%20Distributed%20Robots%20Cornell%20University%20Computer%20Science%20%20Emerging%20Technologies%20September%202017) [GScholar](https://scholar.google.co.uk/scholar?q=Architectural%20System%20Combining%20Living%20Natural%20Plants%20and%20Distributed%20Robots%20Cornell%20University%20Computer%20Science%20%20Emerging%20Technologies%20September%202017) [Scite](https://engine.scholarcy.com/scite_url?query=Architectural%20System%20Combining%20Living%20Natural%20Plants%20and%20Distributed%20Robots%20Cornell%20University%20Computer%20Science%20%20Emerging%20Technologies%20September%202017)

[^Hanif_et+al_2018_a]: Hanif, M. A., Khalid, F., Putra, R., Rehman, S., and Shafique, M., ‘Robust Machine Learning Systems: Reliability and Security for Deep Neural Networks’, 2018 IEEE 24th International Symposium on On-Line Testing And Robust System Design (IOLTS), 2-4 July 2018. [[Hanif_et+al_RobustMachineLearningSystemsReliability_2018]] [OA](https://engine.scholarcy.com/oa_version?query=Hanif%2C%20M.A.%20Khalid%2C%20F.%20Putra%2C%20R.%20Rehman%2C%20S.%20Robust%20Machine%20Learning%20Systems%3A%20Reliability%20and%20Security%20for%20Deep%20Neural%20Networks%202018-07) [GScholar](https://scholar.google.co.uk/scholar?q=Hanif%2C%20M.A.%20Khalid%2C%20F.%20Putra%2C%20R.%20Rehman%2C%20S.%20Robust%20Machine%20Learning%20Systems%3A%20Reliability%20and%20Security%20for%20Deep%20Neural%20Networks%202018-07) [Scite](https://engine.scholarcy.com/scite_url?query=Hanif%2C%20M.A.%20Khalid%2C%20F.%20Putra%2C%20R.%20Rehman%2C%20S.%20Robust%20Machine%20Learning%20Systems%3A%20Reliability%20and%20Security%20for%20Deep%20Neural%20Networks%202018-07)

[^Hanson_2009_a]: Hanson, F. A. (2009). Beyond the skin bag: On the moral responsibility of extended agencies. Ethics and Information Technology, 11(1), 91–99. [[Hanson_BeyondSkinMoralResponsibilityExtended_2009]] [OA](https://engine.scholarcy.com/oa_version?query=Hanson%2C%20F.A.%20Beyond%20the%20skin%20bag%3A%20On%20the%20moral%20responsibility%20of%20extended%20agencies%202009) [GScholar](https://scholar.google.co.uk/scholar?q=Hanson%2C%20F.A.%20Beyond%20the%20skin%20bag%3A%20On%20the%20moral%20responsibility%20of%20extended%20agencies%202009) [Scite](https://engine.scholarcy.com/scite_url?query=Hanson%2C%20F.A.%20Beyond%20the%20skin%20bag%3A%20On%20the%20moral%20responsibility%20of%20extended%20agencies%202009)

[^Harford_2016_a]: Harford, Tim, “Crash: How Computers are Setting us up for Disaster,” The Guardian, 2016. https://www.theguardian.com/technology/2016/oct/11/crash-howcomputers-are-setting-us-updisaster Harnden, Charlie, “How Artificial Intelligence is Destroying Meaningful Work,” Medium, https://medium.com/@charlieharnden/artificial-intelligence-and-meaningful-work-c8f6ec24f11b Hart, Robert David, “If You’re Not a White Male, Artificial Intelligence’s Use in Healthcare Could Be Dangerous,” QZ, July 10, 2017.https://qz.com/1023448/if-youre-not-a-white-male-artificialintelligences-use-in-healthcare-could-be-dangerous/ Hashimi, Ali, “AI Ethics: The Next Big Thing in Government. Anticipating the Impact of AI Ethics within the Public Sector”, World Government Summit; Deloitte, February 2019. [[Harford_CrashComputersSettingDisaster_2016]] [OA](https://www.theguardian.com/technology/2016/oct/11/crash-howcomputers-are-setting-us-updisaster)  [Scite](https://engine.scholarcy.com/scite_url?query=Harford%2C%20Tim%20Crash%3A%20How%20Computers%20are%20Setting%20us%20up%20for%20Disaster%2C%202016-02-10)

[^Havens_2019_a]: Havens, John, “Will we lose our rights as parents once robots are better at raising our kids?,” Quarts, July 10, 2019. Retrieved at https://qz.com/co/2533915/. Hawksworth et al., 2018; World Economic Forum, The Future of Jobs Report 2018. Centre for the New Economy and Society, World Economic Forum, Switzerland, 2018a, http://www3.weforum.org/docs/WEF_Future_of_Jobs_2018.pdf. [[Havens_WillLoseRightsParentsOnce_2019]] [OA](https://qz.com/co/2533915/)  [Scite](https://engine.scholarcy.com/scite_url?query=Havens%2C%20John%20Will%20we%20lose%20our%20rights%20as%20parents%20once%20robots%20are%20better%20at%20raising%20our%20kids%3F%2C%202019-07-10)

[^Hawksworth_2018_a]: Hawksworth, John, and Yuval Fertig, “AI and robots should create as many jobs as they displace in the long run,” PricewaterhouseCoopers, 2018. https://pwc.blogs.com/economics_in_business/2018/07/ai-and-robots-should-create-as-manyjobs-as-they-displace-in-the-long-run.html [[Hawksworth_AiRobotsShouldCreateMany_2018]] [OA](https://pwc.blogs.com/economics_in_business/2018/07/ai-and-robots-should-create-as-manyjobs-as-they-displace-in-the-long-run.html)  [Scite](https://engine.scholarcy.com/scite_url?query=Hawksworth%2C%20John%20Fertig%2C%20Yuval%20AI%20and%20robots%20should%20create%20as%20many%20jobs%20as%20they%20displace%20in%20the%20long%20run%2C%202018)

[^Pricewaterhousecoopers_2018_a]: PricewaterhouseCoopers, 2018. https://www.pwc.co.uk/economic-services/assets/internationalimpact-of-automation-feb-2018.pdf. [[Pricewaterhousecoopers__2018]] [OA](https://www.pwc.co.uk/economic-services/assets/internationalimpact-of-automation-feb-2018.pdf)  

[^Hildebrandt_2010_a]: Hildebrandt, Mireille, “The Meaning and The Mining of Legal Texts,” 2010. https://www.researchgate.net/publication/41463068_The_Meaning_and_the_Mining_of_Legal_Texts Hildebrandt, Mireille, and Serge Gutwirth, "Concise conclusions: Citizens out of control," Profiling the European Citizen, Springer, Dordrecht, 2008, pp.365-368. [[Hildebrandt_MeaningTheMiningLegalTexts_2010]] [OA](https://www.researchgate.net/publication/41463068_The_Meaning_and_the_Mining_of_Legal_Texts)  [Scite](https://engine.scholarcy.com/scite_url?query=Hildebrandt%2C%20Mireille%20The%20Meaning%20and%20The%20Mining%20of%20Legal%20Texts%2C%202010)

[^Hildebrandt_2008_a]: Hildebrandt, Mireille, and Serge Gutwirth, Profiling the European Citizen. Cross Disciplinary Perspectives, Dordrecht: Springer, 2008. [[Hildebrandt_ProfilingEuropeanCitizenCrossDisciplinary_2008]] [OA](https://scholar.google.co.uk/scholar?q=Hildebrandt%2C%20Mireille%20Gutwirth%2C%20Serge%20Profiling%20the%20European%20Citizen.%20Cross%20Disciplinary%20Perspectives%202008) [GScholar](https://scholar.google.co.uk/scholar?q=Hildebrandt%2C%20Mireille%20Gutwirth%2C%20Serge%20Profiling%20the%20European%20Citizen.%20Cross%20Disciplinary%20Perspectives%202008) 

[^Himmelreich_2018_a]: Himmelreich, Johannes, “The Everyday Ethical Challenges of Self-Driving Cars”, The Conversation, March 2018. [[Himmelreich_EverydayEthicalChallengesSelfdrivingCars_2018]] [OA](https://engine.scholarcy.com/oa_version?query=Himmelreich%2C%20Johannes%20The%20Everyday%20Ethical%20Challenges%20of%20Self-Driving%20Cars%202018-03) [GScholar](https://scholar.google.co.uk/scholar?q=Himmelreich%2C%20Johannes%20The%20Everyday%20Ethical%20Challenges%20of%20Self-Driving%20Cars%202018-03) [Scite](https://engine.scholarcy.com/scite_url?query=Himmelreich%2C%20Johannes%20The%20Everyday%20Ethical%20Challenges%20of%20Self-Driving%20Cars%202018-03)

[^Hodgson_2016_a]: Hodgson, Jarrod & Lian Pin Koh, “Best Practice for Minimising Unmanned Aerial Vehicle Disturbance to Wildlife in Biological Field Research”, Current Biology 26(10), May 2016. [[Hodgson_BestPracticeMinimisingUnmannedAerial_2016]] [OA](https://engine.scholarcy.com/oa_version?query=Hodgson%2C%20Jarrod%20Koh%2C%20Lian%20Pin%20Best%20Practice%20for%20Minimising%20Unmanned%20Aerial%20Vehicle%20Disturbance%20to%20Wildlife%20in%20Biological%20Field%20Research%202016-05) [GScholar](https://scholar.google.co.uk/scholar?q=Hodgson%2C%20Jarrod%20Koh%2C%20Lian%20Pin%20Best%20Practice%20for%20Minimising%20Unmanned%20Aerial%20Vehicle%20Disturbance%20to%20Wildlife%20in%20Biological%20Field%20Research%202016-05) [Scite](https://engine.scholarcy.com/scite_url?query=Hodgson%2C%20Jarrod%20Koh%2C%20Lian%20Pin%20Best%20Practice%20for%20Minimising%20Unmanned%20Aerial%20Vehicle%20Disturbance%20to%20Wildlife%20in%20Biological%20Field%20Research%202016-05)

[^Holstein_et+al_2018_a]: Holstein, Tobias, Dodig-Crnkovic, Gordana & Pelliccione, Patrizio, “Ethical and Social Aspects of SelfDriving Cars,” Cornell University, 2018. [[Holstein_et+al_EthicalSocialAspectsSelfdrivingCars_2018]] [OA](https://engine.scholarcy.com/oa_version?query=Holstein%2C%20Tobias%20Dodig-Crnkovic%2C%20Gordana%20Pelliccione%2C%20Patrizio%20Ethical%20and%20Social%20Aspects%20of%20SelfDriving%20Cars%2C%202018) [GScholar](https://scholar.google.co.uk/scholar?q=Holstein%2C%20Tobias%20Dodig-Crnkovic%2C%20Gordana%20Pelliccione%2C%20Patrizio%20Ethical%20and%20Social%20Aspects%20of%20SelfDriving%20Cars%2C%202018) [Scite](https://engine.scholarcy.com/scite_url?query=Holstein%2C%20Tobias%20Dodig-Crnkovic%2C%20Gordana%20Pelliccione%2C%20Patrizio%20Ethical%20and%20Social%20Aspects%20of%20SelfDriving%20Cars%2C%202018)

[^Hopkins_2017_a]: Hopkins, Anne, “The Ethical Debate on Drones”, Digital Commons, 2017. [[Hopkins_EthicalDebateDrones_2017]] [OA](https://engine.scholarcy.com/oa_version?query=Hopkins%2C%20Anne%20The%20Ethical%20Debate%20on%20Drones%202017) [GScholar](https://scholar.google.co.uk/scholar?q=Hopkins%2C%20Anne%20The%20Ethical%20Debate%20on%20Drones%202017) [Scite](https://engine.scholarcy.com/scite_url?query=Hopkins%2C%20Anne%20The%20Ethical%20Debate%20on%20Drones%202017)

[^Hornigold_2018_a]: Hornigold, Thomas, “Is the Rise of AI on Wall Street for Better or Worse?,” Singularity Hub, July 16, 2018. https://singularityhub.com/2018/07/16/is-the-rise-of-ai-on-wall-street-for-better-orworse/ Hovy, Dirk, “Demographic Factors Improve Classification Performance,” Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), 2015. [[Hornigold_isRiseAiWallStreet_2018]] [OA](https://singularityhub.com/2018/07/16/is-the-rise-of-ai-on-wall-street-for-better-orworse/Hovy)  [Scite](https://engine.scholarcy.com/scite_url?query=Hornigold%2C%20Thomas%20%E2%80%9CIs%20the%20Rise%20of%20AI%20on%20Wall%20Street%20for%20Better%20or%20Worse%3F%2C%E2%80%9D%20Singularity%20Hub%202018-07-16)

[^Hovy_2015_a]: Hovy, Dirk, and Anders Søgaard, “Tagging Performance Correlates with Author Age,” Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), 2015. [[Hovy_TaggingPerformanceCorrelatesWithAuthor_2015]] [OA](https://engine.scholarcy.com/oa_version?query=Hovy%2C%20Dirk%20S%C3%B8gaard%2C%20Anders%20Tagging%20Performance%20Correlates%20with%20Author%20Age%2C%202015) [GScholar](https://scholar.google.co.uk/scholar?q=Hovy%2C%20Dirk%20S%C3%B8gaard%2C%20Anders%20Tagging%20Performance%20Correlates%20with%20Author%20Age%2C%202015) [Scite](https://engine.scholarcy.com/scite_url?query=Hovy%2C%20Dirk%20S%C3%B8gaard%2C%20Anders%20Tagging%20Performance%20Correlates%20with%20Author%20Age%2C%202015)

[^Howard_2018_a]: Howard, Ayanna & Borenstein, Jason, “The Ugly Truth About Ourselves and Our Robot Creations: The Problem of Bias and Social Inequity”, Science and Engineering Ethics 24(5), October 2018. [[Howard_UglyTruthAboutOurselvesOur_2018]] [OA](https://engine.scholarcy.com/oa_version?query=Howard%2C%20Ayanna%20Borenstein%2C%20Jason%20The%20Ugly%20Truth%20About%20Ourselves%20and%20Our%20Robot%20Creations%3A%20The%20Problem%20of%20Bias%20and%20Social%20Inequity%202018-10) [GScholar](https://scholar.google.co.uk/scholar?q=Howard%2C%20Ayanna%20Borenstein%2C%20Jason%20The%20Ugly%20Truth%20About%20Ourselves%20and%20Our%20Robot%20Creations%3A%20The%20Problem%20of%20Bias%20and%20Social%20Inequity%202018-10) [Scite](https://engine.scholarcy.com/scite_url?query=Howard%2C%20Ayanna%20Borenstein%2C%20Jason%20The%20Ugly%20Truth%20About%20Ourselves%20and%20Our%20Robot%20Creations%3A%20The%20Problem%20of%20Bias%20and%20Social%20Inequity%202018-10)

[^Howard_et+al_2018_b]: Howard, Philip N., Woolley, Samuel, and Calo, Ryan, “Algorithms, Bots, and Political Communication in the US 2016 Election: The Challenge of Automated Political Communication for Election Law and Administration,” Journal of Information Technology & Politics, Vol. 15, No. 2, 2018, pp. 81–93 Howard, Philip, “How Political Campaigns Weaponize Social Media Bots,” IEEE Spectrum, October 18, 2018. https://spectrum.ieee.org/computing/software/how-political-campaigns-weaponize-socialmedia-bots [[Howard_et+al_AlgorithmsBotsPoliticalCommunicationUs_2018]] [OA](https://spectrum.ieee.org/computing/software/how-political-campaigns-weaponize-socialmedia-bots)  [Scite](https://engine.scholarcy.com/scite_url?query=Howard%20Philip%20N%20Woolley%20Samuel%20and%20Calo%20Ryan%20Algorithms%20Bots%20and%20Political%20Communication%20in%20the%20US%202016%20Election%20The%20Challenge%20of%20Automated%20Political%20Communication%20for%20Election%20Law%20and%20Administration%20Journal%20of%20Information%20Technology%20%20Politics%20Vol%2015%20No%202%202018%20pp%208193%20Howard%20Philip%20How%20Political%20Campaigns%20Weaponize%20Social%20Media%20Bots%20IEEE%20Spectrum%20October%2018%202018%20httpsspectrumieeeorgcomputingsoftwarehowpoliticalcampaignsweaponizesocialmediabots)

[^Hulme_2018_a]: Hulme, David, “Rogue Robots”, Vision Insight: Global Threats, August 2018. [[Hulme_RogueRobots_2018]] [OA](https://engine.scholarcy.com/oa_version?query=Hulme%2C%20David%20Rogue%20Robots%202018-08) [GScholar](https://scholar.google.co.uk/scholar?q=Hulme%2C%20David%20Rogue%20Robots%202018-08) [Scite](https://engine.scholarcy.com/scite_url?query=Hulme%2C%20David%20Rogue%20Robots%202018-08)

[^Hurlburt_2017_a]: Hurlburt, George, “How Much Should We Trust Artificial Intelligence,” InfoQ, September 8, 2017. [[Hurlburt_MuchShouldWeTrustArtificial_2017]] [OA](https://engine.scholarcy.com/oa_version?query=Hurlburt%2C%20George%20How%20Much%20Should%20We%20Trust%20Artificial%20Intelligence%2C%202017-09-08) [GScholar](https://scholar.google.co.uk/scholar?q=Hurlburt%2C%20George%20How%20Much%20Should%20We%20Trust%20Artificial%20Intelligence%2C%202017-09-08) [Scite](https://engine.scholarcy.com/scite_url?query=Hurlburt%2C%20George%20How%20Much%20Should%20We%20Trust%20Artificial%20Intelligence%2C%202017-09-08)

[^Https_n.d._a]: https://www.infoq.com/articles/ai-trust/ IBM, “With AI, our words will be a window into our mental health”, IBM, n.d. [[Https_WithAiWordsWillWindow_n.d.]] [OA](https://www.infoq.com/articles/ai-trust/IBM)  [Scite](https://engine.scholarcy.com/scite_url?query=https%2C%20//www.infoq.com/articles/ai-trust/IBM%20With%20AI%2C%20our%20words%20will%20be%20a%20window%20into%20our%20mental%20health%20n.d.)

[^https://www.research.ibm.com/5-in-5/mental-health/_2018_b]: https://www.research.ibm.com/5-in-5/mental-health/ Ienca, Marcello, Tenzin Wangmo, Fabrice Jotterand, Reto W. Kressig, and Bernice Elger, "Ethical design of intelligent assistive technologies for dementia:a descriptive review," Science and engineering ethics, Vol 24, No.4, 2018, pp.1035-1055. [[https://www.research.ibm.com/5-in-5/mental-health/_EthicalDesignIntelligentAssistiveTechnologies_2018]] [OA](https://www.research.ibm.com/5-in-5/mental-health/Ienca)  [Scite](https://engine.scholarcy.com/scite_url?query=httpswwwresearchibmcom5in5mentalhealth%20Ienca%20Marcello%20Tenzin%20Wangmo%20Fabrice%20Jotterand%20Reto%20W%20Kressig%20and%20Bernice%20Elger%20Ethical%20design%20of%20intelligent%20assistive%20technologies%20for%20dementiaa%20descriptive%20review%20Science%20and%20engineering%20ethics%20Vol%2024%20No4%202018%20pp10351055)

[^Chen_et+al_2019_a]: Irene Y. Chen, Szolovits, Peter, and Ghassemi, Marzyeh, “Can AI Help Reduce Disparities in General Medical and Mental Health Care,” AMA Journal of Ethics, Vol. 21, No. 2, 2019. [[Chen_et+al_AiHelpReduceDisparitiesGeneral_2019]] [OA](https://engine.scholarcy.com/oa_version?query=Chen%2C%20Irene%20Y.%20Szolovits%2C%20Peter%20Ghassemi%2C%20Marzyeh%20Can%20AI%20Help%20Reduce%20Disparities%20in%20General%20Medical%20and%20Mental%20Health%20Care%2C%202019) [GScholar](https://scholar.google.co.uk/scholar?q=Chen%2C%20Irene%20Y.%20Szolovits%2C%20Peter%20Ghassemi%2C%20Marzyeh%20Can%20AI%20Help%20Reduce%20Disparities%20in%20General%20Medical%20and%20Mental%20Health%20Care%2C%202019) [Scite](https://engine.scholarcy.com/scite_url?query=Chen%2C%20Irene%20Y.%20Szolovits%2C%20Peter%20Ghassemi%2C%20Marzyeh%20Can%20AI%20Help%20Reduce%20Disparities%20in%20General%20Medical%20and%20Mental%20Health%20Care%2C%202019)

[^Ivošević_et+al_2017_a]: Ivošević, Bojana, Han, Yong-Gu, Cho, Youngho & Kwon, Ohseok, “Monitoring Butterflies With an Unmanned Aerial Vehicle: Current Possibilities and Future Potentials”, Journal of Ecology and Environment 41(1), 2017. [[Ivošević_et+al_MonitoringButterfliesWithUnmannedAerial_2017]] [OA](https://engine.scholarcy.com/oa_version?query=Ivo%C5%A1evi%C4%87%2C%20Bojana%20Han%2C%20Yong-Gu%20Cho%2C%20Youngho%20Kwon%2C%20Ohseok%20Monitoring%20Butterflies%20With%20an%20Unmanned%20Aerial%20Vehicle%3A%20Current%20Possibilities%20and%20Future%20Potentials%202017) [GScholar](https://scholar.google.co.uk/scholar?q=Ivo%C5%A1evi%C4%87%2C%20Bojana%20Han%2C%20Yong-Gu%20Cho%2C%20Youngho%20Kwon%2C%20Ohseok%20Monitoring%20Butterflies%20With%20an%20Unmanned%20Aerial%20Vehicle%3A%20Current%20Possibilities%20and%20Future%20Potentials%202017) [Scite](https://engine.scholarcy.com/scite_url?query=Ivo%C5%A1evi%C4%87%2C%20Bojana%20Han%2C%20Yong-Gu%20Cho%2C%20Youngho%20Kwon%2C%20Ohseok%20Monitoring%20Butterflies%20With%20an%20Unmanned%20Aerial%20Vehicle%3A%20Current%20Possibilities%20and%20Future%20Potentials%202017)

[^Ivošević_et+al_2015_a]: Ivošević, Bojana, Han, Yong-Gu, Cho, Youngho & Kwon, Ohseok, “The Use of Conservation Drones in Ecology and Wildlife Research”, Ecology and Environment, 38(1), February 2015. [[Ivošević_et+al_UseConservationDronesEcologyWildlife_2015]] [OA](https://engine.scholarcy.com/oa_version?query=Ivo%C5%A1evi%C4%87%2C%20Bojana%20Han%2C%20Yong-Gu%20Cho%2C%20Youngho%20Kwon%2C%20Ohseok%20The%20Use%20of%20Conservation%20Drones%20in%20Ecology%20and%20Wildlife%20Research%202015-02) [GScholar](https://scholar.google.co.uk/scholar?q=Ivo%C5%A1evi%C4%87%2C%20Bojana%20Han%2C%20Yong-Gu%20Cho%2C%20Youngho%20Kwon%2C%20Ohseok%20The%20Use%20of%20Conservation%20Drones%20in%20Ecology%20and%20Wildlife%20Research%202015-02) [Scite](https://engine.scholarcy.com/scite_url?query=Ivo%C5%A1evi%C4%87%2C%20Bojana%20Han%2C%20Yong-Gu%20Cho%2C%20Youngho%20Kwon%2C%20Ohseok%20The%20Use%20of%20Conservation%20Drones%20in%20Ecology%20and%20Wildlife%20Research%202015-02)

[^Jansen_et+al_2018_a]: Jansen, Philip, Stearns Broadhead, Rowena Rodrigues, David Wright, Philip Brey, Alice Fox, Ning Wang, SIENNA D4.1 State-of-the-art Review, WP4 - AI & Robotics, 2018, Public deliverable report from the SIENNA project. http://www.sienna-project.eu/publications/deliverable-reports/ [[Jansen_et+al_NingWangSiennaD41State_2018]] [OA](http://www.sienna-project.eu/publications/deliverable-reports/)  [Scite](https://engine.scholarcy.com/scite_url?query=Jansen%2C%20Philip%20Broadhead%2C%20Stearns%20Rodrigues%2C%20Rowena%20Wright%2C%20David%20Ning%20Wang%2C%20SIENNA%20D4.1%20State-of-the-art%20Review%2C%20WP4%20-%20AI%202018)

[^Burrell_2016_b]: Jenna Burrell, (2016). How the machine ‘thinks’: Understanding opacity in machine learning algorithms. Big Data & Society, 3(1). Jensen, Ole B., “Drone City—Power, Design, and Aerial Mobility in the Age of Smart Cities”, April 2016. Job loss due to AI — How bad is it going to be? (2019, February 4). Skynet Today. Retrieved at: https://www.skynettoday.com/editorials/ai-automation-job-loss. John Arquilla and David Ronfeldt, “Swarming & The Future of Conflict”, RAND, National Defense Research Institute, 2000. See also section on use of AI and robotics in the defence sector in the present report. [[Burrell_MachinethinksUnderstandingOpacityMachine_2016]] [OA](https://www.skynettoday.com/editorials/ai-automation-job-loss)  [Scite](https://engine.scholarcy.com/scite_url?query=Burrell%2C%20Jenna%20How%20the%20machine%20%E2%80%98thinks%E2%80%99%3A%20Understanding%20opacity%20in%20machine%20learning%20algorithms.%20Big%20Data%20%26%202016-02)

[^Johnson_2018_a]: Johnson, J. A. (2018). Open Data, Big Data, and Just Data. In Toward Information Justice, ed. by J. A. Johnson, 23-49. [[Johnson_OpenDataBigDataJust_2018]] [OA](https://scholar.google.co.uk/scholar?q=Johnson%2C%20J.A.%20Open%20Data%2C%20Big%20Data%2C%20and%20Just%20Data%202018) [GScholar](https://scholar.google.co.uk/scholar?q=Johnson%2C%20J.A.%20Open%20Data%2C%20Big%20Data%2C%20and%20Just%20Data%202018) 

[^Johnson_2019_a]: Johnson, Khari, “How AI Can Strengthen and Defend Democracy,” Venture Beat, 4 July 2019. https://venturebeat.com/2019/07/04/how-ai-can-strengthen-and-defend-democracy/ [[Johnson_howAiCanStrengthenDefend_2019]] [OA](https://venturebeat.com/2019/07/04/how-ai-can-strengthen-and-defend-democracy/)  

[^Jackson_2018_a]: Joni R. Jackson, (2018). Algorithmic Bias. Journal of Leadership, Accountability and Ethics, 15(4), 5565. [[Jackson_AlgorithmicBias_2018]] [OA](https://engine.scholarcy.com/oa_version?query=Jackson%2C%20Joni%20R.%20Algorithmic%20Bias%202018) [GScholar](https://scholar.google.co.uk/scholar?q=Jackson%2C%20Joni%20R.%20Algorithmic%20Bias%202018) [Scite](https://engine.scholarcy.com/scite_url?query=Jackson%2C%20Joni%20R.%20Algorithmic%20Bias%202018)

[^Jordan_2015_a]: Jordan, M. I., and T. M. Mitchell, “Machine Learning: Trends, Perspectives, and Prospects,” Science, Vol. 349, No. 6245, 2015, pp. 255–260. [[Jordan_MachineLearningTrendsPerspectivesProspects_2015]] [OA](https://engine.scholarcy.com/oa_version?query=Jordan%2C%20M.I.%20Mitchell%2C%20T.M.%20Machine%20Learning%3A%20Trends%2C%20Perspectives%2C%20and%20Prospects%2C%202015) [GScholar](https://scholar.google.co.uk/scholar?q=Jordan%2C%20M.I.%20Mitchell%2C%20T.M.%20Machine%20Learning%3A%20Trends%2C%20Perspectives%2C%20and%20Prospects%2C%202015) [Scite](https://engine.scholarcy.com/scite_url?query=Jordan%2C%20M.I.%20Mitchell%2C%20T.M.%20Machine%20Learning%3A%20Trends%2C%20Perspectives%2C%20and%20Prospects%2C%202015)

[^Jordan_2015_a]: Jordan, M. I., and T. M. Mitchell, “Machine Learning: Trends, Perspectives, and Prospects,” Science, Vol. 349, No. 6245, 2015, pp. 255–260. Kahn Jr. Peter H., Kanda, Takayuki, & Ishiguro, Hiroshi et al., “Do People Hold a Humanoid Robot Morally Accountable for the Harm it Causes?”, Attitudes and Responses to Social Robots, March 2012. Kahn Jr., Peter H., Kanda, Takayuki, & Ishiguro, Hiroshi, et al., “’Robovie, You’ll Have to Go into the Closet Now’: Children’s Social and Moral Relationships with a Humanoid Robot”, Developmental Psychology 48(2), 2012. [[Jordan_MachineLearningTrendsPerspectivesProspects_2015]] [OA](https://engine.scholarcy.com/oa_version?query=Jordan%2C%20M.I.%20Mitchell%2C%20T.M.%20Machine%20Learning%3A%20Trends%2C%20Perspectives%2C%20and%20Prospects%2C%202015-03) [GScholar](https://scholar.google.co.uk/scholar?q=Jordan%2C%20M.I.%20Mitchell%2C%20T.M.%20Machine%20Learning%3A%20Trends%2C%20Perspectives%2C%20and%20Prospects%2C%202015-03) [Scite](https://engine.scholarcy.com/scite_url?query=Jordan%2C%20M.I.%20Mitchell%2C%20T.M.%20Machine%20Learning%3A%20Trends%2C%20Perspectives%2C%20and%20Prospects%2C%202015-03)

[^Kamishima_2011_a]: Kamishima, Akaho, & Sakuma, 2011, p. 644; see also Dwork, Cynthia, and Aaron Roth, "The algorithmic foundations of differential privacy," Foundations and Trends in Theoretical Computer Science, Vol. 9, No. 3–4, 2014, pp. 211-407. [[Kamishima_AlgorithmicFoundationsDifferentialPrivacy_2011]] [OA](https://engine.scholarcy.com/oa_version?query=Kamishima%2C%20Akaho%20Sakuma%20The%20algorithmic%20foundations%20of%20differential%20privacy%2C%202011) [GScholar](https://scholar.google.co.uk/scholar?q=Kamishima%2C%20Akaho%20Sakuma%20The%20algorithmic%20foundations%20of%20differential%20privacy%2C%202011) [Scite](https://engine.scholarcy.com/scite_url?query=Kamishima%2C%20Akaho%20Sakuma%20The%20algorithmic%20foundations%20of%20differential%20privacy%2C%202011)

[^Kamishima_et+al_2011_b]: Kamishima, T., Akaho, S., & Sakuma, J. (2011, December). Fairness-aware learning through regularization approach. In 2011 IEEE 11th International Conference on Data Mining Workshops (pp. 643-650). IEEE. [[Kamishima_et+al_FairnessawareLearningThroughRegularizationApproach_2011]] [OA](https://engine.scholarcy.com/oa_version?query=Kamishima%2C%20T.%20Akaho%2C%20S.%20Sakuma%2C%20J.%20Fairness-aware%20learning%20through%20regularization%20approach%202011-12) [GScholar](https://scholar.google.co.uk/scholar?q=Kamishima%2C%20T.%20Akaho%2C%20S.%20Sakuma%2C%20J.%20Fairness-aware%20learning%20through%20regularization%20approach%202011-12) [Scite](https://engine.scholarcy.com/scite_url?query=Kamishima%2C%20T.%20Akaho%2C%20S.%20Sakuma%2C%20J.%20Fairness-aware%20learning%20through%20regularization%20approach%202011-12)

[^Kaushik_0000_a]: Kaushik, Preetam, “Is Artifical Intelligence the way Forward for Personal Finance,” Wired. http://wired.com/insights/2014/02/artificial-intelligence-way-forward-personal-finance/ [[Kaushik_ArtificalIntelligenceForwardPersonalFinance_0000]] [OA](http://wired.com/insights/2014/02/artificial-intelligence-way-forward-personal-finance/)  [Scite](https://engine.scholarcy.com/scite_url?query=Kaushik%2C%20Preetam%20Is%20Artifical%20Intelligence%20the%20way%20Forward%20for%20Personal%20Finance%2C)

[^Keeling_2018_a]: Keeling, Geoff, “Legal Necessity, Pareto Efficiency & Justified Killing in Autonomous Vehicle Collisions”, Ethical Theory and Moral Practice 21(2), April 2018. [[Keeling_LegalNecessityParetoEfficiency_2018]] [OA](https://engine.scholarcy.com/oa_version?query=Keeling%2C%20Geoff%20Legal%20Necessity%2C%20Pareto%20Efficiency%20%26%20Justified%20Killing%20in%20Autonomous%20Vehicle%20Collisions%202018-04) [GScholar](https://scholar.google.co.uk/scholar?q=Keeling%2C%20Geoff%20Legal%20Necessity%2C%20Pareto%20Efficiency%20%26%20Justified%20Killing%20in%20Autonomous%20Vehicle%20Collisions%202018-04) [Scite](https://engine.scholarcy.com/scite_url?query=Keeling%2C%20Geoff%20Legal%20Necessity%2C%20Pareto%20Efficiency%20%26%20Justified%20Killing%20in%20Autonomous%20Vehicle%20Collisions%202018-04)

[^Kemp_2019_a]: Kemp, Luke, “In the Age of Deepfakes, Could Virtual Actors Put Humans out of Business?,” The Guardian, Guardian News and Media, July 8, 2019. https://www.theguardian.com/film/2019/jul/03/in-the-age-of-deepfakes-could-virtual-actorsput-humans-out-of-business [[Kemp_AgeDeepfakesCouldVirtualActors_2019]] [OA](https://www.theguardian.com/film/2019/jul/03/in-the-age-of-deepfakes-could-virtual-actorsput-humans-out-of-business)  [Scite](https://engine.scholarcy.com/scite_url?query=Kemp%2C%20Luke%20In%20the%20Age%20of%20Deepfakes%2C%20Could%20Virtual%20Actors%20Put%20Humans%20out%20of%20Business%3F%2C%202019-07-08)

[^Keyes_2018_a]: Keyes, O., ‘The Misgendering Machines: Trans/HCI Implications of Automatic Gender Recognition’, Proceeding of the ACM on Human-Computer Interaction, Vol. 2, No. CSCW, Article 88, November 2018. [[Keyes_MisgenderingMachinesTranshciImplicationsAutomatic_2018]] [OA](https://scholar.google.co.uk/scholar?q=Keyes%2C%20O.%20The%20Misgendering%20Machines%3A%20Trans/HCI%20Implications%20of%20Automatic%20Gender%20Recognition%202018-11) [GScholar](https://scholar.google.co.uk/scholar?q=Keyes%2C%20O.%20The%20Misgendering%20Machines%3A%20Trans/HCI%20Implications%20of%20Automatic%20Gender%20Recognition%202018-11) 

[^King_et+al_2019_a]: King, T. C., Aggarwal, N., Taddeo, M., and Floridi, L., ‘Artificial Intelligence Crime: An Interdisciplinary Analysis of Foreseeable Threats and Solutions’, Science and Engineering Ethics, 2019. https://doi.org/10.1007/s11948-018-00081-0. [[King_et+al_ArtificialIntelligenceCrimeInterdisciplinaryAnalysis_2019]] [OA](https://doi.org/10.1007/s11948-018-00081-0)  [Scite](https://scite.ai/reports/10.1007/s11948-018-00081-0)

[^Kirschgens_et+al_2018_a]: Kirschgens, Laura, Ugarte, Irati & Uriarte, Endika et al., “Robot Hazards: From Safety to Security”, Whitepaper, 2018. [[Kirschgens_et+al_RobotHazardsFromSafetySecurity_2018]] [OA](https://engine.scholarcy.com/oa_version?query=Kirschgens%2C%20Laura%20Ugarte%2C%20Irati%20Uriarte%2C%20Endika%20Robot%20Hazards%3A%20From%20Safety%20to%20Security%202018) [GScholar](https://scholar.google.co.uk/scholar?q=Kirschgens%2C%20Laura%20Ugarte%2C%20Irati%20Uriarte%2C%20Endika%20Robot%20Hazards%3A%20From%20Safety%20to%20Security%202018) [Scite](https://engine.scholarcy.com/scite_url?query=Kirschgens%2C%20Laura%20Ugarte%2C%20Irati%20Uriarte%2C%20Endika%20Robot%20Hazards%3A%20From%20Safety%20to%20Security%202018)

[^Kitano_2016_a]: Kitano, Hiroaki, "Artificial intelligence to win the nobel prize and beyond: Creating the engine for scientific discovery," AI magazine Vol. 37, No. 1, 2016, pp. 39-49. DOI: https://doi.org/10.1609/aimag.v37i1.2642. [[Kitano_ArtificialIntelligenceNobelPrizeBeyond_2016]] [OA](https://doi.org/10.1609/aimag.v37i1.2642)  [Scite](https://scite.ai/reports/10.1609/aimag.v37i1.2642)

[^Kitchin_2014_a]: Kitchin, Rob, "The real-time city? Big data and smart urbanism," GeoJournal, Vol. 79, No. 1, 2014, pp. 1-14. [[Kitchin_RealtimeCityBigDataSmart_2014]] [OA](https://engine.scholarcy.com/oa_version?query=Kitchin%2C%20Rob%20The%20real-time%20city%3F%20Big%20data%20and%20smart%20urbanism%2C%202014) [GScholar](https://scholar.google.co.uk/scholar?q=Kitchin%2C%20Rob%20The%20real-time%20city%3F%20Big%20data%20and%20smart%20urbanism%2C%202014) [Scite](https://engine.scholarcy.com/scite_url?query=Kitchin%2C%20Rob%20The%20real-time%20city%3F%20Big%20data%20and%20smart%20urbanism%2C%202014)

[^Kladitis_2010_a]: Kladitis, Paul E., “How small is too small? True microrobots and nanorobots for military applications in 2035”, Research Report, Maxwell Air Force Base, Alabama, April 2010 and see also references in the section on Swarm robots. [[Kladitis_SmallSmallTrueMicrorobotsNanorobots_2010]] [OA](https://engine.scholarcy.com/oa_version?query=Kladitis%2C%20Paul%20E.%20How%20small%20is%20too%20small%3F%20True%20microrobots%20and%20nanorobots%20for%20military%20applications%20in%202035%202010-04) [GScholar](https://scholar.google.co.uk/scholar?q=Kladitis%2C%20Paul%20E.%20How%20small%20is%20too%20small%3F%20True%20microrobots%20and%20nanorobots%20for%20military%20applications%20in%202035%202010-04) [Scite](https://engine.scholarcy.com/scite_url?query=Kladitis%2C%20Paul%20E.%20How%20small%20is%20too%20small%3F%20True%20microrobots%20and%20nanorobots%20for%20military%20applications%20in%202035%202010-04)

[^Knight_2000_a]: Knight, Kevin, and Irene Langkilde, "Preserving ambiguities in generation via automata intersection," AAAI/IAAI, pp. 697-702, 2000. [[Knight_PreservingAmbiguitiesGenerationAutomataIntersection_2000]] [OA](https://engine.scholarcy.com/oa_version?query=Knight%2C%20Kevin%20Langkilde%2C%20Irene%20Preserving%20ambiguities%20in%20generation%20via%20automata%20intersection%2C%202000) [GScholar](https://scholar.google.co.uk/scholar?q=Knight%2C%20Kevin%20Langkilde%2C%20Irene%20Preserving%20ambiguities%20in%20generation%20via%20automata%20intersection%2C%202000) [Scite](https://engine.scholarcy.com/scite_url?query=Knight%2C%20Kevin%20Langkilde%2C%20Irene%20Preserving%20ambiguities%20in%20generation%20via%20automata%20intersection%2C%202000)

[^Donald_1974_a]: Knuth. Donald. “Computer Science and Its Relation to Mathematics,” American Mathematical Monthly, Vol. 81, No. 4, 1974, pp. 323-343. [[Donald_ComputerScienceItsRelationMathematics_1974]] [OA](https://engine.scholarcy.com/oa_version?query=Donald%2C%20Knuth%20Computer%20Science%20and%20Its%20Relation%20to%20Mathematics%2C%201974) [GScholar](https://scholar.google.co.uk/scholar?q=Donald%2C%20Knuth%20Computer%20Science%20and%20Its%20Relation%20to%20Mathematics%2C%201974) [Scite](https://engine.scholarcy.com/scite_url?query=Donald%2C%20Knuth%20Computer%20Science%20and%20Its%20Relation%20to%20Mathematics%2C%201974)

[^Kobie_2018_a]: Kobie, Nicole, “The questionable ethics of treating autistic children with robots,” WIRED, July 18, 2018. https://www.wired.co.uk/article/autisim-children-treatment-robots [[Kobie_QuestionableEthicsTreatingAutisticChildren_2018]] [OA](https://www.wired.co.uk/article/autisim-children-treatment-robots)  [Scite](https://engine.scholarcy.com/scite_url?query=Kobie%2C%20Nicole%20The%20questionable%20ethics%20of%20treating%20autistic%20children%20with%20robots%2C%202018-07-18)

[^Koene_et+al_2019_a]: Koene, Ansgar, Chris Clifton, Yohko Hatada, Helena Webb, Menisha Patel, Caio Machado, Jack LaViolette, Rashida Richardson, and Dillon Reisman, “A governance framework for algorithmic accountability and transparency,” European Parliamentary Research Service, 2019. http://www.europarl.europa.eu/thinktank/en/document.html?reference=EPRS_STU(2019)62426 2 [[Koene_et+al_GovernanceFrameworkAlgorithmicAccountabilityTransparency_2019]] [OA](http://www.europarl.europa.eu/thinktank/en/document.html?reference=EPRS_STU(2019)62426)  [Scite](https://engine.scholarcy.com/scite_url?query=Koene%2C%20Ansgar%20Clifton%2C%20Chris%20Hatada%2C%20Yohko%20Webb%2C%20Helena%20A%20governance%20framework%20for%20algorithmic%20accountability%20and%20transparency%2C%202019)

[^Prosperity_2019_a]: Prosperity, February 2019. https://econfip.org/policy-brief/labor-in-the-age-of-automation-andartificial-intelligence/. Korinek, Anton, and Joseph E. Stiglitz, “Artificial intelligence and its implications for income distribution and unemployment,” in Goldfarb, Avi, Joshua Gans, and Ajay Agrawal (eds.), The Economics of Artificial Intelligence: An Agenda, University of Chicago Press, 2019. [[Prosperity_StiglitzartificialIntelligenceImplicationsIncome_2019]] [OA](https://econfip.org/policy-brief/labor-in-the-age-of-automation-andartificial-intelligence/)  [Scite](https://engine.scholarcy.com/scite_url?query=Prosperity%20Stiglitz%2C%20%E2%80%9CArtificial%20intelligence%20and%20its%20implications%20for%20income%20distribution%20and%20unemployment%2C%E2%80%9D%202019-02)

[^Koertner_2016_a]: Körtner, T., “Ethical challenges in the use of social service robots for elderly people,” Zeitschrift Für Gerontologie Und Geriatrie, Vol. 49, No. 4, 2016, pp. 303–307. [[Koertner_EthicalChallengesSocialServiceRobots_2016]] [OA](https://engine.scholarcy.com/oa_version?query=K%C3%B6rtner%2C%20T.%20Ethical%20challenges%20in%20the%20use%20of%20social%20service%20robots%20for%20elderly%20people%2C%202016) [GScholar](https://scholar.google.co.uk/scholar?q=K%C3%B6rtner%2C%20T.%20Ethical%20challenges%20in%20the%20use%20of%20social%20service%20robots%20for%20elderly%20people%2C%202016) [Scite](https://engine.scholarcy.com/scite_url?query=K%C3%B6rtner%2C%20T.%20Ethical%20challenges%20in%20the%20use%20of%20social%20service%20robots%20for%20elderly%20people%2C%202016)

[^Krach_et+al_2010_a]: Krach, Sören, Frieder M. Paulus, Maren Bodden, and Tilo Kircher, “The Rewarding Nature of Social Interactions,” Frontiers in behavioral neuroscience, Vol. 4, p. 22, 2010. [[Krach_et+al_RewardingNatureSocialInteractions_2010]] [OA](https://engine.scholarcy.com/oa_version?query=Krach%2C%20S%C3%B6ren%20Paulus%2C%20Frieder%20M.%20Bodden%2C%20Maren%20Kircher%2C%20Tilo%20The%20Rewarding%20Nature%20of%20Social%20Interactions%2C%202010) [GScholar](https://scholar.google.co.uk/scholar?q=Krach%2C%20S%C3%B6ren%20Paulus%2C%20Frieder%20M.%20Bodden%2C%20Maren%20Kircher%2C%20Tilo%20The%20Rewarding%20Nature%20of%20Social%20Interactions%2C%202010) [Scite](https://engine.scholarcy.com/scite_url?query=Krach%2C%20S%C3%B6ren%20Paulus%2C%20Frieder%20M.%20Bodden%2C%20Maren%20Kircher%2C%20Tilo%20The%20Rewarding%20Nature%20of%20Social%20Interactions%2C%202010)

[^Kraemer_et+al_2011_a]: Kraemer, Felicitas, Kees van Overveld, and Martin Peterson, “Is there an ethics of algorithms?,” Ethics and Information Technology, Volume 13, Issue 3, 2011, pp. 251–260. [[Kraemer_et+al_ThereEthicsAlgorithms_2011]] [OA](https://engine.scholarcy.com/oa_version?query=Kraemer%2C%20Felicitas%20van%20Overveld%2C%20Kees%20Peterson%2C%20Martin%20Is%20there%20an%20ethics%20of%20algorithms%3F%2C%202011) [GScholar](https://scholar.google.co.uk/scholar?q=Kraemer%2C%20Felicitas%20van%20Overveld%2C%20Kees%20Peterson%2C%20Martin%20Is%20there%20an%20ethics%20of%20algorithms%3F%2C%202011) [Scite](https://engine.scholarcy.com/scite_url?query=Kraemer%2C%20Felicitas%20van%20Overveld%2C%20Kees%20Peterson%2C%20Martin%20Is%20there%20an%20ethics%20of%20algorithms%3F%2C%202011)

[^Lum_2016_a]: Kristian Lum and James Johndrow, ‘A Statistical Framework For Fair Predictive Algorithms’, 2016, 1 <https://arxiv.org/abs/1610.08077> [accessed 1 July 2019]. [[Lum_StatisticalFrameworkForFairPredictive_2016]] [OA](https://arxiv.org/abs/1610.08077>)  

[^Kulić_2007_a]: Kulić, Dana, and Elizabeth Croft. "Pre-collision safety strategies for human-robot interaction." Autonomous Robots22, no. 2 (2007): 149-164. [[Kulić_collisionSafetyStrategiesHumanrobotInteraction_2007]] [OA](https://engine.scholarcy.com/oa_version?query=Kuli%C4%87%2C%20Dana%20Croft%2C%20Elizabeth%20Pre-collision%20safety%20strategies%20for%20human-robot%20interaction.%202007) [GScholar](https://scholar.google.co.uk/scholar?q=Kuli%C4%87%2C%20Dana%20Croft%2C%20Elizabeth%20Pre-collision%20safety%20strategies%20for%20human-robot%20interaction.%202007) [Scite](https://engine.scholarcy.com/scite_url?query=Kuli%C4%87%2C%20Dana%20Croft%2C%20Elizabeth%20Pre-collision%20safety%20strategies%20for%20human-robot%20interaction.%202007)

[^Kulkarni_et+al_2018_a]: Kulkarni, Anagha, Chakraborti, Tathagata & Zha, Yantian et al., “Explicable Robot Planning as Minimized Distance from Expected Behavior”, Cornell University, July 2018. [[Kulkarni_et+al_ExplicableRobotPlanningMinimizedDistance_2018]] [OA](https://scholar.google.co.uk/scholar?q=Kulkarni%2C%20Anagha%20Chakraborti%2C%20Tathagata%20Zha%2C%20Yantian%20Explicable%20Robot%20Planning%20as%20Minimized%20Distance%20from%20Expected%20Behavior%202018-07) [GScholar](https://scholar.google.co.uk/scholar?q=Kulkarni%2C%20Anagha%20Chakraborti%2C%20Tathagata%20Zha%2C%20Yantian%20Explicable%20Robot%20Planning%20as%20Minimized%20Distance%20from%20Expected%20Behavior%202018-07) 

[^Lachow_2017_a]: Lachow, Irving, “The Upside and Downside of Swarming Drones,” Bulletin of the Atomic Scientists, Vol. 73, No. 2, 2017, p. 96; Bredeche, Nicolas, Haasdijk, Evert, and Prieto, Abraham, “Embodied Evolution in Collective Robotics: A Review,” Frontiers in Robotics and AI, Vol. 5, No. 12, 2018; Magnuson, Stew, “Military Beefs Up Research Into Swarming Drones,” National Defense Magazine, March 1, 2016. https://www.nationaldefensemagazine.org/articles/2016/2/29/2016march-military-beefs-upresearch-into-swarming-drones [[Lachow_UpsideDownsideSwarmingDrones_2017]] [OA](https://www.nationaldefensemagazine.org/articles/2016/2/29/2016march-military-beefs-upresearch-into-swarming-drones)  [Scite](https://engine.scholarcy.com/scite_url?query=Lachow%2C%20Irving%20The%20Upside%20and%20Downside%20of%20Swarming%20Drones%2C%202017-03-01)

[^Lakoff_1987_a]: Lakoff, G. (1987). Women, Fire, and Dangerous Things: What Categories Reveal About the Mind. University of Chicago Press. [[Lakoff_WomenFireDangerousThingsWhat_1987]] [OA](https://scholar.google.co.uk/scholar?q=Lakoff%2C%20G.%20Women%2C%20Fire%2C%20and%20Dangerous%20Things%3A%20What%20Categories%20Reveal%20About%20the%20Mind%201987) [GScholar](https://scholar.google.co.uk/scholar?q=Lakoff%2C%20G.%20Women%2C%20Fire%2C%20and%20Dangerous%20Things%3A%20What%20Categories%20Reveal%20About%20the%20Mind%201987) 

[^Lambrecht_2018_a]: Lambrecht, Anja and Catherine Tucker, “Algorithmic Bias? An Empirical Study into Apparent GenderBased Discrimination in the Display of STEM Career Ads,” SSRN, 2018, retrievable at https://ssrn.com/abstract=2852260 or http://dx.doi.org/10.2139/ssrn.2852260. [[Lambrecht_AlgorithmicBiasAnEmpiricalStudy_2018]] [OA](https://doi.org/10.2139/ssrn.2852260)  [Scite](https://scite.ai/reports/10.2139/ssrn.2852260)

[^Langheinrich_2001_a]: Langheinrich, Marc, “Privacy by Design — Principles of Privacy-Aware Ubiquitous Systems,” Ubicomp 2001: Ubiquitous Computing Lecture Notes in Computer Science, 2001, pp. 273–291., p. 6. [[Langheinrich_PrivacyDesignPrinciplesPrivacyaware_2001]] [OA](https://engine.scholarcy.com/oa_version?query=Langheinrich%2C%20Marc%20Privacy%20by%20Design%20%E2%80%94%20Principles%20of%20Privacy-Aware%20Ubiquitous%20Systems%2C%202001) [GScholar](https://scholar.google.co.uk/scholar?q=Langheinrich%2C%20Marc%20Privacy%20by%20Design%20%E2%80%94%20Principles%20of%20Privacy-Aware%20Ubiquitous%20Systems%2C%202001) [Scite](https://engine.scholarcy.com/scite_url?query=Langheinrich%2C%20Marc%20Privacy%20by%20Design%20%E2%80%94%20Principles%20of%20Privacy-Aware%20Ubiquitous%20Systems%2C%202001)

[^Lau_et+al_2018_a]: Lau, Josephine, Benjamin Zimmerman, and Florian Schaub, "Alexa, are you listening?: Privacy perceptions, concerns and privacy-seeking behaviors with smart speakers," Proceedings of the ACM on Human-Computer Interaction, Vol 2, No. CSCW, 2018, p. 102. [[Lau_et+al_AlexaListeningPrivacyPerceptionsConcerns_2018]] [OA](https://engine.scholarcy.com/oa_version?query=Lau%2C%20Josephine%20Zimmerman%2C%20Benjamin%20Schaub%2C%20Florian%20Alexa%2C%20are%20you%20listening%3F%3A%20Privacy%20perceptions%2C%20concerns%20and%20privacy-seeking%20behaviors%20with%20smart%20speakers%2C%202018) [GScholar](https://scholar.google.co.uk/scholar?q=Lau%2C%20Josephine%20Zimmerman%2C%20Benjamin%20Schaub%2C%20Florian%20Alexa%2C%20are%20you%20listening%3F%3A%20Privacy%20perceptions%2C%20concerns%20and%20privacy-seeking%20behaviors%20with%20smart%20speakers%2C%202018) [Scite](https://engine.scholarcy.com/scite_url?query=Lau%2C%20Josephine%20Zimmerman%2C%20Benjamin%20Schaub%2C%20Florian%20Alexa%2C%20are%20you%20listening%3F%3A%20Privacy%20perceptions%2C%20concerns%20and%20privacy-seeking%20behaviors%20with%20smart%20speakers%2C%202018)

[^Leavy_2018_a]: Leavy, Susan, “Gender bias in artificial intelligence: the need for diversity and gender theory in machine learning,” 2018 ACM/IEEE 1st International Workshop on Gender Equality in Software Engineering. DOI: 10.1145/3195570.3195580. Lee, Byounggwan, Ohkyun Kwon, Inseong Lee, and Jinwoo Kim, "Companionship with smart home devices: The impact of social connectedness and interaction types on perceived social support and companionship in smart homes," Computers in Human Behavior, Vol 75, 2017, pp. 922-934. [[Leavy_GenderBiasArtificialIntelligenceNeed_2018]] [OA](https://doi.org/10.1145/3195570.3195580)  [Scite](https://scite.ai/reports/10.1145/3195570.3195580)

[^Lee_2004_a]: Lee, J. D., and See, K. A., ‘Trust in Automation: Designing for Appropriate Reliance’, Human Factors, Vol. 46, No. 1, pp. 50-80, Spring 2004. [[Lee_TrustAutomationDesigningAppropriateReliance_2004]] [OA](https://engine.scholarcy.com/oa_version?query=Lee%2C%20J.D.%20See%2C%20K.A.%20Trust%20in%20Automation%3A%20Designing%20for%20Appropriate%20Reliance%202004) [GScholar](https://scholar.google.co.uk/scholar?q=Lee%2C%20J.D.%20See%2C%20K.A.%20Trust%20in%20Automation%3A%20Designing%20for%20Appropriate%20Reliance%202004) [Scite](https://engine.scholarcy.com/scite_url?query=Lee%2C%20J.D.%20See%2C%20K.A.%20Trust%20in%20Automation%3A%20Designing%20for%20Appropriate%20Reliance%202004)

[^Leggett_2018_a]: Leggett, Theo, “Who is to Blame for ‘Self-Driving Car’ Deaths?”, BBC, May 2018. [[Leggett_BlameselfdrivingCarDeaths_2018]] [OA](https://engine.scholarcy.com/oa_version?query=Leggett%2C%20Theo%20Who%20is%20to%20Blame%20for%20%E2%80%98Self-Driving%20Car%E2%80%99%20Deaths%3F%202018-05) [GScholar](https://scholar.google.co.uk/scholar?q=Leggett%2C%20Theo%20Who%20is%20to%20Blame%20for%20%E2%80%98Self-Driving%20Car%E2%80%99%20Deaths%3F%202018-05) [Scite](https://engine.scholarcy.com/scite_url?query=Leggett%2C%20Theo%20Who%20is%20to%20Blame%20for%20%E2%80%98Self-Driving%20Car%E2%80%99%20Deaths%3F%202018-05)

[^Lei_et+al_2016_a]: Lei, Tao, Regina Barzilay, and Tommi Jaakkola, “Rationalizing Neural Predictions,” Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, 2016. [[Lei_et+al_RationalizingNeuralPredictions_2016]] [OA](https://engine.scholarcy.com/oa_version?query=Lei%2C%20Tao%20Barzilay%2C%20Regina%20Jaakkola%2C%20Tommi%20Rationalizing%20Neural%20Predictions%2C%202016) [GScholar](https://scholar.google.co.uk/scholar?q=Lei%2C%20Tao%20Barzilay%2C%20Regina%20Jaakkola%2C%20Tommi%20Rationalizing%20Neural%20Predictions%2C%202016) [Scite](https://engine.scholarcy.com/scite_url?query=Lei%2C%20Tao%20Barzilay%2C%20Regina%20Jaakkola%2C%20Tommi%20Rationalizing%20Neural%20Predictions%2C%202016)

[^Leidner_2017_a]: Leidner, Jochen L., and Vassilis Plachouras, “Ethical by Design: Ethics Best Practices for Natural Language Processing,” Proceedings of the First ACL Workshop on Ethics in Natural Language Processing, 2017. [[Leidner_EthicalDesignEthicsBestPractices_2017]] [OA](https://engine.scholarcy.com/oa_version?query=Leidner%2C%20Jochen%20L.%20Plachouras%2C%20Vassilis%20Ethical%20by%20Design%3A%20Ethics%20Best%20Practices%20for%20Natural%20Language%20Processing%2C%202017) [GScholar](https://scholar.google.co.uk/scholar?q=Leidner%2C%20Jochen%20L.%20Plachouras%2C%20Vassilis%20Ethical%20by%20Design%3A%20Ethics%20Best%20Practices%20for%20Natural%20Language%20Processing%2C%202017) [Scite](https://engine.scholarcy.com/scite_url?query=Leidner%2C%20Jochen%20L.%20Plachouras%2C%20Vassilis%20Ethical%20by%20Design%3A%20Ethics%20Best%20Practices%20for%20Natural%20Language%20Processing%2C%202017)

[^Lenca_et+al_2017_a]: Lenca, Marcello, Kressig, Reto & Jotterand, Fabrice et al., “Proactive Ethical Design for Neuroengineering, Assistive and Rehabilitation Technologies: The Cybathlon Lesson”, Journal of Neuroengineering Rehabilitation 14, November 2017. [[Lenca_et+al_ProactiveEthicalDesignNeuroengineeringAssistive_2017]] [OA](https://engine.scholarcy.com/oa_version?query=Lenca%2C%20Marcello%20Kressig%2C%20Reto%20Jotterand%2C%20Fabrice%20Proactive%20Ethical%20Design%20for%20Neuroengineering%2C%20Assistive%20and%20Rehabilitation%20Technologies%3A%20The%20Cybathlon%20Lesson%202017-11-14) [GScholar](https://scholar.google.co.uk/scholar?q=Lenca%2C%20Marcello%20Kressig%2C%20Reto%20Jotterand%2C%20Fabrice%20Proactive%20Ethical%20Design%20for%20Neuroengineering%2C%20Assistive%20and%20Rehabilitation%20Technologies%3A%20The%20Cybathlon%20Lesson%202017-11-14) [Scite](https://engine.scholarcy.com/scite_url?query=Lenca%2C%20Marcello%20Kressig%2C%20Reto%20Jotterand%2C%20Fabrice%20Proactive%20Ethical%20Design%20for%20Neuroengineering%2C%20Assistive%20and%20Rehabilitation%20Technologies%3A%20The%20Cybathlon%20Lesson%202017-11-14)

[^Li_2015_a]: Li, Jamy, “The benefit of being physically present: A survey of experimental works comparing copresent robots, telepresent robots and virtual agents,” International Journal of Human-Computer Studies, Vol. 77, 2015, pp. 23–37. [[Li_BenefitBeingPhysicallyPresentSurvey_2015]] [OA](https://engine.scholarcy.com/oa_version?query=Li%2C%20Jamy%20The%20benefit%20of%20being%20physically%20present%3A%20A%20survey%20of%20experimental%20works%20comparing%20copresent%20robots%2C%20telepresent%20robots%20and%20virtual%20agents%2C%202015) [GScholar](https://scholar.google.co.uk/scholar?q=Li%2C%20Jamy%20The%20benefit%20of%20being%20physically%20present%3A%20A%20survey%20of%20experimental%20works%20comparing%20copresent%20robots%2C%20telepresent%20robots%20and%20virtual%20agents%2C%202015) [Scite](https://engine.scholarcy.com/scite_url?query=Li%2C%20Jamy%20The%20benefit%20of%20being%20physically%20present%3A%20A%20survey%20of%20experimental%20works%20comparing%20copresent%20robots%2C%20telepresent%20robots%20and%20virtual%20agents%2C%202015)

[^Lidynia_et+al_2017_a]: Lidynia, Chantal, Philipsen, Ralf & Ziefle, Martina, “Droning on About Drones—Acceptance of and Perceived Barriers to Drones in Civil Usage Contexts”, Advances in Human Factors in Robots and Unmanned Systems, 2017. [[Lidynia_et+al_DroningAboutDronesacceptancePerceivedBarriers_2017]] [OA](https://engine.scholarcy.com/oa_version?query=Lidynia%2C%20Chantal%20Philipsen%2C%20Ralf%20Ziefle%2C%20Martina%20Droning%20on%20About%20Drones%E2%80%94Acceptance%20of%20and%20Perceived%20Barriers%20to%20Drones%20in%20Civil%20Usage%20Contexts%202017) [GScholar](https://scholar.google.co.uk/scholar?q=Lidynia%2C%20Chantal%20Philipsen%2C%20Ralf%20Ziefle%2C%20Martina%20Droning%20on%20About%20Drones%E2%80%94Acceptance%20of%20and%20Perceived%20Barriers%20to%20Drones%20in%20Civil%20Usage%20Contexts%202017) [Scite](https://engine.scholarcy.com/scite_url?query=Lidynia%2C%20Chantal%20Philipsen%2C%20Ralf%20Ziefle%2C%20Martina%20Droning%20on%20About%20Drones%E2%80%94Acceptance%20of%20and%20Perceived%20Barriers%20to%20Drones%20in%20Civil%20Usage%20Contexts%202017)

[^Lin_2011_a]: Lin, Patrick, “Drone-Ethics Briefings: What a Leading Robot Expert Told the CIA,” The Atlantic, December 21, 2011. [[Lin_DroneethicsBriefingsWhatLeadingRobot_2011]] [OA](https://engine.scholarcy.com/oa_version?query=Lin%2C%20Patrick%20Drone-Ethics%20Briefings%3A%20What%20a%20Leading%20Robot%20Expert%20Told%20the%20CIA%2C%202011-12-21) [GScholar](https://scholar.google.co.uk/scholar?q=Lin%2C%20Patrick%20Drone-Ethics%20Briefings%3A%20What%20a%20Leading%20Robot%20Expert%20Told%20the%20CIA%2C%202011-12-21) [Scite](https://engine.scholarcy.com/scite_url?query=Lin%2C%20Patrick%20Drone-Ethics%20Briefings%3A%20What%20a%20Leading%20Robot%20Expert%20Told%20the%20CIA%2C%202011-12-21)

[^Social_2016_a]: Social Aspects, 2016. [[Social__2016]] [OA](https://scholar.google.co.uk/scholar?q=Social%20Aspects%202016) [GScholar](https://scholar.google.co.uk/scholar?q=Social%20Aspects%202016) 

[^World”_2011_a]: World”, Artificial Intelligence, Vol. 175, 2011, p. 947. [[World”_World_2011]] [OA](https://engine.scholarcy.com/oa_version?query=World%20Artificial%20Intelligence%20Vol%20175%202011%20p%20947) [GScholar](https://scholar.google.co.uk/scholar?q=World%20Artificial%20Intelligence%20Vol%20175%202011%20p%20947) [Scite](https://engine.scholarcy.com/scite_url?query=World%20Artificial%20Intelligence%20Vol%20175%202011%20p%20947)

[^Robot_2012_a]: Robot Ethics: The Ethical and Social Implications of Robotics, MIT Press, 2012. Lindsey Barrett, “Reasonably Suspicious Algorithms: Predictive Policing at the United States Border”, N.Y.U. Review of Law and Social Change, Vol. 41, 2017, p. 343. [[Robot_RobotEthicsEthicalSocialImplications_2012]] [OA](https://engine.scholarcy.com/oa_version?query=Robot%20Ethics%20The%20Ethical%20and%20Social%20Implications%20of%20Robotics%20MIT%20Press%202012%20Lindsey%20Barrett%20Reasonably%20Suspicious%20Algorithms%20Predictive%20Policing%20at%20the%20United%20States%20Border%20NYU%20Review%20of%20Law%20and%20Social%20Change%20Vol%2041%202017%20p%20343) [GScholar](https://scholar.google.co.uk/scholar?q=Robot%20Ethics%20The%20Ethical%20and%20Social%20Implications%20of%20Robotics%20MIT%20Press%202012%20Lindsey%20Barrett%20Reasonably%20Suspicious%20Algorithms%20Predictive%20Policing%20at%20the%20United%20States%20Border%20NYU%20Review%20of%20Law%20and%20Social%20Change%20Vol%2041%202017%20p%20343) [Scite](https://engine.scholarcy.com/scite_url?query=Robot%20Ethics%20The%20Ethical%20and%20Social%20Implications%20of%20Robotics%20MIT%20Press%202012%20Lindsey%20Barrett%20Reasonably%20Suspicious%20Algorithms%20Predictive%20Policing%20at%20the%20United%20States%20Border%20NYU%20Review%20of%20Law%20and%20Social%20Change%20Vol%2041%202017%20p%20343)

[^_2018_a]: 10, 2018, pp. 36–43. [[__2018]] [OA](https://scholar.google.co.uk/scholar?q=2018%20pp%203643) [GScholar](https://scholar.google.co.uk/scholar?q=2018%20pp%203643) 

[^Lipton_0000_a]: Lipton, Zachary C., “The Mythos of Model Interpretability,” Communications of the ACM, Vol. 61, No. [[Lipton_MythosModelInterpretability_0000]] [OA](https://engine.scholarcy.com/oa_version?query=Lipton%2C%20Zachary%20C.%20The%20Mythos%20of%20Model%20Interpretability%2C) [GScholar](https://scholar.google.co.uk/scholar?q=Lipton%2C%20Zachary%20C.%20The%20Mythos%20of%20Model%20Interpretability%2C) [Scite](https://engine.scholarcy.com/scite_url?query=Lipton%2C%20Zachary%20C.%20The%20Mythos%20of%20Model%20Interpretability%2C)

[^Doshi-Velez_2017_a]: 10, 2018, pp. 36–43.; Doshi-Velez, Finale, and Been Kim, "Towards a rigorous science of interpretable machine learning," arXiv preprint arXiv:1702.08608, 2017. [[Doshi-Velez_TowardsRigorousScienceInterpretableMachine_2017]] [OA](https://export.arxiv.org/pdf/1702.08608)  

[^Liu_et+al_2019_a]: Liu, Hui, Yinghui Huang, Zichao Wang, Kai Liu, Xiangen Hu, and Weijun Wang, “Personality or Value: A Comparative Study of Psychographic Segmentation Based on an Online Review Enhanced Recommender System,” Applied Sciences, Vol. 9, No. 10, 2019, pp. 1992, DOI:10.3390/app9101992. Lubin, Gus, “The Incredible Story Of How Target Exposed A Teen Girl's Pregnancy,” Business Insider, February 12, 2012, https://www.businessinsider.com/the-incredible-story-of-how-targetexposed-a-teen-girls-pregnancy-2012-2?international=true&r=US&IR=T. Lutz, Christoph & Tamò, Aurelia, “Privacy and Healthcare Robots—An ANT Analysis”, 2016. Lyria Bennett Moses and Janet Chan, ‘Algorithmic Prediction in Policing: Assumptions, Evaluation, and Accountability’, Policing and Society, Vol.28, No.7, 2018, p.806. [[Liu_et+al_PersonalityValueComparativeStudyPsychographic_2019]] [OA](https://doi.org/10.3390/app9101992)  [Scite](https://scite.ai/reports/10.3390/app9101992)

[^Macnish_2012_a]: Macnish, K., ‘Unblinking Eyes: The Ethics of Automating Surveillance’, Ethics and Information Technology, Vol. 14, No. 2, pp. 151-167, June 2012. [[Macnish_UnblinkingEyesEthicsAutomatingSurveillance_2012]] [OA](https://engine.scholarcy.com/oa_version?query=Macnish%2C%20K.%20Unblinking%20Eyes%3A%20The%20Ethics%20of%20Automating%20Surveillance%202012-06) [GScholar](https://scholar.google.co.uk/scholar?q=Macnish%2C%20K.%20Unblinking%20Eyes%3A%20The%20Ethics%20of%20Automating%20Surveillance%202012-06) [Scite](https://engine.scholarcy.com/scite_url?query=Macnish%2C%20K.%20Unblinking%20Eyes%3A%20The%20Ethics%20of%20Automating%20Surveillance%202012-06)

[^Mainbot_2018_a]: Mainbot, “Industrial Objectives”, accessed December 2018. [[Mainbot_IndustrialObjectives_2018]] [OA](https://scholar.google.co.uk/scholar?q=Mainbot%20Industrial%20Objectives%202018) [GScholar](https://scholar.google.co.uk/scholar?q=Mainbot%20Industrial%20Objectives%202018) 

[^Mariano_et+al_2017_a]: Mariano, Pedro, Salem, Ziad & Mills, Rob et al., “Design Choice for Adapting Bio-Hybrid Systems with Evolutionary Computation”, GECCO 2017 Companion, July 2017. [[Mariano_et+al_DesignChoiceAdaptingBiohybridSystems_2017]] [OA](https://engine.scholarcy.com/oa_version?query=Mariano%2C%20Pedro%20Salem%2C%20Ziad%20Mills%2C%20Rob%20Design%20Choice%20for%20Adapting%20Bio-Hybrid%20Systems%20with%20Evolutionary%20Computation%202017-07) [GScholar](https://scholar.google.co.uk/scholar?q=Mariano%2C%20Pedro%20Salem%2C%20Ziad%20Mills%2C%20Rob%20Design%20Choice%20for%20Adapting%20Bio-Hybrid%20Systems%20with%20Evolutionary%20Computation%202017-07) [Scite](https://engine.scholarcy.com/scite_url?query=Mariano%2C%20Pedro%20Salem%2C%20Ziad%20Mills%2C%20Rob%20Design%20Choice%20for%20Adapting%20Bio-Hybrid%20Systems%20with%20Evolutionary%20Computation%202017-07)

[^Coeckelbergh_2011_a]: Mark Coeckelbergh, “From Killer Machines to Doctrines and Swarms, or Why Ethics of Military Robotics Is Not (Necessarily) About Robots,” Philosophy & Technology, Vol. 24, 2011, p. 273. [[Coeckelbergh_FromKillerMachinesDoctrinesSwarms_2011]] [OA](https://engine.scholarcy.com/oa_version?query=Coeckelbergh%2C%20Mark%20From%20Killer%20Machines%20to%20Doctrines%20and%20Swarms%2C%20or%20Why%20Ethics%20of%20Military%20Robotics%20Is%20Not%20%28Necessarily%29%20About%20Robots%2C%202011) [GScholar](https://scholar.google.co.uk/scholar?q=Coeckelbergh%2C%20Mark%20From%20Killer%20Machines%20to%20Doctrines%20and%20Swarms%2C%20or%20Why%20Ethics%20of%20Military%20Robotics%20Is%20Not%20%28Necessarily%29%20About%20Robots%2C%202011) [Scite](https://engine.scholarcy.com/scite_url?query=Coeckelbergh%2C%20Mark%20From%20Killer%20Machines%20to%20Doctrines%20and%20Swarms%2C%20or%20Why%20Ethics%20of%20Military%20Robotics%20Is%20Not%20%28Necessarily%29%20About%20Robots%2C%202011)

[^Markou_2017_a]: Markou, Christopher, “Why Using AI to Sentence Criminals Is a Dangerous Idea,” The Conversation, May 2017. http://theconversation.com/why-using-ai-to-sentence-criminals-is-a-dangerous-idea77734 [[Markou_UsingAiSentenceCriminalsIs_2017]] [OA](http://theconversation.com/why-using-ai-to-sentence-criminals-is-a-dangerous-idea77734)  [Scite](https://engine.scholarcy.com/scite_url?query=Markou%2C%20Christopher%20Why%20Using%20AI%20to%20Sentence%20Criminals%20Is%20a%20Dangerous%20Idea%2C%202017-05)

[^Martin_2003_a]: Martin, Ron, and Philip S. Morrison (eds.), Geographies of labour market inequality, Routledge, London and New York, 2003. [[Martin__2003]] [OA](https://engine.scholarcy.com/oa_version?query=Martin%20Ron%20and%20Philip%20S%20Morrison%20eds%20Geographies%20of%20labour%20market%20inequality%20Routledge%20London%20and%20New%20York%202003) [GScholar](https://scholar.google.co.uk/scholar?q=Martin%20Ron%20and%20Philip%20S%20Morrison%20eds%20Geographies%20of%20labour%20market%20inequality%20Routledge%20London%20and%20New%20York%202003) [Scite](https://engine.scholarcy.com/scite_url?query=Martin%20Ron%20and%20Philip%20S%20Morrison%20eds%20Geographies%20of%20labour%20market%20inequality%20Routledge%20London%20and%20New%20York%202003)

[^Marx_2016_a]: Marx, Paris, “Humans to serve the rich, robots to serve the poor,” Medium, August 28, 2016. https://medium.com/radical-urbanist/humans-to-serve-the-rich-robots-to-serve-the-poor6e2efc95c1b4 [[Marx_HumansServeRichRobotsServe_2016]] [OA](https://medium.com/radical-urbanist/humans-to-serve-the-rich-robots-to-serve-the-poor6e2efc95c1b4)  [Scite](https://engine.scholarcy.com/scite_url?query=Marx%2C%20Paris%20Humans%20to%20serve%20the%20rich%2C%20robots%20to%20serve%20the%20poor%2C%202016-08-28)

[^Automata,”_2004_a]: Automata,” Ethics and Information Technology, Vol. 6, No. 3, 2004, pp. 175–183. [[Automata,”_Automata_2004]] [OA](https://engine.scholarcy.com/oa_version?query=Automata%20Ethics%20and%20Information%20Technology%20Vol%206%20No%203%202004%20pp%20175183) [GScholar](https://scholar.google.co.uk/scholar?q=Automata%20Ethics%20and%20Information%20Technology%20Vol%206%20No%203%202004%20pp%20175183) [Scite](https://engine.scholarcy.com/scite_url?query=Automata%20Ethics%20and%20Information%20Technology%20Vol%206%20No%203%202004%20pp%20175183)

[^Matz_et+al_2017_a]: Matz, Sandra C., Michal Kosinski, Gideon Nave, and David J. Stillwell, “Psychological targeting as an effective approach to digital mass persuasion,” Proceedings of the National Academy of Sciences of the United States of America Vol. 114, No. 48, 2017, pp. 12714–12719. Maurice, Pauline, Allienne, Ludivine & Malaise, Adrien et al, “Ethical and Social Considerations for the Introduction of Human-Centered Technologies at Work”, IEEE Workshop on Advanced Robotics and its Social Impacts, 2018. [[Matz_et+al_PsychologicalTargetingEffectiveApproachDigital_2017]] [OA](https://engine.scholarcy.com/oa_version?query=Matz%2C%20Sandra%20C.%20Kosinski%2C%20Michal%20Nave%2C%20Gideon%20Stillwell%2C%20David%20J.%20Psychological%20targeting%20as%20an%20effective%20approach%20to%20digital%20mass%20persuasion%2C%202017) [GScholar](https://scholar.google.co.uk/scholar?q=Matz%2C%20Sandra%20C.%20Kosinski%2C%20Michal%20Nave%2C%20Gideon%20Stillwell%2C%20David%20J.%20Psychological%20targeting%20as%20an%20effective%20approach%20to%20digital%20mass%20persuasion%2C%202017) [Scite](https://engine.scholarcy.com/scite_url?query=Matz%2C%20Sandra%20C.%20Kosinski%2C%20Michal%20Nave%2C%20Gideon%20Stillwell%2C%20David%20J.%20Psychological%20targeting%20as%20an%20effective%20approach%20to%20digital%20mass%20persuasion%2C%202017)

[^Mavroforou_et+al_2010_a]: Mavroforou, A., Michalodimitrakis, E., Hatzitheo-Filou, C., & Giannoukas, A. “Legal and Ethical Issues in Robotic Surgery”, PubMed, February 2010. [[Mavroforou_et+al_LegalEthicalIssuesRoboticSurgery_2010]] [OA](https://engine.scholarcy.com/oa_version?query=Mavroforou%2C%20A.%20Michalodimitrakis%2C%20E.%20Hatzitheo-Filou%2C%20C.%20Giannoukas%2C%20A.%20Legal%20and%20Ethical%20Issues%20in%20Robotic%20Surgery%202010-02) [GScholar](https://scholar.google.co.uk/scholar?q=Mavroforou%2C%20A.%20Michalodimitrakis%2C%20E.%20Hatzitheo-Filou%2C%20C.%20Giannoukas%2C%20A.%20Legal%20and%20Ethical%20Issues%20in%20Robotic%20Surgery%202010-02) [Scite](https://engine.scholarcy.com/scite_url?query=Mavroforou%2C%20A.%20Michalodimitrakis%2C%20E.%20Hatzitheo-Filou%2C%20C.%20Giannoukas%2C%20A.%20Legal%20and%20Ethical%20Issues%20in%20Robotic%20Surgery%202010-02)

[^Maxmen_2018_a]: Maxmen, Amy, “Self-Driving Car Dilemmas Reveal that Moral Choices are Not Universal”, Nature, October 2018. McFarland, Matt, “DARPA’s Robotics Challenge has a gender problem.” Washington Post, June 5, 2015. Retrieved at http://www.washingtonpost.com/blogs/innovations/wp/2015/06/05/darpasrobotics-challenge-has-a-gender-problem/. Meghdari, Ali & Alemi, Minoo, “Recent Advances in Social & Cognitive Robotics and Imminent Ethical Challenges”, Advances in Social Science, Education, and Humanities Research 211, 2018. [[Maxmen_SelfdrivingCarDilemmasRevealThat_2018]] [OA](http://www.washingtonpost.com/blogs/innovations/wp/2015/06/05/darpasrobotics-challenge-has-a-gender-problem/)  [Scite](https://engine.scholarcy.com/scite_url?query=Maxmen%2C%20Amy%20Self-Driving%20Car%20Dilemmas%20Reveal%20that%20Moral%20Choices%20are%20Not%20Universal%202018-10)

[^Mehr_2017_a]: Mehr, Hila, “Artificial Intelligence for Citizen Services and Government”, Harvard Ash Center for Democratic Governance and Innovation, August 2017. [[Mehr_ArtificialIntelligenceCitizenServicesGovernment_2017]] [OA](https://scholar.google.co.uk/scholar?q=Mehr%2C%20Hila%20Artificial%20Intelligence%20for%20Citizen%20Services%20and%20Government%202017-08) [GScholar](https://scholar.google.co.uk/scholar?q=Mehr%2C%20Hila%20Artificial%20Intelligence%20for%20Citizen%20Services%20and%20Government%202017-08) 

[^Meinecke_2016_a]: Meinecke, Lisa & Voss, Laura, “I Robot, You Unemployed: Robotics in Science Fiction and Media Discourse”, Schafft Wissen: Gemeinsames und Geteiltes Wissen in Wissenschaft und Technik, pp.203-215, October 2016. [[Meinecke_RobotYouUnemployedRoboticsScience_2016]] [OA](https://engine.scholarcy.com/oa_version?query=Meinecke%2C%20Lisa%20Voss%2C%20Laura%20I%20Robot%2C%20You%20Unemployed%3A%20Robotics%20in%20Science%20Fiction%20and%20Media%20Discourse%202016-10) [GScholar](https://scholar.google.co.uk/scholar?q=Meinecke%2C%20Lisa%20Voss%2C%20Laura%20I%20Robot%2C%20You%20Unemployed%3A%20Robotics%20in%20Science%20Fiction%20and%20Media%20Discourse%202016-10) [Scite](https://engine.scholarcy.com/scite_url?query=Meinecke%2C%20Lisa%20Voss%2C%20Laura%20I%20Robot%2C%20You%20Unemployed%3A%20Robotics%20in%20Science%20Fiction%20and%20Media%20Discourse%202016-10)

[^Microsoft_2018_a]: Microsoft, “Healthcare, Artificial Intelligence, Data and Ethics – A 2030 Vision How responsible innovation can lead to a healthier society”, December 2018. https://www.digitaleurope.org/wp/wp-content/uploads/2019/02/Healthcare-AI-Data-Ethics2030-vision.pdf [[Microsoft_HealthcareArtificialIntelligenceDataEthics_2018]] [OA](https://www.digitaleurope.org/wp/wp-content/uploads/2019/02/Healthcare-AI-Data-Ethics2030-vision.pdf)  

[^Mieskes_2017_a]: Mieskes, Margot, “A Quantitative Study of Data in the NLP Community,” Proceedings of the First ACL Workshop on Ethics in Natural Language Processing, 2017. [[Mieskes_QuantitativeStudyDataNlpCommunity_2017]] [OA](https://engine.scholarcy.com/oa_version?query=Mieskes%2C%20Margot%20A%20Quantitative%20Study%20of%20Data%20in%20the%20NLP%20Community%2C%202017) [GScholar](https://scholar.google.co.uk/scholar?q=Mieskes%2C%20Margot%20A%20Quantitative%20Study%20of%20Data%20in%20the%20NLP%20Community%2C%202017) [Scite](https://engine.scholarcy.com/scite_url?query=Mieskes%2C%20Margot%20A%20Quantitative%20Study%20of%20Data%20in%20the%20NLP%20Community%2C%202017)

[^Milano_et+al_2019_a]: Milano, Silvia, Mariarosaria Taddeo, and Luciano Floridi, “Recommender Systems and their Ethical Challenges,” 2019. http://dx.doi.org/10.2139/ssrn.3378581 [[Milano_et+al_RecommenderSystemsTheirEthicalChallenges_2019]] [OA](https://doi.org/10.2139/ssrn.3378581)  [Scite](https://scite.ai/reports/10.2139/ssrn.3378581)

[^Miller_et+al_2019_a]: Miller, Christopher A., Harry Funk, Robert Goldman, John Meisner, and Peggy Wu, "Implications of adaptive vs. adaptable UIs on decision making: Why “automated adaptiveness” is not always the right answer," In Proceedings of the 1st international conference on augmented cognition, pp. 2227. 2005., p. 3 Miller, T., ‘Explanation in Artificial Intelligence: Insights From the Social Sciences’, Artificial Intelligence, Vol. 267, pp. 1-38, February 2019. [[Miller_et+al_ImplicationsAdaptiveAdaptableUis_2019]] [OA](https://engine.scholarcy.com/oa_version?query=Miller%2C%20Christopher%20A.%20Funk%2C%20Harry%20Goldman%2C%20Robert%20Meisner%2C%20John%20Implications%20of%20adaptive%20vs.%20adaptable%20UIs%20on%20decision%20making%3A%20Why%20%E2%80%9Cautomated%20adaptiveness%E2%80%9D%20is%20not%20always%20the%20right%20answer%2C%202019-02) [GScholar](https://scholar.google.co.uk/scholar?q=Miller%2C%20Christopher%20A.%20Funk%2C%20Harry%20Goldman%2C%20Robert%20Meisner%2C%20John%20Implications%20of%20adaptive%20vs.%20adaptable%20UIs%20on%20decision%20making%3A%20Why%20%E2%80%9Cautomated%20adaptiveness%E2%80%9D%20is%20not%20always%20the%20right%20answer%2C%202019-02) [Scite](https://engine.scholarcy.com/scite_url?query=Miller%2C%20Christopher%20A.%20Funk%2C%20Harry%20Goldman%2C%20Robert%20Meisner%2C%20John%20Implications%20of%20adaptive%20vs.%20adaptable%20UIs%20on%20decision%20making%3A%20Why%20%E2%80%9Cautomated%20adaptiveness%E2%80%9D%20is%20not%20always%20the%20right%20answer%2C%202019-02)

[^Miller_et+al_2017_a]: Miller, Tim, Piers Howe, and Liz Sonenberg, "Explainable AI: Beware of inmates running the asylum or: How I learnt to stop worrying and love the social and behavioural sciences," arXiv preprint arXiv:1712.00547, 2017. [[Miller_et+al_ExplainableAiBewareInmatesRunning_2017]] [OA](https://export.arxiv.org/pdf/1712.00547)  

[^Mitchell_2018_a]: Mitchell, Anna and Diamond, Larry, “China’s Surveillance State Should Scare Everyone,” The Atlantic, 2 February 2018. https://www.theatlantic.com/international/archive/2018/02/chinasurveillance/552203/ [[Mitchell_ChinaSurveillanceStateShouldScare_2018]] [OA](https://www.theatlantic.com/international/archive/2018/02/chinasurveillance/552203/)  [Scite](https://engine.scholarcy.com/scite_url?query=Mitchell%2C%20Anna%20Diamond%2C%20Larry%20China%E2%80%99s%20Surveillance%20State%20Should%20Scare%20Everyone%2C%202018-02-02)

[^Mitchell_2006_a]: Mitchell, T. M. (2006). The discipline of machine learning (Vol. 9). Pittsburgh, PA: Carnegie Mellon University, School of Computer Science, Machine Learning Department. [[Mitchell_DisciplineMachineLearningvol_2006]] [OA](https://scholar.google.co.uk/scholar?q=Mitchell%2C%20T.M.%20The%20discipline%20of%20machine%20learning%20%28Vol%202006) [GScholar](https://scholar.google.co.uk/scholar?q=Mitchell%2C%20T.M.%20The%20discipline%20of%20machine%20learning%20%28Vol%202006) 

[^Mitchell_1997_a]: Mitchell, Tom M., Machine Learning, McGraw Hill, New York, 1997. [[Mitchell__1997]] [OA](https://engine.scholarcy.com/oa_version?query=Mitchell%20Tom%20M%20Machine%20Learning%20McGraw%20Hill%20New%20York%201997) [GScholar](https://scholar.google.co.uk/scholar?q=Mitchell%20Tom%20M%20Machine%20Learning%20McGraw%20Hill%20New%20York%201997) [Scite](https://engine.scholarcy.com/scite_url?query=Mitchell%20Tom%20M%20Machine%20Learning%20McGraw%20Hill%20New%20York%201997)

[^Mittelstadt_et+al_2016_a]: Mittelstadt, B., Allo, P., Taddeo, M., Wachter, S. and Floridi, L. (2016). The Ethics of Algorithms: Mapping the Debate. Mladenovic, Milos N. & McPherson, Tristram, “Engineering Social Justice into Traffic Control for Self-Driving Vehicles?” Science and Engineering Ethics, August 2016. [[Mittelstadt_et+al_EthicsAlgorithmsMapping_2016]] [OA](https://engine.scholarcy.com/oa_version?query=Mittelstadt%2C%20B.%20Allo%2C%20P.%20Taddeo%2C%20M.%20Wachter%2C%20S.%20The%20Ethics%20of%20Algorithms%3A%20Mapping%20the%202016-08) [GScholar](https://scholar.google.co.uk/scholar?q=Mittelstadt%2C%20B.%20Allo%2C%20P.%20Taddeo%2C%20M.%20Wachter%2C%20S.%20The%20Ethics%20of%20Algorithms%3A%20Mapping%20the%202016-08) [Scite](https://engine.scholarcy.com/scite_url?query=Mittelstadt%2C%20B.%20Allo%2C%20P.%20Taddeo%2C%20M.%20Wachter%2C%20S.%20The%20Ethics%20of%20Algorithms%3A%20Mapping%20the%202016-08)

[^Morgan_2017_a]: Morgan, Blake, “How Artificial Intelligence Will Impact the Insurance Industry,” Forbes, July 25, 2017. [[Morgan_ArtificialIntelligenceWillImpactInsurance_2017]] [OA](https://engine.scholarcy.com/oa_version?query=Morgan%2C%20Blake%20How%20Artificial%20Intelligence%20Will%20Impact%20the%20Insurance%20Industry%2C%202017-07-25) [GScholar](https://scholar.google.co.uk/scholar?q=Morgan%2C%20Blake%20How%20Artificial%20Intelligence%20Will%20Impact%20the%20Insurance%20Industry%2C%202017-07-25) [Scite](https://engine.scholarcy.com/scite_url?query=Morgan%2C%20Blake%20How%20Artificial%20Intelligence%20Will%20Impact%20the%20Insurance%20Industry%2C%202017-07-25)

[^Mortensen_2017_a]: http://forbes.com/sites/blakemorgan/2017/07/25/how-artificial-intelligence-will-impact-theinsurance-industry/#5255ab2e6531 Mortensen, Dennis, “Automation May Take Our Jobs—But It’ll Restore Our Humanity”, Quartz Automation Revolution, August 2017. [[Mortensen_AutomationMayTakeOurJobsbut_2017]] [OA](http://forbes.com/sites/blakemorgan/2017/07/25/how-artificial-intelligence-will-impact-theinsurance-industry/#5255ab2e6531)  [Scite](https://engine.scholarcy.com/scite_url?query=Mortensen%2C%20Dennis%20Automation%20May%20Take%20Our%20Jobs%E2%80%94But%20It%E2%80%99ll%20Restore%20Our%20Humanity%202017-08)

[^Mouthuy_2017_a]: Mouthuy, Pierre-Alexis & Carr, Andrew, “Growing Tissue Grafts on Humanoid Robots: A Future Strategy in Regenerative Medicine?” Science Robotics 2(4), March 2017. [[Mouthuy_GrowingTissueGraftsHumanoidRobots_2017]] [OA](https://engine.scholarcy.com/oa_version?query=Mouthuy%2C%20Pierre-Alexis%20Carr%2C%20Andrew%20Growing%20Tissue%20Grafts%20on%20Humanoid%20Robots%3A%20A%20Future%20Strategy%20in%20Regenerative%20Medicine%3F%202017-03) [GScholar](https://scholar.google.co.uk/scholar?q=Mouthuy%2C%20Pierre-Alexis%20Carr%2C%20Andrew%20Growing%20Tissue%20Grafts%20on%20Humanoid%20Robots%3A%20A%20Future%20Strategy%20in%20Regenerative%20Medicine%3F%202017-03) [Scite](https://engine.scholarcy.com/scite_url?query=Mouthuy%2C%20Pierre-Alexis%20Carr%2C%20Andrew%20Growing%20Tissue%20Grafts%20on%20Humanoid%20Robots%3A%20A%20Future%20Strategy%20in%20Regenerative%20Medicine%3F%202017-03)

[^Moyle_et+al_2017_a]: Moyle, Wendy, Bramble, Marguerite, Jones, Cindy & Murfield, Jenny, “’She Had a Smile on Her Face as Wide as the Great Australian Bite’: A Qualitative Examination of Family Perceptions of a Therapeutic Robot and a Plush Toy”, The Gerontologist 00(00), October 2017. [[Moyle_et+al_sheHadSmileHerFace_2017]] [OA](https://engine.scholarcy.com/oa_version?query=Moyle%2C%20Wendy%20Bramble%2C%20Marguerite%20Jones%2C%20Cindy%20Murfield%2C%20Jenny%20%E2%80%99She%20Had%20a%20Smile%20on%20Her%20Face%20as%20Wide%20as%20the%20Great%20Australian%20Bite%E2%80%99%3A%20A%20Qualitative%20Examination%20of%20Family%20Perceptions%20of%20a%20Therapeutic%20Robot%20and%20a%20Plush%20Toy%202017-10) [GScholar](https://scholar.google.co.uk/scholar?q=Moyle%2C%20Wendy%20Bramble%2C%20Marguerite%20Jones%2C%20Cindy%20Murfield%2C%20Jenny%20%E2%80%99She%20Had%20a%20Smile%20on%20Her%20Face%20as%20Wide%20as%20the%20Great%20Australian%20Bite%E2%80%99%3A%20A%20Qualitative%20Examination%20of%20Family%20Perceptions%20of%20a%20Therapeutic%20Robot%20and%20a%20Plush%20Toy%202017-10) [Scite](https://engine.scholarcy.com/scite_url?query=Moyle%2C%20Wendy%20Bramble%2C%20Marguerite%20Jones%2C%20Cindy%20Murfield%2C%20Jenny%20%E2%80%99She%20Had%20a%20Smile%20on%20Her%20Face%20as%20Wide%20as%20the%20Great%20Australian%20Bite%E2%80%99%3A%20A%20Qualitative%20Examination%20of%20Family%20Perceptions%20of%20a%20Therapeutic%20Robot%20and%20a%20Plush%20Toy%202017-10)

[^Moyle_et+al_2016_a]: Moyle, Wendy, Bramble, Marguerite, Jones, Cindy & Murfield, Jenny, “Care Staff Perceptions of a Social Robot Called Paro and a Look-Alike Plush Toy: A Descriptive Qualitative Approach”, Aging & Mental Health 22(3), November 2016. [[Moyle_et+al_CareStaffPerceptionsSocialRobot_2016]] [OA](https://engine.scholarcy.com/oa_version?query=Moyle%2C%20Wendy%20Bramble%2C%20Marguerite%20Jones%2C%20Cindy%20Murfield%2C%20Jenny%20Care%20Staff%20Perceptions%20of%20a%20Social%20Robot%20Called%20Paro%20and%20a%20Look-Alike%20Plush%20Toy%3A%20A%20Descriptive%20Qualitative%20Approach%202016-11) [GScholar](https://scholar.google.co.uk/scholar?q=Moyle%2C%20Wendy%20Bramble%2C%20Marguerite%20Jones%2C%20Cindy%20Murfield%2C%20Jenny%20Care%20Staff%20Perceptions%20of%20a%20Social%20Robot%20Called%20Paro%20and%20a%20Look-Alike%20Plush%20Toy%3A%20A%20Descriptive%20Qualitative%20Approach%202016-11) [Scite](https://engine.scholarcy.com/scite_url?query=Moyle%2C%20Wendy%20Bramble%2C%20Marguerite%20Jones%2C%20Cindy%20Murfield%2C%20Jenny%20Care%20Staff%20Perceptions%20of%20a%20Social%20Robot%20Called%20Paro%20and%20a%20Look-Alike%20Plush%20Toy%3A%20A%20Descriptive%20Qualitative%20Approach%202016-11)

[^Mueller_2014_a]: Müller, V. C. (ed.), Risks of Artificial Intelligence, Boca Raton, CRC Press, 2016; Bostrom, N., Superintelligence: Paths, Dangers, Strategies. Oxford: Oxford University Press, 2014. [[Mueller__2014]] [OA](https://engine.scholarcy.com/oa_version?query=M%C3%BCller%20V%20C%20ed%20Risks%20of%20Artificial%20Intelligence%20Boca%20Raton%20CRC%20Press%202016%20Bostrom%20N%20Superintelligence%20Paths%20Dangers%20Strategies%20Oxford%20Oxford%20University%20Press%202014) [GScholar](https://scholar.google.co.uk/scholar?q=M%C3%BCller%20V%20C%20ed%20Risks%20of%20Artificial%20Intelligence%20Boca%20Raton%20CRC%20Press%202016%20Bostrom%20N%20Superintelligence%20Paths%20Dangers%20Strategies%20Oxford%20Oxford%20University%20Press%202014) [Scite](https://engine.scholarcy.com/scite_url?query=M%C3%BCller%20V%20C%20ed%20Risks%20of%20Artificial%20Intelligence%20Boca%20Raton%20CRC%20Press%202016%20Bostrom%20N%20Superintelligence%20Paths%20Dangers%20Strategies%20Oxford%20Oxford%20University%20Press%202014)

[^Mueller_2016_a]: Müller, Vincent C., and Nick Bostrom, “Future Progress in Artificial Intelligence: A Survey of Expert Opinion,” Fundamental Issues of Artificial Intelligence, 2016, pp. 555–572. [[Mueller_FutureProgressArtificialIntelligenceSurvey_2016]] [OA](https://engine.scholarcy.com/oa_version?query=M%C3%BCller%2C%20Vincent%20C.%20Bostrom%2C%20Nick%20Future%20Progress%20in%20Artificial%20Intelligence%3A%20A%20Survey%20of%20Expert%20Opinion%2C%202016) [GScholar](https://scholar.google.co.uk/scholar?q=M%C3%BCller%2C%20Vincent%20C.%20Bostrom%2C%20Nick%20Future%20Progress%20in%20Artificial%20Intelligence%3A%20A%20Survey%20of%20Expert%20Opinion%2C%202016) [Scite](https://engine.scholarcy.com/scite_url?query=M%C3%BCller%2C%20Vincent%20C.%20Bostrom%2C%20Nick%20Future%20Progress%20in%20Artificial%20Intelligence%3A%20A%20Survey%20of%20Expert%20Opinion%2C%202016)

[^Mulligan_2017_a]: Mulligan, Christina, “Revenge Against Robots”, Brooklyn Law School, 2017. [[Mulligan_RevengeAgainstRobots_2017]] [OA](https://engine.scholarcy.com/oa_version?query=Mulligan%2C%20Christina%20Revenge%20Against%20Robots%202017) [GScholar](https://scholar.google.co.uk/scholar?q=Mulligan%2C%20Christina%20Revenge%20Against%20Robots%202017) [Scite](https://engine.scholarcy.com/scite_url?query=Mulligan%2C%20Christina%20Revenge%20Against%20Robots%202017)

[^Murashov_et+al_2016_a]: Murashov, Vladamir, Hearl, Frank & Howard, John, “Working Safely With Robot Workers: Recommendations for the New Workplace”, Journal of Occupational and Environmental Hygiene, 13(3), 2016. [[Murashov_et+al_WorkingSafelyWithRobotWorkers_2016]] [OA](https://engine.scholarcy.com/oa_version?query=Murashov%2C%20Vladamir%20Hearl%2C%20Frank%20Howard%2C%20John%20Working%20Safely%20With%20Robot%20Workers%3A%20Recommendations%20for%20the%20New%20Workplace%202016) [GScholar](https://scholar.google.co.uk/scholar?q=Murashov%2C%20Vladamir%20Hearl%2C%20Frank%20Howard%2C%20John%20Working%20Safely%20With%20Robot%20Workers%3A%20Recommendations%20for%20the%20New%20Workplace%202016) [Scite](https://engine.scholarcy.com/scite_url?query=Murashov%2C%20Vladamir%20Hearl%2C%20Frank%20Howard%2C%20John%20Working%20Safely%20With%20Robot%20Workers%3A%20Recommendations%20for%20the%20New%20Workplace%202016)

[^Murison_2018_a]: Murison, Malek, “The Great Firewall: China looks to AI to censor online material,” Internet of Business, May 23, 2018. https://internetofbusiness.com/china-censorship-online-material-ai/ Muro, Mark, Robert Maxim, and Jacob Whiton, "Automation and Artificial Intelligence: How Machines are Affecting People and Places," Brookings Institution, https://www.brookings.edu/research/automation-and-artificial-intelligence-how-machinesaffect-people-and-places/, 2019. [[Murison_GreatFirewallChinaLooksAi_2018]] [OA](https://internetofbusiness.com/china-censorship-online-material-ai/Muro)  [Scite](https://engine.scholarcy.com/scite_url?query=Murison%2C%20Malek%20The%20Great%20Firewall%3A%20China%20looks%20to%20AI%20to%20censor%20online%20material%2C%202018-05-23)

[^Mussolum_2007_a]: Mussolum, Erin, “How Art Shapes Identity”, Trinity Western University, October 2007. [[Mussolum_ArtShapesIdentity_2007]] [OA](https://scholar.google.co.uk/scholar?q=Mussolum%2C%20Erin%20How%20Art%20Shapes%20Identity%202007-10) [GScholar](https://scholar.google.co.uk/scholar?q=Mussolum%2C%20Erin%20How%20Art%20Shapes%20Identity%202007-10) 

[^Myers_2017_a]: Myers, Andrew, “An artificial intelligence algorithm developed by Stanford researchers can determine a neighborhood’s political leanings by its cars,” Stanford News, November 28, 2017. https://news.stanford.edu/2017/11/28/neighborhoods-cars-indicate-political-leanings/ Nan, Khan, and Iqbal, “Real-Time Fault Diagnosis Using Knowledge-Based Expert System”. Naser and Zaqout, “Knowledge-Based Systems That Determine the Appropriate Students Major”. National Science Board, Science & Engineering Indicators 2018. Retrieved at https://nsf.gov/statistics/2018/nsb20181/report. [[Myers_ArtificialIntelligenceAlgorithmDevelopedStanford_2017]] [OA](https://news.stanford.edu/2017/11/28/neighborhoods-cars-indicate-political-leanings/Nan)  [Scite](https://engine.scholarcy.com/scite_url?query=Myers%2C%20Andrew%20An%20artificial%20intelligence%20algorithm%20developed%20by%20Stanford%20researchers%20can%20determine%20a%20neighborhood%E2%80%99s%20political%20leanings%20by%20its%20cars%2C%202017-11-28)

[^Nau_2007_a]: Nau, Dana S, "Current trends in automated planning," AI magazine, Vol. 28, no. 4 (2007): 43-58., p. 43 [[Nau_CurrentTrendsAutomatedPlanning_2007]] [OA](https://engine.scholarcy.com/oa_version?query=Nau%2C%20Dana%20S.%20Current%20trends%20in%20automated%20planning%2C%202007) [GScholar](https://scholar.google.co.uk/scholar?q=Nau%2C%20Dana%20S.%20Current%20trends%20in%20automated%20planning%2C%202007) [Scite](https://engine.scholarcy.com/scite_url?query=Nau%2C%20Dana%20S.%20Current%20trends%20in%20automated%20planning%2C%202007)

[^Nedelkoska_2018_a]: Nedelkoska & Quintini, 2018 Nedelkoska, Ljubica, and Glenda Quintini, "Automation, skills use and training," OECD Social, Employment and Migration Working Papers, No. 202, OECD Publishing, Paris, 2018, http://dx.doi.org/10.1787/2e2f4eea-en. NEM, “Artificial Intelligence in the Media and Creative Industries,” Position paper, July 2018, https://nem-initiative.org/wp-content/uploads/2018/10/nem-positionpaperaiinceativeindustry.pdf [[Nedelkoska_AutomationSkillsTraining_2018]] [OA](https://doi.org/10.1787/2e2f4eea-en)  [Scite](https://scite.ai/reports/10.1787/2e2f4eea-en)

[^Neudert_2019_a]: Neudert, Lisa Maria, and Marchal, Nahema, “Polarisation and the Use of Technology in Political Campaigns and Communication,” Study Panel for the Future of Science and Technology, European Parliamentary Research Service, Brussels, March 2019.,,,http://www.europarl.europa.eu/thinktank/en/document.html?reference=EPRS_STU(2019 )634414 Nezhadali, Vaheed, “Multi-Objective Optimization of Industrial Robots”, Linköpings Universitet, 2011. [[Neudert_polarisationUseTechnologyPoliticalCampaigns_2019]] [OA](http://www.europarl.europa.eu/thinktank/en/document.html?reference=EPRS_STU(2019)  [Scite](https://engine.scholarcy.com/scite_url?query=Neudert%2C%20Lisa%20Maria%20Marchal%2C%20Nahema%20%E2%80%9CPolarisation%20and%20the%20Use%20of%20Technology%20in%20Political%20Campaigns%20and%20Communication%2C%E2%80%9D%20Study%20Panel%20for%20the%20Future%20of%20Science%20and%20Technology%2C%20European%20Parliamentary%20Research%20Service%202019-03)

[^Nissenbaum_2001_a]: Nissenbaum, Helen, “How computer systems embody values,” Computer, Vol. 34, No. 3, 2001, pp. 120–119. [[Nissenbaum_ComputerSystemsEmbodyValues_2001]] [OA](https://engine.scholarcy.com/oa_version?query=Nissenbaum%2C%20Helen%20How%20computer%20systems%20embody%20values%2C%202001) [GScholar](https://scholar.google.co.uk/scholar?q=Nissenbaum%2C%20Helen%20How%20computer%20systems%20embody%20values%2C%202001) [Scite](https://engine.scholarcy.com/scite_url?query=Nissenbaum%2C%20Helen%20How%20computer%20systems%20embody%20values%2C%202001)

[^Nitzberg_et+al_2017_a]: Nitzberg, Mark, Olaf Groth, and Mark Esposito, “AI Isn't Just Compromising Our Privacy-It Can Limit Our Choices, Too,” Quartz, December 13, 2017. https://qz.com/1153647/ai-isnt-just-taking-awayour-privacy-its-destroying-our-free-will-too/. [[Nitzberg_et+al_AiIsnJustCompromisingOur_2017]] [OA](https://qz.com/1153647/ai-isnt-just-taking-awayour-privacy-its-destroying-our-free-will-too/)  [Scite](https://engine.scholarcy.com/scite_url?query=Nitzberg%2C%20Mark%20Groth%2C%20Olaf%20Esposito%2C%20Mark%20AI%20Isn%27t%20Just%20Compromising%20Our%20Privacy-It%20Can%20Limit%20Our%20Choices%2C%20Too%2C%202017-12-13)

[^Norval_2017_a]: Norval, A., and Prasopoulou, E., ‘Public Faces? A Critical Exploration of the Diffusion of Face Recognition Technologies in Online Social Networks’, New Media & Society, Vol. 19, No. 4, pp. 637-654, April 2017. [[Norval_PublicFacesACriticalExploration_2017]] [OA](https://engine.scholarcy.com/oa_version?query=Norval%2C%20A.%20Prasopoulou%2C%20E.%20Public%20Faces%3F%20A%20Critical%20Exploration%20of%20the%20Diffusion%20of%20Face%20Recognition%20Technologies%20in%20Online%20Social%20Networks%202017-04) [GScholar](https://scholar.google.co.uk/scholar?q=Norval%2C%20A.%20Prasopoulou%2C%20E.%20Public%20Faces%3F%20A%20Critical%20Exploration%20of%20the%20Diffusion%20of%20Face%20Recognition%20Technologies%20in%20Online%20Social%20Networks%202017-04) [Scite](https://engine.scholarcy.com/scite_url?query=Norval%2C%20A.%20Prasopoulou%2C%20E.%20Public%20Faces%3F%20A%20Critical%20Exploration%20of%20the%20Diffusion%20of%20Face%20Recognition%20Technologies%20in%20Online%20Social%20Networks%202017-04)

[^Nunez_2017_a]: Nunez, Catherine, “Artificial Intelligence and Legal Ethics: Whether AI Lawyers Can Make Ethical Decisions,” Tulane Journal of Technology and Intellectual Property, Vol. 20, 2017, pp. 189–204 O’Neil, Weapons of Math Destruction, London, Allen Lane, 2016. [[Nunez_ArtificialIntelligenceLegalEthicsWhether_2017]] [OA](https://engine.scholarcy.com/oa_version?query=Nunez%2C%20Catherine%20Artificial%20Intelligence%20and%20Legal%20Ethics%3A%20Whether%20AI%20Lawyers%20Can%20Make%20Ethical%20Decisions%2C%202017) [GScholar](https://scholar.google.co.uk/scholar?q=Nunez%2C%20Catherine%20Artificial%20Intelligence%20and%20Legal%20Ethics%3A%20Whether%20AI%20Lawyers%20Can%20Make%20Ethical%20Decisions%2C%202017) [Scite](https://engine.scholarcy.com/scite_url?query=Nunez%2C%20Catherine%20Artificial%20Intelligence%20and%20Legal%20Ethics%3A%20Whether%20AI%20Lawyers%20Can%20Make%20Ethical%20Decisions%2C%202017)

[^Papernot_et+al_2016_a]: Papernot, N., McDaniel, P., Sinha, A., and Wellman, M., ‘Towards the Science of Security and Privacy in Machine Learning’, p. 4, 2016. https://arxiv.org/abs/1611.03814. [[Papernot_et+al_TowardsScienceSecurityPrivacyMachine_2016]] [OA](https://arxiv.org/abs/1611.03814)  

[^Papernot_et+al_2018_a]: Papernot, Nicolas, Patrick Mcdaniel, Arunesh Sinha, and Michael P. Wellman, “SoK: Security and Privacy in Machine Learning,” 2018 IEEE European Symposium on Security and Privacy (EuroS&P), 2018. [[Papernot_et+al_sok_2018]] [OA](https://engine.scholarcy.com/oa_version?query=Papernot%2C%20Nicolas%20Mcdaniel%2C%20Patrick%20Sinha%2C%20Arunesh%20Wellman%2C%20Michael%20P.%20%E2%80%9CSoK%202018) [GScholar](https://scholar.google.co.uk/scholar?q=Papernot%2C%20Nicolas%20Mcdaniel%2C%20Patrick%20Sinha%2C%20Arunesh%20Wellman%2C%20Michael%20P.%20%E2%80%9CSoK%202018) [Scite](https://engine.scholarcy.com/scite_url?query=Papernot%2C%20Nicolas%20Mcdaniel%2C%20Patrick%20Sinha%2C%20Arunesh%20Wellman%2C%20Michael%20P.%20%E2%80%9CSoK%202018)

[^Pariser_2011_a]: Pariser, Eli, The Filter Bubble: What the Internet Is Hiding from You, Penguin Books, London, 2011. [[Pariser_FilterBubbleWhatInternetIs_2011]] [OA](https://scholar.google.co.uk/scholar?q=Pariser%2C%20Eli%20The%20Filter%20Bubble%3A%20What%20the%20Internet%20Is%20Hiding%20from%20You%202011) [GScholar](https://scholar.google.co.uk/scholar?q=Pariser%2C%20Eli%20The%20Filter%20Bubble%3A%20What%20the%20Internet%20Is%20Hiding%20from%20You%202011) 

[^Parsons_et+al_2008_a]: Parsons, E. C. M., Sarah J. Dolman, Andrew J. Wright, Naomi A. Rose, and W. C. G. Burns. "Navy sonar and cetaceans: Just how much does the gun need to smoke before we act?." Marine pollution bulletin 56, no. 7 (2008): 1248-1257. Passchier-Vermeer, Willy, and Wim F. Passchier. "Noise exposure and public health." Environmental health perspectives 108, no. suppl 1 (2000): 123-131. Patrick Lin, Keith Abney and George Bekey, “Robot Ethics: Mapping the Issues for a Mechanized World”, Artificial Intelligence, vol. 175, 2011, p. 947. [[Parsons_et+al_NavySonarCetaceansJustMuch_2008]] [OA](https://engine.scholarcy.com/oa_version?query=Parsons%2C%20E.C.M.%20Dolman%2C%20Sarah%20J.%20Wright%2C%20Andrew%20J.%20Rose%2C%20Naomi%20A.%20Navy%20sonar%20and%20cetaceans%3A%20Just%20how%20much%20does%20the%20gun%20need%20to%20smoke%20before%20we%20act%3F.%202008) [GScholar](https://scholar.google.co.uk/scholar?q=Parsons%2C%20E.C.M.%20Dolman%2C%20Sarah%20J.%20Wright%2C%20Andrew%20J.%20Rose%2C%20Naomi%20A.%20Navy%20sonar%20and%20cetaceans%3A%20Just%20how%20much%20does%20the%20gun%20need%20to%20smoke%20before%20we%20act%3F.%202008) [Scite](https://engine.scholarcy.com/scite_url?query=Parsons%2C%20E.C.M.%20Dolman%2C%20Sarah%20J.%20Wright%2C%20Andrew%20J.%20Rose%2C%20Naomi%20A.%20Navy%20sonar%20and%20cetaceans%3A%20Just%20how%20much%20does%20the%20gun%20need%20to%20smoke%20before%20we%20act%3F.%202008)

[^Patterson_2016_a]: Patterson, Dan, “How AI-Powered Robots Will Protect the Networked Soldier,” TechRepublic, April 6, 2016. https://www.techrepublic.com/article/how-ai-powered-robots-will-protect-the-networkedsoldier. Pearson, Yvette & Borenstein, Jason, “Creating ‘Companions’ for Children: The Ethics of Designing Esthetic Features for Robots”, AI & Society, February 2014. Peertechz Journal, Engineering Group, “Aims and Scope”, Annals of Robotics and Automation, accessed December 2018. PERVADE: Pervasive Data Ethics, ““The study has been approved by the IRB”: Gayface AI, research hype and the pervasive data ethics gap,” Medium (PERVADE: Pervasive Data Ethics), November 30, 2018.https://medium.com/@pervade_team/the-study-has-been-approved-by-the-irbgayface-ai-research-hype-and-the-pervasive-data-ethics-3b36c5a53eec Petropoulos, Georgios, “The Impact of Artificial Intelligence on Employment”, in Neufeind, Max, O’Reilly, Jacqueline, Ranft, Florian, Work in the digital age, Rowman and Littlefield, London, 2018, pp.119-132. [[Patterson_AipoweredRobotsWillProtectNetworked_2016]] [OA](https://www.techrepublic.com/article/how-ai-powered-robots-will-protect-the-networkedsoldier)  [Scite](https://engine.scholarcy.com/scite_url?query=Patterson%2C%20Dan%20How%20AI-Powered%20Robots%20Will%20Protect%20the%20Networked%20Soldier%2C%202016-02-06)

[^Pfeifle_2018_a]: Pfeifle, Anne, "Alexa, What Should We Do about Privacy: Protecting Privacy for Users of VoiceActivated Devices," Wash. L. Rev, Vol 93, 2018, pp. 421-458. Pinar Saygin, Ayse, Chaminade, Thierry & Ishiguro, Hiroshi et al., “The Thing That Should Not Be: Predictive Coding and the Uncanny Valley in Perceiving Human and Humanoid Robot Actions”, Social Cognitive and Affective Neuroscience 7(4), April 2012. [[Pfeifle_AlexaWhatShouldWeDo_2018]] [OA](https://engine.scholarcy.com/oa_version?query=Pfeifle%2C%20Anne%20Alexa%2C%20What%20Should%20We%20Do%20about%20Privacy%3A%20Protecting%20Privacy%20for%20Users%20of%20VoiceActivated%20Devices%2C%202018-04) [GScholar](https://scholar.google.co.uk/scholar?q=Pfeifle%2C%20Anne%20Alexa%2C%20What%20Should%20We%20Do%20about%20Privacy%3A%20Protecting%20Privacy%20for%20Users%20of%20VoiceActivated%20Devices%2C%202018-04) [Scite](https://engine.scholarcy.com/scite_url?query=Pfeifle%2C%20Anne%20Alexa%2C%20What%20Should%20We%20Do%20about%20Privacy%3A%20Protecting%20Privacy%20for%20Users%20of%20VoiceActivated%20Devices%2C%202018-04)

[^Pisch_2017_a]: Pisch, Anita, “The Ethics of Human Robots: Sam Jinks Brings an Artist’s Perspective to the Discourse”, The Conversation, October 2017. [[Pisch_EthicsHumanRobotsJinksBrings_2017]] [OA](https://engine.scholarcy.com/oa_version?query=Pisch%2C%20Anita%20The%20Ethics%20of%20Human%20Robots%3A%20Sam%20Jinks%20Brings%20an%20Artist%E2%80%99s%20Perspective%20to%20the%20Discourse%202017-10) [GScholar](https://scholar.google.co.uk/scholar?q=Pisch%2C%20Anita%20The%20Ethics%20of%20Human%20Robots%3A%20Sam%20Jinks%20Brings%20an%20Artist%E2%80%99s%20Perspective%20to%20the%20Discourse%202017-10) [Scite](https://engine.scholarcy.com/scite_url?query=Pisch%2C%20Anita%20The%20Ethics%20of%20Human%20Robots%3A%20Sam%20Jinks%20Brings%20an%20Artist%E2%80%99s%20Perspective%20to%20the%20Discourse%202017-10)

[^Polgar_2017_a]: Polgar, David Ryan, “Is it Unethical to Design Robots to Resemble Humans?”, Quartz, June 2017. [[Polgar_UnethicalDesignRobotsResembleHumans_2017]] [OA](https://engine.scholarcy.com/oa_version?query=Polgar%2C%20David%20Ryan%20Is%20it%20Unethical%20to%20Design%20Robots%20to%20Resemble%20Humans%3F%202017-06) [GScholar](https://scholar.google.co.uk/scholar?q=Polgar%2C%20David%20Ryan%20Is%20it%20Unethical%20to%20Design%20Robots%20to%20Resemble%20Humans%3F%202017-06) [Scite](https://engine.scholarcy.com/scite_url?query=Polgar%2C%20David%20Ryan%20Is%20it%20Unethical%20to%20Design%20Robots%20to%20Resemble%20Humans%3F%202017-06)

[^Popenici_2017_a]: Popenici, Stefan A., and Sharon Kerr, "Exploring the impact of artificial intelligence on teaching and learning in higher education," Research and Practice in Technology Enhanced Learning, Vol. 12, No. 1, 2017, p. 22. DOI 10.1186/s41039-017-0062-8.; Johnson, Jeffrey Alan, "The ethics of big data in higher education," International Review of Information Ethics, Vol. 21, No. 21, 2014, pp. 3-10. http://www.i-r-i-e.net/inhalt/021/IRIE-021-Johnson.pdf [[Popenici_ExploringImpactArtificialIntelligenceTeaching_2017]] [OA](https://doi.org/10.1186/s41039-017-0062-8)  [Scite](https://scite.ai/reports/10.1186/s41039-017-0062-8)

[^Powles_2017_a]: Powles and Hodson, REMOVE, 2017; Nuffield Council on Bioethics, “Artificial Intelligence (AI) in Healthcare and Research”, May 2018, p. 2. [[Powles_NuffieldCouncilBioethicsartificialIntelligence_2017]] [OA](https://scholar.google.co.uk/scholar?q=Powles%20Hodson%2C%20R.E.M.O.V.E.%20Nuffield%20Council%20on%20Bioethics%2C%20%E2%80%9CArtificial%20Intelligence%20%28AI%29%20in%20Healthcare%20and%20Research%E2%80%9D%202017-05) [GScholar](https://scholar.google.co.uk/scholar?q=Powles%20Hodson%2C%20R.E.M.O.V.E.%20Nuffield%20Council%20on%20Bioethics%2C%20%E2%80%9CArtificial%20Intelligence%20%28AI%29%20in%20Healthcare%20and%20Research%E2%80%9D%202017-05) 

[^Powles_2017_b]: Powles, Julia, and Hodson, Hal, “Google DeepMind and Healthcare in an Age of Algorithms,” Health and Technology, Vol. 7, 2017; Forbes Insights, “Rethinking Medical Ethics,” February 2019. https://www.forbes.com/sites/insights-intelai/2019/02/11/rethinking-medical-ethics/ [[Powles_GoogleDeepmindHealthcareAgeAlgorithms_2017]] [OA](https://www.forbes.com/sites/insights-intelai/2019/02/11/rethinking-medical-ethics/)  [Scite](https://engine.scholarcy.com/scite_url?query=Powles%2C%20Julia%20Hodson%2C%20Hal%20Google%20DeepMind%20and%20Healthcare%20in%20an%20Age%20of%20Algorithms%2C%202017-02)

[^Prabhakar_et+al_2003_a]: Prabhakar, S., Pankanti, S., and Jain, A. K., ‘Biometric Recognition: Security and Privacy Concerns’, IEEE Security & Privacy, Vol. 1, No. 2, pp. 33-42, March-April 2003. [[Prabhakar_et+al_BiometricRecognitionSecurityPrivacyConcerns_2003]] [OA](https://engine.scholarcy.com/oa_version?query=Prabhakar%2C%20S.%20Pankanti%2C%20S.%20Jain%2C%20A.K.%20Biometric%20Recognition%3A%20Security%20and%20Privacy%20Concerns%202003) [GScholar](https://scholar.google.co.uk/scholar?q=Prabhakar%2C%20S.%20Pankanti%2C%20S.%20Jain%2C%20A.K.%20Biometric%20Recognition%3A%20Security%20and%20Privacy%20Concerns%202003) [Scite](https://engine.scholarcy.com/scite_url?query=Prabhakar%2C%20S.%20Pankanti%2C%20S.%20Jain%2C%20A.K.%20Biometric%20Recognition%3A%20Security%20and%20Privacy%20Concerns%202003)

[^Prescott_et+al_2018_a]: Prescott, Tony, Lepora, Nathan & Verschure, Paul (eds.), Living Machines: A Handbook of Research in Biomimetic and Biohybrid Systems, Oxford University Press, 2018. [[Prescott_et+al__2018]] [OA](https://engine.scholarcy.com/oa_version?query=Prescott%20Tony%20Lepora%20Nathan%20%20Verschure%20Paul%20eds%20Living%20Machines%20A%20Handbook%20of%20Research%20in%20Biomimetic%20and%20Biohybrid%20Systems%20Oxford%20University%20Press%202018) [GScholar](https://scholar.google.co.uk/scholar?q=Prescott%20Tony%20Lepora%20Nathan%20%20Verschure%20Paul%20eds%20Living%20Machines%20A%20Handbook%20of%20Research%20in%20Biomimetic%20and%20Biohybrid%20Systems%20Oxford%20University%20Press%202018) [Scite](https://engine.scholarcy.com/scite_url?query=Prescott%20Tony%20Lepora%20Nathan%20%20Verschure%20Paul%20eds%20Living%20Machines%20A%20Handbook%20of%20Research%20in%20Biomimetic%20and%20Biohybrid%20Systems%20Oxford%20University%20Press%202018)

[^International_n.d._a]: Privacy International, “Artificial Intelligence”, Privacy International, n.d. https://privacyinternational.org/topics/artificial-intelligence [[International_ArtificialIntelligence_n.d.]] [OA](https://privacyinternational.org/topics/artificial-intelligence)  [Scite](https://engine.scholarcy.com/scite_url?query=International%2C%20Privacy%20Artificial%20Intelligence%20n.d.)

[^Pryzant_et+al_2018_a]: Pryzant, Reid, Kelly Shen, Dan Jurafsky, and Stefan Wagner, “Deconfounded Lexicon Induction for Interpretable Social Science,” Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), 2018., p. 1. [[Pryzant_et+al_DeconfoundedLexiconInductionInterpretableSocial_2018]] [OA](https://engine.scholarcy.com/oa_version?query=Pryzant%2C%20Reid%20Shen%2C%20Kelly%20Jurafsky%2C%20Dan%20Wagner%2C%20Stefan%20Deconfounded%20Lexicon%20Induction%20for%20Interpretable%20Social%20Science%2C%202018) [GScholar](https://scholar.google.co.uk/scholar?q=Pryzant%2C%20Reid%20Shen%2C%20Kelly%20Jurafsky%2C%20Dan%20Wagner%2C%20Stefan%20Deconfounded%20Lexicon%20Induction%20for%20Interpretable%20Social%20Science%2C%202018) [Scite](https://engine.scholarcy.com/scite_url?query=Pryzant%2C%20Reid%20Shen%2C%20Kelly%20Jurafsky%2C%20Dan%20Wagner%2C%20Stefan%20Deconfounded%20Lexicon%20Induction%20for%20Interpretable%20Social%20Science%2C%202018)

[^Radford_et+al_2019_a]: Radford, A., Wu, J., Amodei, D., Amodei, D., Clark, J., Brundage, M., and Sutskever, I., ‘Better Language Models and Their Implications’, OpenAI, February 14, 2019. https://openai.com/blog/better-language-models/. [[Radford_et+al_BetterLanguageModelsTheirImplications_2019]] [OA](https://openai.com/blog/better-language-models/)  [Scite](https://engine.scholarcy.com/scite_url?query=Radford%2C%20A.%20Wu%2C%20J.%20Amodei%2C%20D.%20Amodei%2C%20D.%20Better%20Language%20Models%20and%20Their%20Implications%202019-02-14)

[^Raisinghani_et+al_2004_a]: Raisinghani, Mahesh S., Ally Benoit, Jianchun Ding, Maria Gomez, Kanak Gupta, Victor Gusila, Daniel Power and Oliver Schmedding, "Ambient intelligence: Changing forms of human-computer interaction and their social implications," Journal of digital information, Vol. 5, No. 4, 2004. [[Raisinghani_et+al_DanielPowerOliverSchmeddingambient_2004]] [OA](https://engine.scholarcy.com/oa_version?query=Raisinghani%2C%20Mahesh%20S.%20Benoit%2C%20Ally%20Ding%2C%20Jianchun%20Gomez%2C%20Maria%20Daniel%20Power%20and%20Oliver%20Schmedding%2C%20%22Ambient%20intelligence%3A%20Changing%20forms%20of%20human-computer%20interaction%20and%20their%20social%20implications%2C%22%202004) [GScholar](https://scholar.google.co.uk/scholar?q=Raisinghani%2C%20Mahesh%20S.%20Benoit%2C%20Ally%20Ding%2C%20Jianchun%20Gomez%2C%20Maria%20Daniel%20Power%20and%20Oliver%20Schmedding%2C%20%22Ambient%20intelligence%3A%20Changing%20forms%20of%20human-computer%20interaction%20and%20their%20social%20implications%2C%22%202004) [Scite](https://engine.scholarcy.com/scite_url?query=Raisinghani%2C%20Mahesh%20S.%20Benoit%2C%20Ally%20Ding%2C%20Jianchun%20Gomez%2C%20Maria%20Daniel%20Power%20and%20Oliver%20Schmedding%2C%20%22Ambient%20intelligence%3A%20Changing%20forms%20of%20human-computer%20interaction%20and%20their%20social%20implications%2C%22%202004)

[^Rajkomar_2018_a]: Rajkomar, Alvin, “Scalable and accurate deep learning with electronic health records,” NJP Digital Magazine, 2018. https://www.nature.com/articles/s41746-018-0029-1 [[Rajkomar_ScalableAccurateDeepLearningWith_2018]] [OA](https://www.nature.com/articles/s41746-018-0029-1)  [Scite](https://engine.scholarcy.com/scite_url?query=Rajkomar%2C%20Alvin%20Scalable%20and%20accurate%20deep%20learning%20with%20electronic%20health%20records%2C%202018)

[^Rajpurkar_et+al_2017_a]: Rajpurkar, Pranav, Awni Hannun, Masoumeh Haghpanahi, Codie Bourn, and Andrew Ng, “Cardiologist-Level Arrhythmia Detection with Convolutional Neural Networks,” Cornell University arXiv, 2017. https://arxiv.org/pdf/1707.01836.pdf [[Rajpurkar_et+al_CardiologistlevelArrhythmiaDetectionWithConvolutional_2017]] [OA](https://arxiv.org/pdf/1707.01836.pdf)  

[^Fiorini_et+al_2017_a]: Rama Fiorini, Sandro, Bermejo-Alonso, et al., “A Suite of Ontologies for Robotics and Automation”, IEEE Robotics & Automation Magazine, March 2017. [[Fiorini_et+al_SuiteOntologiesRoboticsAutomation_2017]] [OA](https://engine.scholarcy.com/oa_version?query=Fiorini%2C%20Rama%20Sandro%2C%20Bermejo-Alonso%20A%20Suite%20of%20Ontologies%20for%20Robotics%20and%20Automation%202017-03) [GScholar](https://scholar.google.co.uk/scholar?q=Fiorini%2C%20Rama%20Sandro%2C%20Bermejo-Alonso%20A%20Suite%20of%20Ontologies%20for%20Robotics%20and%20Automation%202017-03) [Scite](https://engine.scholarcy.com/scite_url?query=Fiorini%2C%20Rama%20Sandro%2C%20Bermejo-Alonso%20A%20Suite%20of%20Ontologies%20for%20Robotics%20and%20Automation%202017-03)

[^Raman_2017_a]: Raman, Ritu & Bashir, Rashid, “Biomimicry, Biofabrication, and Biohybrid Systems: The Emergence and Evolution of Biological Design”, Advanced Healthcare Materials, 2017. [[Raman_BiomimicryBiofabricationBiohybridSystemsEmergence_2017]] [OA](https://engine.scholarcy.com/oa_version?query=Raman%2C%20Ritu%20Bashir%2C%20Rashid%20Biomimicry%2C%20Biofabrication%2C%20and%20Biohybrid%20Systems%3A%20The%20Emergence%20and%20Evolution%20of%20Biological%20Design%202017) [GScholar](https://scholar.google.co.uk/scholar?q=Raman%2C%20Ritu%20Bashir%2C%20Rashid%20Biomimicry%2C%20Biofabrication%2C%20and%20Biohybrid%20Systems%3A%20The%20Emergence%20and%20Evolution%20of%20Biological%20Design%202017) [Scite](https://engine.scholarcy.com/scite_url?query=Raman%2C%20Ritu%20Bashir%2C%20Rashid%20Biomimicry%2C%20Biofabrication%2C%20and%20Biohybrid%20Systems%3A%20The%20Emergence%20and%20Evolution%20of%20Biological%20Design%202017)

[^Rawls_1971_a]: Rawls, J. (1971), A Theory of Justice, Cambridge, MA, Harvard University Press. Reiter, Ehud, and Robert Dale, Building natural language generation systems, Cambridge university press, 2000. [[Rawls_TheoryJustice_1971]] [OA](https://engine.scholarcy.com/oa_version?query=Rawls%2C%20J.%20A%20Theory%20of%20Justice%201971) [GScholar](https://scholar.google.co.uk/scholar?q=Rawls%2C%20J.%20A%20Theory%20of%20Justice%201971) [Scite](https://engine.scholarcy.com/scite_url?query=Rawls%2C%20J.%20A%20Theory%20of%20Justice%201971)

[^Dynamical_2001_a]: Dynamical Systems, Cambridge, Massachusetts: The MIT Press, 2001. [[Dynamical__2001]] [OA](https://scholar.google.co.uk/scholar?q=Dynamical%20Systems%20Cambridge%20Massachusetts%20The%20MIT%20Press%202001) [GScholar](https://scholar.google.co.uk/scholar?q=Dynamical%20Systems%20Cambridge%20Massachusetts%20The%20MIT%20Press%202001) 

[^Reuters_2012_a]: Reuters, “The Rise of Lousy and Lovely Jobs”, 13 April 2012. [[Reuters_RiseLousyLovelyJobs_2012]] [OA](https://scholar.google.co.uk/scholar?q=Reuters%20The%20Rise%20of%20Lousy%20and%20Lovely%20Jobs%202012-04-13) [GScholar](https://scholar.google.co.uk/scholar?q=Reuters%20The%20Rise%20of%20Lousy%20and%20Lovely%20Jobs%202012-04-13) 

[^Reynolds_0000_a]: https://www.reuters.com/article/idUS380409786120120412 Reynolds, Carson, and Picard, Rosalind, “Affective Sensors, Privacy, and Ethical Contracts,” [[Reynolds_AffectiveSensorsPrivacyEthicalContracts_0000]] [OA](https://www.reuters.com/article/idUS380409786120120412)  

[^Proceedings_2004_b]: Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, CHI 2004 Extended Abstracts on Human Factors in Computing Systems, pp. 1103–1106, Vienna, Austria, ACM, 2004. [[Proceedings__2004]] [OA](https://engine.scholarcy.com/oa_version?query=Proceedings%20of%20the%20SIGCHI%20Conference%20on%20Human%20Factors%20in%20Computing%20Systems%20CHI%202004%20Extended%20Abstracts%20on%20Human%20Factors%20in%20Computing%20Systems%20pp%2011031106%20Vienna%20Austria%20ACM%202004) [GScholar](https://scholar.google.co.uk/scholar?q=Proceedings%20of%20the%20SIGCHI%20Conference%20on%20Human%20Factors%20in%20Computing%20Systems%20CHI%202004%20Extended%20Abstracts%20on%20Human%20Factors%20in%20Computing%20Systems%20pp%2011031106%20Vienna%20Austria%20ACM%202004) [Scite](https://engine.scholarcy.com/scite_url?query=Proceedings%20of%20the%20SIGCHI%20Conference%20on%20Human%20Factors%20in%20Computing%20Systems%20CHI%202004%20Extended%20Abstracts%20on%20Human%20Factors%20in%20Computing%20Systems%20pp%2011031106%20Vienna%20Austria%20ACM%202004)

[^Ribeiro_et+al_2000_a]: Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin, "Semantically equivalent adversarial rules for debugging nlp models," Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 856-865, 2018., p. 856 Rickenberg, Raoul, and Byron Reeves, “The effects of animated characters on anxiety, task performance, and evaluations of user interfaces,” CHI ’00: Proceedings of the SIGCHI conference on Human factors in computing systems, New York, NY, USA: ACM Press, 2000, pp. 49–56. [[Ribeiro_et+al_SemanticallyEquivalentAdversarialRulesDebugging_2000]] [OA](https://engine.scholarcy.com/oa_version?query=Ribeiro%2C%20Marco%20Tulio%20Singh%2C%20Sameer%20Guestrin%2C%20Carlos%20Semantically%20equivalent%20adversarial%20rules%20for%20debugging%20nlp%20models%2C%202000) [GScholar](https://scholar.google.co.uk/scholar?q=Ribeiro%2C%20Marco%20Tulio%20Singh%2C%20Sameer%20Guestrin%2C%20Carlos%20Semantically%20equivalent%20adversarial%20rules%20for%20debugging%20nlp%20models%2C%202000) [Scite](https://engine.scholarcy.com/scite_url?query=Ribeiro%2C%20Marco%20Tulio%20Singh%2C%20Sameer%20Guestrin%2C%20Carlos%20Semantically%20equivalent%20adversarial%20rules%20for%20debugging%20nlp%20models%2C%202000)

[^Rid_0000_a]: Rid, Thomas, “Cyberwar – does it exist?”, Nato Review Magazine. https://www.nato.int/docu/review/2013/Cyber/Cyberwar-does-it-exist/EN/index.htm [[Rid_CyberwarDoesExist_0000]] [OA](https://www.nato.int/docu/review/2013/Cyber/Cyberwar-does-it-exist/EN/index.htm)  [Scite](https://engine.scholarcy.com/scite_url?query=Rid%2C%20Thomas%20Cyberwar%20%E2%80%93%20does%20it%20exist%3F)

[^Riek_2014_a]: Riek, Laurel & Howard, Don, “A Code of Ethics for the Human-Robot Interaction Profession”, We Robot, 2014. [[Riek_CodeEthicsHumanrobotInteractionProfession_2014]] [OA](https://scholar.google.co.uk/scholar?q=Riek%2C%20Laurel%20Howard%2C%20Don%20A%20Code%20of%20Ethics%20for%20the%20Human-Robot%20Interaction%20Profession%202014) [GScholar](https://scholar.google.co.uk/scholar?q=Riek%2C%20Laurel%20Howard%2C%20Don%20A%20Code%20of%20Ethics%20for%20the%20Human-Robot%20Interaction%20Profession%202014) 

[^Riek_et+al_2009_a]: Riek, Laurel D., Rabinowitch, Tal-Chen, Chakrabarti, Bhismadev, & Robinson, Peter, “How Anthropomoprhism Affects Empathy Towards Robots”, Cambridge, 2009. [[Riek_et+al_AnthropomoprhismAffectsEmpathyTowardsRobots_2009]] [OA](https://engine.scholarcy.com/oa_version?query=Riek%2C%20Laurel%20D.%20Rabinowitch%2C%20Tal-Chen%20Chakrabarti%2C%20Bhismadev%20Robinson%2C%20Peter%20How%20Anthropomoprhism%20Affects%20Empathy%20Towards%20Robots%202009) [GScholar](https://scholar.google.co.uk/scholar?q=Riek%2C%20Laurel%20D.%20Rabinowitch%2C%20Tal-Chen%20Chakrabarti%2C%20Bhismadev%20Robinson%2C%20Peter%20How%20Anthropomoprhism%20Affects%20Empathy%20Towards%20Robots%202009) [Scite](https://engine.scholarcy.com/scite_url?query=Riek%2C%20Laurel%20D.%20Rabinowitch%2C%20Tal-Chen%20Chakrabarti%2C%20Bhismadev%20Robinson%2C%20Peter%20How%20Anthropomoprhism%20Affects%20Empathy%20Towards%20Robots%202009)

[^Riek_2014_b]: Riek, Laurel, and Don Howard. "A code of ethics for the human-robot interaction profession." Proceedings of We Robot (2014). [[Riek_CodeEthicsHumanrobotInteractionProfession_2014]] [OA](https://engine.scholarcy.com/oa_version?query=Riek%2C%20Laurel%20Howard%2C%20Don%20A%20code%20of%20ethics%20for%20the%20human-robot%20interaction%20profession.%202014) [GScholar](https://scholar.google.co.uk/scholar?q=Riek%2C%20Laurel%20Howard%2C%20Don%20A%20code%20of%20ethics%20for%20the%20human-robot%20interaction%20profession.%202014) [Scite](https://engine.scholarcy.com/scite_url?query=Riek%2C%20Laurel%20Howard%2C%20Don%20A%20code%20of%20ethics%20for%20the%20human-robot%20interaction%20profession.%202014)

[^Rigby_2019_a]: Rigby, Michael J., “Ethical Dimensions of Using Artificial Intelligence in Health Care,” AMA Journal of Ethics, Vol. 21, No. 2, 2019, p. 122. [[Rigby_EthicalDimensionsUsingArtificialIntelligence_2019]] [OA](https://engine.scholarcy.com/oa_version?query=Rigby%2C%20Michael%20J.%20Ethical%20Dimensions%20of%20Using%20Artificial%20Intelligence%20in%20Health%20Care%2C%202019) [GScholar](https://scholar.google.co.uk/scholar?q=Rigby%2C%20Michael%20J.%20Ethical%20Dimensions%20of%20Using%20Artificial%20Intelligence%20in%20Health%20Care%2C%202019) [Scite](https://engine.scholarcy.com/scite_url?query=Rigby%2C%20Michael%20J.%20Ethical%20Dimensions%20of%20Using%20Artificial%20Intelligence%20in%20Health%20Care%2C%202019)

[^Righetti_et+al_2018_a]: Righetti, L., Q.-C. Pham, R. Madhavan, and R. Chatila, “Lethal Autonomous Weapon Systems”, IEEE Robotics & Automation Magazine, March 2018, p. 124. [[Righetti_et+al_LethalAutonomousWeaponSystems_2018]] [OA](https://engine.scholarcy.com/oa_version?query=Righetti%2C%20L.%20Pham%2C%20Q.-C.%20Madhavan%2C%20R.%20Chatila%2C%20R.%20Lethal%20Autonomous%20Weapon%20Systems%202018-03) [GScholar](https://scholar.google.co.uk/scholar?q=Righetti%2C%20L.%20Pham%2C%20Q.-C.%20Madhavan%2C%20R.%20Chatila%2C%20R.%20Lethal%20Autonomous%20Weapon%20Systems%202018-03) [Scite](https://engine.scholarcy.com/scite_url?query=Righetti%2C%20L.%20Pham%2C%20Q.-C.%20Madhavan%2C%20R.%20Chatila%2C%20R.%20Lethal%20Autonomous%20Weapon%20Systems%202018-03)

[^Kitchin_2014_b]: Robert Kitchin, (2014). ‘Big Data, new epistemologies and paradigm shifts’ in Big Data & Society, 1(1): 1-12. [[Kitchin_bigDataEpistemologiesParadigmShifts_2014]] [OA](https://engine.scholarcy.com/oa_version?query=Kitchin%2C%20Robert%20%E2%80%98Big%20Data%2C%20new%20epistemologies%20and%20paradigm%20shifts%E2%80%99%20in%20Big%202014) [GScholar](https://scholar.google.co.uk/scholar?q=Kitchin%2C%20Robert%20%E2%80%98Big%20Data%2C%20new%20epistemologies%20and%20paradigm%20shifts%E2%80%99%20in%20Big%202014) [Scite](https://engine.scholarcy.com/scite_url?query=Kitchin%2C%20Robert%20%E2%80%98Big%20Data%2C%20new%20epistemologies%20and%20paradigm%20shifts%E2%80%99%20in%20Big%202014)

[^Robertson_2017_a]: Robertson, Jennifer, Robo Sapiens Japanicus: Robots, Gender, Family, and The Japanese Nation, University of California Press, 2017. [[Robertson_RoboSapiensJapanicusRobotsGender_2017]] [OA](https://scholar.google.co.uk/scholar?q=Robertson%2C%20Jennifer%20Robo%20Sapiens%20Japanicus%3A%20Robots%2C%20Gender%2C%20Family%2C%20and%20The%20Japanese%20Nation%202017) [GScholar](https://scholar.google.co.uk/scholar?q=Robertson%2C%20Jennifer%20Robo%20Sapiens%20Japanicus%3A%20Robots%2C%20Gender%2C%20Family%2C%20and%20The%20Japanese%20Nation%202017) 

[^Robitzski_2019_a]: Robitzski, Dan, “New AI Generates Horrifyingly Plausible Fake News,” Futurism, May 30, 2019. https://futurism.com/ai-generates-fake-news. [[Robitzski_AiGeneratesHorrifyinglyPlausibleFake_2019]] [OA](https://futurism.com/ai-generates-fake-news)  [Scite](https://engine.scholarcy.com/scite_url?query=Robitzski%2C%20Dan%20New%20AI%20Generates%20Horrifyingly%20Plausible%20Fake%20News%2C%202019-05-30)

[^Robotworx_2018_a]: RobotWorx, “How Can Industrial Robots Improve My Profits?” Accessed December 2018. robots.com/faq/how-can-industrial-robots-improve-my-profits [[Robotworx_CanIndustrialRobotsImproveMy_2018]] [OA](https://scholar.google.co.uk/scholar?q=RobotWorx%20How%20Can%20Industrial%20Robots%20Improve%20My%20Profits%3F%202018) [GScholar](https://scholar.google.co.uk/scholar?q=RobotWorx%20How%20Can%20Industrial%20Robots%20Improve%20My%20Profits%3F%202018) 

[^Rodrigues_2019_a]: Rodrigues, Rowena and Anais Resseguier, “The underdog in the AI ethical and legal debate: human autonomy”, Ethics Dialogues, 12 June 2019. https://www.ethicsdialogues.eu/2019/06/12/theunderdog-in-the-ai-ethical-and-legal-debate-human-autonomy/ [[Rodrigues_UnderdogAiEthicalLegalDebate_2019]] [OA](https://www.ethicsdialogues.eu/2019/06/12/theunderdog-in-the-ai-ethical-and-legal-debate-human-autonomy/)  [Scite](https://engine.scholarcy.com/scite_url?query=Rodrigues%2C%20Rowena%20Resseguier%2C%20Anais%20The%20underdog%20in%20the%20AI%20ethical%20and%20legal%20debate%3A%20human%20autonomy%202019-06-12)

[^Rodrigues_2019_b]: Rodrigues, Rowena and Jansen, Philip, “Brief report of the SIENNA foresight workshop on the social and ethical issues of AI and robotics”, SIENNA, January 2019. [[Rodrigues_BriefReportSiennaForesightWorkshop_2019]] [OA](https://engine.scholarcy.com/oa_version?query=Rodrigues%2C%20Rowena%20Jansen%2C%20Philip%20Brief%20report%20of%20the%20SIENNA%20foresight%20workshop%20on%20the%20social%20and%20ethical%20issues%20of%20AI%20and%20robotics%202019-01) [GScholar](https://scholar.google.co.uk/scholar?q=Rodrigues%2C%20Rowena%20Jansen%2C%20Philip%20Brief%20report%20of%20the%20SIENNA%20foresight%20workshop%20on%20the%20social%20and%20ethical%20issues%20of%20AI%20and%20robotics%202019-01) [Scite](https://engine.scholarcy.com/scite_url?query=Rodrigues%2C%20Rowena%20Jansen%2C%20Philip%20Brief%20report%20of%20the%20SIENNA%20foresight%20workshop%20on%20the%20social%20and%20ethical%20issues%20of%20AI%20and%20robotics%202019-01)

[^Rodrigues_et+al_2018_a]: Rodrigues, Rowena, et al., D1.1: The consortium’s methodological handbook, WP1, 2018, Public deliverable report from the SIENNA project. [[Rodrigues_et+al_1ConsortiumMethodologicalHandbookWp1_2018]] [OA](https://scholar.google.co.uk/scholar?q=Rodrigues%20Rowena%20et%20al%20D11%20The%20consortiums%20methodological%20handbook%20WP1%202018%20Public%20deliverable%20report%20from%20the%20SIENNA%20project) [GScholar](https://scholar.google.co.uk/scholar?q=Rodrigues%20Rowena%20et%20al%20D11%20The%20consortiums%20methodological%20handbook%20WP1%202018%20Public%20deliverable%20report%20from%20the%20SIENNA%20project) 

[^Roff_2017_a]: Roff quoted in Lachow, “The Upside and Downside of Swarming Drones,” Bulletin of the Atomic Scientists, Vol. 73, No. 2, 2017, p. 96 [[Roff_RoffQuotedLachowtheUpside_2017]] [OA](https://engine.scholarcy.com/oa_version?query=Roff%20quoted%20in%20Lachow%20The%20Upside%20and%20Downside%20of%20Swarming%20Drones%20Bulletin%20of%20the%20Atomic%20Scientists%20Vol%2073%20No%202%202017%20p%2096) [GScholar](https://scholar.google.co.uk/scholar?q=Roff%20quoted%20in%20Lachow%20The%20Upside%20and%20Downside%20of%20Swarming%20Drones%20Bulletin%20of%20the%20Atomic%20Scientists%20Vol%2073%20No%202%202017%20p%2096) [Scite](https://engine.scholarcy.com/scite_url?query=Roff%20quoted%20in%20Lachow%20The%20Upside%20and%20Downside%20of%20Swarming%20Drones%20Bulletin%20of%20the%20Atomic%20Scientists%20Vol%2073%20No%202%202017%20p%2096)

[^Romano_2018_a]: Romano, Aja, “Jordan Peele’s simulated Obama PSA is a double-edged warning against fake news,” Vox, April 18, 2018. https://www.vox.com/2018/4/18/17252410/jordan-peele-obama-deepfakebuzzfeed [[Romano_JordanPeeleSimulatedObamaPsa_2018]] [OA](https://www.vox.com/2018/4/18/17252410/jordan-peele-obama-deepfakebuzzfeed)  [Scite](https://engine.scholarcy.com/scite_url?query=Romano%2C%20Aja%20Jordan%20Peele%E2%80%99s%20simulated%20Obama%20PSA%20is%20a%20double-edged%20warning%20against%20fake%20news%2C%202018-04-18)

[^Romano_et+al_2017_a]: Romano, Donato, Donati, Elisa, Benelli, Giovanni & Stefanini, Cesare, “A Review on Animal-Robot Interaction: From Bio-Hybrid Organisms to Mixed Societies”, Biological Cybernetics, October 2017. [[Romano_et+al_ReviewAnimalrobotInteractionFromBiohybrid_2017]] [OA](https://engine.scholarcy.com/oa_version?query=Romano%2C%20Donato%20Donati%2C%20Elisa%20Benelli%2C%20Giovanni%20Stefanini%2C%20Cesare%20A%20Review%20on%20Animal-Robot%20Interaction%3A%20From%20Bio-Hybrid%20Organisms%20to%20Mixed%20Societies%202017-10) [GScholar](https://scholar.google.co.uk/scholar?q=Romano%2C%20Donato%20Donati%2C%20Elisa%20Benelli%2C%20Giovanni%20Stefanini%2C%20Cesare%20A%20Review%20on%20Animal-Robot%20Interaction%3A%20From%20Bio-Hybrid%20Organisms%20to%20Mixed%20Societies%202017-10) [Scite](https://engine.scholarcy.com/scite_url?query=Romano%2C%20Donato%20Donati%2C%20Elisa%20Benelli%2C%20Giovanni%20Stefanini%2C%20Cesare%20A%20Review%20on%20Animal-Robot%20Interaction%3A%20From%20Bio-Hybrid%20Organisms%20to%20Mixed%20Societies%202017-10)

[^Rossi_2017_a]: Rossi, Francesca, “Human-AI Collaboration: Technical & Ethical Challenges,” OECD Conference, October 26, 2017. http://www.oecd.org/going-digital/ai-intelligent-machines-smartpolicies/conference-agenda/ai-intelligent-machines-smart-policies-rossi.pdf [[Rossi_HumanaiCollaborationTechnicalEthical_2017]] [OA](http://www.oecd.org/going-digital/ai-intelligent-machines-smartpolicies/conference-agenda/ai-intelligent-machines-smart-policies-rossi.pdf)  [Scite](https://engine.scholarcy.com/scite_url?query=Rossi%2C%20Francesca%20Human-AI%20Collaboration%3A%20Technical%20%26%20Ethical%20Challenges%2C%202017-10-26)

[^Rueben_et+al_2017_a]: Rueben, Matthew, Bernieri, Frank & Grimm, Cindy et al., “Framing Effects on Privacy Concerns about a Home Telepresence Robot”, 2017 IEEE International Conference on Human-Robot Interaction, March 2017. [[Rueben_et+al_FramingEffectsPrivacyConcernsAbout_2017]] [OA](https://engine.scholarcy.com/oa_version?query=Rueben%2C%20Matthew%20Bernieri%2C%20Frank%20Grimm%2C%20Cindy%20Framing%20Effects%20on%20Privacy%20Concerns%20about%20a%20Home%20Telepresence%20Robot%202017-03) [GScholar](https://scholar.google.co.uk/scholar?q=Rueben%2C%20Matthew%20Bernieri%2C%20Frank%20Grimm%2C%20Cindy%20Framing%20Effects%20on%20Privacy%20Concerns%20about%20a%20Home%20Telepresence%20Robot%202017-03) [Scite](https://engine.scholarcy.com/scite_url?query=Rueben%2C%20Matthew%20Bernieri%2C%20Frank%20Grimm%2C%20Cindy%20Framing%20Effects%20on%20Privacy%20Concerns%20about%20a%20Home%20Telepresence%20Robot%202017-03)

[^Russell_2016_a]: Russell, S., and Norvig, P., Artificial Intelligence: A Modern Approach, 3rd ed., Essex, Pearson, 2016, p. 929. [[Russell_ArtificialIntelligenceModernApproach_2016]] [OA](https://scholar.google.co.uk/scholar?q=Russell%2C%20S.%20Norvig%2C%20P.%20Artificial%20Intelligence%3A%20A%20Modern%20Approach%202016) [GScholar](https://scholar.google.co.uk/scholar?q=Russell%2C%20S.%20Norvig%2C%20P.%20Artificial%20Intelligence%3A%20A%20Modern%20Approach%202016) 

[^Ryan_et+al_2019_a]: Ryan, Mark, Philip Brey, Kevin Macnish, Tally Hatzakis, Owen King, Jonne Maas, Ruben Haasjes, Ana Fernandez, Sebastiano Martorana, Isaac Oluoch, Selen Eren, and Roxanne Van der Puil, Report on Ethical Tensions and Social Impacts. SHERPA Project, 2019, https://doi.org/10.21253/DMU.8397134. [[Ryan_et+al_ReportEthicalTensionsSocialImpacts_2019]] [OA](https://doi.org/10.21253/DMU.8397134)  [Scite](https://scite.ai/reports/10.21253/DMU.8397134)

[^Sadowski_2014_a]: Sadowski, Jathan, “Exoskeletons in a Disabilities Context: The Need for Social and Ethical Research”, Journal of Responsible Innovation, May 2014. [[Sadowski_ExoskeletonsDisabilitiesContextNeedSocial_2014]] [OA](https://engine.scholarcy.com/oa_version?query=Sadowski%2C%20Jathan%20Exoskeletons%20in%20a%20Disabilities%20Context%3A%20The%20Need%20for%20Social%20and%20Ethical%20Research%202014-05) [GScholar](https://scholar.google.co.uk/scholar?q=Sadowski%2C%20Jathan%20Exoskeletons%20in%20a%20Disabilities%20Context%3A%20The%20Need%20for%20Social%20and%20Ethical%20Research%202014-05) [Scite](https://engine.scholarcy.com/scite_url?query=Sadowski%2C%20Jathan%20Exoskeletons%20in%20a%20Disabilities%20Context%3A%20The%20Need%20for%20Social%20and%20Ethical%20Research%202014-05)

[^Salem_2015_a]: Salem, Maha & Dautenhahn, Kerstin, “Evaluating Trust and Safety in HRI: Practical Issues and Ethical Challenges”, The Emerging Policy of Ethics of Human Robot Interaction, March 2015. [[Salem_EvaluatingTrustSafetyHriPractical_2015]] [OA](https://engine.scholarcy.com/oa_version?query=Salem%2C%20Maha%20Dautenhahn%2C%20Kerstin%20Evaluating%20Trust%20and%20Safety%20in%20HRI%3A%20Practical%20Issues%20and%20Ethical%20Challenges%202015-03) [GScholar](https://scholar.google.co.uk/scholar?q=Salem%2C%20Maha%20Dautenhahn%2C%20Kerstin%20Evaluating%20Trust%20and%20Safety%20in%20HRI%3A%20Practical%20Issues%20and%20Ethical%20Challenges%202015-03) [Scite](https://engine.scholarcy.com/scite_url?query=Salem%2C%20Maha%20Dautenhahn%2C%20Kerstin%20Evaluating%20Trust%20and%20Safety%20in%20HRI%3A%20Practical%20Issues%20and%20Ethical%20Challenges%202015-03)

[^Coll_2014_a]: Sami Coll. (2014). Power, knowledge, and the subjects of privacy: understanding privacy as the ally of surveillance. Information, Communication & Society, 17(10), 1250-1263; Gordon Hull, (2015). Successful failure: what Foucault can teach us about privacy self-management in a world of Facebook and big data. Ethics and Information Technology, 17(2), 89-101; Omar Tene and [[Coll_PowerKnowledgeSubjectsPrivacyUnderstanding_2014]] [OA](https://engine.scholarcy.com/oa_version?query=Coll%2C%20Sami%20Power%2C%20knowledge%2C%20and%20the%20subjects%20of%20privacy%3A%20understanding%20privacy%20as%20the%20ally%20of%20surveillance%202014) [GScholar](https://scholar.google.co.uk/scholar?q=Coll%2C%20Sami%20Power%2C%20knowledge%2C%20and%20the%20subjects%20of%20privacy%3A%20understanding%20privacy%20as%20the%20ally%20of%20surveillance%202014) [Scite](https://engine.scholarcy.com/scite_url?query=Coll%2C%20Sami%20Power%2C%20knowledge%2C%20and%20the%20subjects%20of%20privacy%3A%20understanding%20privacy%20as%20the%20ally%20of%20surveillance%202014)

[^Polonetsky_2012_a]: Polonetsky, J. (2012). Big data for all: Privacy and user control in the age of analytics. Northwestern Journal of Technology & Intellectual Property, 11(5): 238-273. Santoni de Sio, Filippo, and Jeroen Van den Hoven, "Meaningful human control over autonomous systems: a philosophical account," Frontiers in Robotics and AI, Vol. 5, No. 15, 2018, DOI: 10.3389/frobt.2018.00015. SATORI, “CEN Workshop Agreement: Ethics assessment for research and innovation - Part 2: Ethical impact assessment framework, CWA 17145-2, June 2017. http://satoriproject.eu/media/CWA17145-23d2017.pdf [[Polonetsky_DataPrivacyUserControl_2012]] [OA](https://doi.org/10.3389/frobt.2018.00015)  [Scite](https://scite.ai/reports/10.3389/frobt.2018.00015)

[^Scharre_2014_a]: Scharre, Paul, “Robotics on the Battlefield Part II. The Coming Swarm”, Center for a New American Security, October 2014, p. 5–6; Magnuson, op. cit, 2016. [[Scharre_roboticsBattlefieldPartIiThe_2014]] [OA](https://engine.scholarcy.com/oa_version?query=Scharre%2C%20Paul%20%E2%80%9CRobotics%20on%20the%20Battlefield%20Part%20II.%20The%20Coming%20Swarm%E2%80%9D%2C%20Center%20for%20a%20New%202014-10) [GScholar](https://scholar.google.co.uk/scholar?q=Scharre%2C%20Paul%20%E2%80%9CRobotics%20on%20the%20Battlefield%20Part%20II.%20The%20Coming%20Swarm%E2%80%9D%2C%20Center%20for%20a%20New%202014-10) [Scite](https://engine.scholarcy.com/scite_url?query=Scharre%2C%20Paul%20%E2%80%9CRobotics%20on%20the%20Battlefield%20Part%20II.%20The%20Coming%20Swarm%E2%80%9D%2C%20Center%20for%20a%20New%202014-10)

[^Schlogl_et+al_2018_a]: Schlogl, L., and A. Sumner, A., “The Rise of the Robot Reserve Army: Automation and the Future of Economic Development, Work, and Wages in Developing Countries,” SSRN Electronic Journal, 2018. doi: 10.2139/ssrn.3208816 [[Schlogl_et+al_RiseRobotReserveArmyAutomation_2018]] [OA](https://doi.org/10.2139/ssrn.3208816)  [Scite](https://scite.ai/reports/10.2139/ssrn.3208816)

[^Searle_1999_a]: Searle, John R., Mind, Language and Society: Philosophy in the Real World, Phoenix, New York, 1999. [[Searle_LanguageSocietyPhilosophyRealWorld_1999]] [OA](https://scholar.google.co.uk/scholar?q=Searle%2C%20John%20R.%20Mind%20Language%20and%20Society%3A%20Philosophy%20in%20the%20Real%20World%201999) [GScholar](https://scholar.google.co.uk/scholar?q=Searle%2C%20John%20R.%20Mind%20Language%20and%20Society%3A%20Philosophy%20in%20the%20Real%20World%201999) 

[^Selbst_2017_a]: Selbst, Andrew D., “Disparate Impact in Big Data Policing,” Georgia Law Review, Vol. 52, Issue 1, 2017, p. 120. [[Selbst_DisparateImpactBigDataPolicing_2017]] [OA](https://engine.scholarcy.com/oa_version?query=Selbst%2C%20Andrew%20D.%20Disparate%20Impact%20in%20Big%20Data%20Policing%2C%202017) [GScholar](https://scholar.google.co.uk/scholar?q=Selbst%2C%20Andrew%20D.%20Disparate%20Impact%20in%20Big%20Data%20Policing%2C%202017) [Scite](https://engine.scholarcy.com/scite_url?query=Selbst%2C%20Andrew%20D.%20Disparate%20Impact%20in%20Big%20Data%20Policing%2C%202017)

[^Hao_2019_a]: Hao, Karen, “AI is sending people to jail – and getting it wrong”, MIT Technology Review, 21 January 2019. https://www.technologyreview.com/s/612775/algorithms-criminaljustice-ai [[Hao_AiSendingPeopleJail_2019]] [OA](https://www.technologyreview.com/s/612775/algorithms-criminaljustice-ai)  [Scite](https://engine.scholarcy.com/scite_url?query=Hao%2C%20Karen%20AI%20is%20sending%20people%20to%20jail%20%E2%80%93%20and%20getting%20it%20wrong%202019-01-21)

[^Selgelid_2009_a]: Selgelid, M., ‘Dual-Use Research Codes of Conduct: Lessons from the Life Sciences’, NanoEthics, Vol. 3, No. 3, pp. 175-183, 2009. [[Selgelid_DualuseResearchCodesConductLessons_2009]] [OA](https://engine.scholarcy.com/oa_version?query=Selgelid%2C%20M.%20Dual-Use%20Research%20Codes%20of%20Conduct%3A%20Lessons%20from%20the%20Life%20Sciences%202009) [GScholar](https://scholar.google.co.uk/scholar?q=Selgelid%2C%20M.%20Dual-Use%20Research%20Codes%20of%20Conduct%3A%20Lessons%20from%20the%20Life%20Sciences%202009) [Scite](https://engine.scholarcy.com/scite_url?query=Selgelid%2C%20M.%20Dual-Use%20Research%20Codes%20of%20Conduct%3A%20Lessons%20from%20the%20Life%20Sciences%202009)

[^Shakarian_et+al_2013_a]: Shakarian, Paulo, Jana Shakarian, and Andrew Ruel, Introduction to cyber-warfare: a multidisciplinary approach, Amsterdam, Morgan Kaufmann Publishers, 2013. [[Shakarian_et+al_IntroductionCyberwarfareMultidisciplinaryApproach_2013]] [OA](https://scholar.google.co.uk/scholar?q=Shakarian%2C%20Paulo%20Shakarian%2C%20Jana%20Ruel%2C%20Andrew%20Introduction%20to%20cyber-warfare%3A%20a%20multidisciplinary%20approach%202013) [GScholar](https://scholar.google.co.uk/scholar?q=Shakarian%2C%20Paulo%20Shakarian%2C%20Jana%20Ruel%2C%20Andrew%20Introduction%20to%20cyber-warfare%3A%20a%20multidisciplinary%20approach%202013) 

[^Shamsuddin_et+al_2012_a]: Shamsuddin, Syamimi, Yussof, Hanafiah & Ismail, Luthffi, et al., “Initial Response of Autistic Children in Human-Robot Interaction Therapy with Humanoid Robot NAO”, IEEE, March 2012. Shannon Liao, ‘Chinese Facial Recognition System Mistakes a Face on a Bus for a Jaywalker’, The Verge, 22 November 2018 <https://www.theverge.com/2018/11/22/18107885/china-facialrecognition-mistaken-jaywalker> [accessed 31 May 2019]. Shariff, Azim & Rahwan, Iyad, “Psychological Roadblocks to the Adoption of Self-Driving Vehicles”, Nature: Human Behaviour, September 2017. [[Shamsuddin_et+al_InitialResponseAutisticChildrenHumanrobot_2012]] [OA](https://www.theverge.com/2018/11/22/18107885/china-facialrecognition-mistaken-jaywalker>)  [Scite](https://engine.scholarcy.com/scite_url?query=Shamsuddin%2C%20Syamimi%20Yussof%2C%20Hanafiah%20Ismail%2C%20Luthffi%20Initial%20Response%20of%20Autistic%20Children%20in%20Human-Robot%20Interaction%20Therapy%20with%20Humanoid%20Robot%20NAO%202012-03-22)

[^Sharkey_2010_a]: Sharkey, Amanda, and Noel Sharkey, “Granny and the robots: ethical issues in robot care for the elderly,” Ethics and Information Technology, Vol. 14, No. 1, 2010, pp. 27–40. [[Sharkey_GrannyRobotsEthicalIssuesRobot_2010]] [OA](https://engine.scholarcy.com/oa_version?query=Sharkey%2C%20Amanda%20Sharkey%2C%20Noel%20Granny%20and%20the%20robots%3A%20ethical%20issues%20in%20robot%20care%20for%20the%20elderly%2C%202010) [GScholar](https://scholar.google.co.uk/scholar?q=Sharkey%2C%20Amanda%20Sharkey%2C%20Noel%20Granny%20and%20the%20robots%3A%20ethical%20issues%20in%20robot%20care%20for%20the%20elderly%2C%202010) [Scite](https://engine.scholarcy.com/scite_url?query=Sharkey%2C%20Amanda%20Sharkey%2C%20Noel%20Granny%20and%20the%20robots%3A%20ethical%20issues%20in%20robot%20care%20for%20the%20elderly%2C%202010)

[^Sharkey_2010_b]: Sharkey, Noel & Sharkey, Amanda, “The Crying Shame of Robot Nannies: An Ethical Appraisal”, Interaction Studies, 11(2), 2010. [[Sharkey_CryingShameRobotNanniesEthical_2010]] [OA](https://engine.scholarcy.com/oa_version?query=Sharkey%2C%20Noel%20Sharkey%2C%20Amanda%20The%20Crying%20Shame%20of%20Robot%20Nannies%3A%20An%20Ethical%20Appraisal%202010) [GScholar](https://scholar.google.co.uk/scholar?q=Sharkey%2C%20Noel%20Sharkey%2C%20Amanda%20The%20Crying%20Shame%20of%20Robot%20Nannies%3A%20An%20Ethical%20Appraisal%202010) [Scite](https://engine.scholarcy.com/scite_url?query=Sharkey%2C%20Noel%20Sharkey%2C%20Amanda%20The%20Crying%20Shame%20of%20Robot%20Nannies%3A%20An%20Ethical%20Appraisal%202010)

[^Shedletsky_2018_a]: Shedletsky, Anna-Katrina, “When Factories Have a Choice, It’s Best to Start with People”, Forbes, June 2018. [[Shedletsky_WhenFactoriesHaveChoiceIt_2018]] [OA](https://engine.scholarcy.com/oa_version?query=Shedletsky%2C%20Anna-Katrina%20When%20Factories%20Have%20a%20Choice%2C%20It%E2%80%99s%20Best%20to%20Start%20with%20People%202018-06) [GScholar](https://scholar.google.co.uk/scholar?q=Shedletsky%2C%20Anna-Katrina%20When%20Factories%20Have%20a%20Choice%2C%20It%E2%80%99s%20Best%20to%20Start%20with%20People%202018-06) [Scite](https://engine.scholarcy.com/scite_url?query=Shedletsky%2C%20Anna-Katrina%20When%20Factories%20Have%20a%20Choice%2C%20It%E2%80%99s%20Best%20to%20Start%20with%20People%202018-06)

[^Sheh_2018_a]: Sheh, Raymond, and Isaac Monteath, "Defining Explainable AI for Requirements Analysis," KIKünstliche Intelligenz, Vol. 32, No. 4, 2018, pp. 261-266., p. 263 Simon, Matt, “Companion Robots are Here. Just Don’t Fall in Love with Them”, Wired, February 2017. [[Sheh_DefiningExplainableAiRequirementsAnalysis_2018]] [OA](https://engine.scholarcy.com/oa_version?query=Sheh%2C%20Raymond%20Monteath%2C%20Isaac%20Defining%20Explainable%20AI%20for%20Requirements%20Analysis%2C%202018-02) [GScholar](https://scholar.google.co.uk/scholar?q=Sheh%2C%20Raymond%20Monteath%2C%20Isaac%20Defining%20Explainable%20AI%20for%20Requirements%20Analysis%2C%202018-02) [Scite](https://engine.scholarcy.com/scite_url?query=Sheh%2C%20Raymond%20Monteath%2C%20Isaac%20Defining%20Explainable%20AI%20for%20Requirements%20Analysis%2C%202018-02)

[^Simon_2018_a]: Simon, Matt, “The Serious Security Problem Looming Over Robotics”, Wired Science, August 2018. [[Simon_SeriousSecurityProblemLoomingOver_2018]] [OA](https://engine.scholarcy.com/oa_version?query=Simon%2C%20Matt%20The%20Serious%20Security%20Problem%20Looming%20Over%20Robotics%202018-08) [GScholar](https://scholar.google.co.uk/scholar?q=Simon%2C%20Matt%20The%20Serious%20Security%20Problem%20Looming%20Over%20Robotics%202018-08) [Scite](https://engine.scholarcy.com/scite_url?query=Simon%2C%20Matt%20The%20Serious%20Security%20Problem%20Looming%20Over%20Robotics%202018-08)

[^Simonite_2017_a]: Simonite, T., ‘Machines Taught by Photos Learn a Sexist View of Women’, Wired, August 21, 2017. https://www.wired.com/story/machines-taught-by-photos-learn-a-sexist-view-of-women/ [[Simonite_MachinesTaughtPhotosLearnSexist_2017]] [OA](https://www.wired.com/story/machines-taught-by-photos-learn-a-sexist-view-of-women/)  [Scite](https://engine.scholarcy.com/scite_url?query=Simonite%2C%20T.%20Machines%20Taught%20by%20Photos%20Learn%20a%20Sexist%20View%20of%20Women%202017-08-21)

[^Simonite_2018_a]: Simonite, T., ‘When It Comes to Gorillas, Google Photos Remains Blind’, Wired, January 11, 2018. https://www.wired.com/story/when-it-comes-to-gorillas-google-photos-remains-blind/ Simpson, Trudy, “Rise of the Healthcare Robots: Five Ethical Issues to Consider”, Christian Medical Fellowship, March 2016. [[Simonite_WhenItComesGorillasGoogle_2018]] [OA](https://www.wired.com/story/when-it-comes-to-gorillas-google-photos-remains-blind/Simpson)  [Scite](https://engine.scholarcy.com/scite_url?query=Simonite%2C%20T.%20When%20It%20Comes%20to%20Gorillas%2C%20Google%20Photos%20Remains%20Blind%202018-01-11)

[^Simshaw_2019_a]: Simshaw, Drew, “Ethical Issues in Robo-Lawyering: The Need for Guidance on Developing and Using Artificial Intelligence in the Practice of Law,” Hastings Law Journal, Vol. 70, 2019, pp. 173–214. [[Simshaw_EthicalIssuesRobolawyeringNeedGuidance_2019]] [OA](https://engine.scholarcy.com/oa_version?query=Simshaw%2C%20Drew%20Ethical%20Issues%20in%20Robo-Lawyering%3A%20The%20Need%20for%20Guidance%20on%20Developing%20and%20Using%20Artificial%20Intelligence%20in%20the%20Practice%20of%20Law%2C%202019) [GScholar](https://scholar.google.co.uk/scholar?q=Simshaw%2C%20Drew%20Ethical%20Issues%20in%20Robo-Lawyering%3A%20The%20Need%20for%20Guidance%20on%20Developing%20and%20Using%20Artificial%20Intelligence%20in%20the%20Practice%20of%20Law%2C%202019) [Scite](https://engine.scholarcy.com/scite_url?query=Simshaw%2C%20Drew%20Ethical%20Issues%20in%20Robo-Lawyering%3A%20The%20Need%20for%20Guidance%20on%20Developing%20and%20Using%20Artificial%20Intelligence%20in%20the%20Practice%20of%20Law%2C%202019)

[^Singler_2018_a]: Singler, Beth, “Are We Expecting Automation to Give Us Modern Day Slaves?”, World Economic Forum, May 2018. [[Singler_WeExpectingAutomationGiveUs_2018]] [OA](https://engine.scholarcy.com/oa_version?query=Singler%2C%20Beth%20Are%20We%20Expecting%20Automation%20to%20Give%20Us%20Modern%20Day%20Slaves%3F%202018-05) [GScholar](https://scholar.google.co.uk/scholar?q=Singler%2C%20Beth%20Are%20We%20Expecting%20Automation%20to%20Give%20Us%20Modern%20Day%20Slaves%3F%202018-05) [Scite](https://engine.scholarcy.com/scite_url?query=Singler%2C%20Beth%20Are%20We%20Expecting%20Automation%20to%20Give%20Us%20Modern%20Day%20Slaves%3F%202018-05)

[^Skrzypczak_et+al_2017_a]: Skrzypczak, Tomasz, Krela, Rafal & Kwiatkowski, Wojciech et al, “Plant Science View on Biohybrid Development”, Frontiers in Bioengineering and Biotechnology, 2017. [[Skrzypczak_et+al_WojciechplantScienceView_2017]] [OA](https://engine.scholarcy.com/oa_version?query=Skrzypczak%2C%20Tomasz%20Krela%2C%20Rafal%20Kwiatkowski%20Wojciech%20et%20al%2C%20%E2%80%9CPlant%20Science%20View%20on%20Biohybrid%20Development%E2%80%9D%202017) [GScholar](https://scholar.google.co.uk/scholar?q=Skrzypczak%2C%20Tomasz%20Krela%2C%20Rafal%20Kwiatkowski%20Wojciech%20et%20al%2C%20%E2%80%9CPlant%20Science%20View%20on%20Biohybrid%20Development%E2%80%9D%202017) [Scite](https://engine.scholarcy.com/scite_url?query=Skrzypczak%2C%20Tomasz%20Krela%2C%20Rafal%20Kwiatkowski%20Wojciech%20et%20al%2C%20%E2%80%9CPlant%20Science%20View%20on%20Biohybrid%20Development%E2%80%9D%202017)

[^Machines_2018_a]: Machines, March 2018. [[Machines__2018]] [OA](https://scholar.google.co.uk/scholar?q=Machines%20March%202018) [GScholar](https://scholar.google.co.uk/scholar?q=Machines%20March%202018) 

[^Neuroprostheses_2018_a]: Neuroprostheses for Patients with Spinal Cord Injuries”, PM&R 10(9), September 2018. Solon Barocas and Andrew D. Selbst, ‘Big Data’s Disparate Impact’, California Law Review, Vol. 194, 2016, p. 674; Sarah Brayne, ‘Big Data Surveillance: The Case of Policing’, American Sociological Review, Vol. 82, No. 5 2017, p. 978. [[Neuroprostheses_NeuroprosthesesPatientsWithSpinalCord_2018]] [OA](https://engine.scholarcy.com/oa_version?query=Neuroprostheses%20for%20Patients%20with%20Spinal%20Cord%20Injuries%20PMR%20109%20September%202018%20Solon%20Barocas%20and%20Andrew%20D%20Selbst%20Big%20Datas%20Disparate%20Impact%20California%20Law%20Review%20Vol%20194%202016%20p%20674%20Sarah%20Brayne%20Big%20Data%20Surveillance%20The%20Case%20of%20Policing%20American%20Sociological%20Review%20Vol%2082%20No%205%202017%20p%20978) [GScholar](https://scholar.google.co.uk/scholar?q=Neuroprostheses%20for%20Patients%20with%20Spinal%20Cord%20Injuries%20PMR%20109%20September%202018%20Solon%20Barocas%20and%20Andrew%20D%20Selbst%20Big%20Datas%20Disparate%20Impact%20California%20Law%20Review%20Vol%20194%202016%20p%20674%20Sarah%20Brayne%20Big%20Data%20Surveillance%20The%20Case%20of%20Policing%20American%20Sociological%20Review%20Vol%2082%20No%205%202017%20p%20978) [Scite](https://engine.scholarcy.com/scite_url?query=Neuroprostheses%20for%20Patients%20with%20Spinal%20Cord%20Injuries%20PMR%20109%20September%202018%20Solon%20Barocas%20and%20Andrew%20D%20Selbst%20Big%20Datas%20Disparate%20Impact%20California%20Law%20Review%20Vol%20194%202016%20p%20674%20Sarah%20Brayne%20Big%20Data%20Surveillance%20The%20Case%20of%20Policing%20American%20Sociological%20Review%20Vol%2082%20No%205%202017%20p%20978)

[^Soraker_2007_a]: Soraker, Johnny Hartz, and Philip Brey, "Ambient intelligence and problems with inferring desires from behaviour," International Review of Information Ethics, Vol. 8, no. 1, 2007, pp. 7-12. [[Soraker_AmbientIntelligenceProblemsWithInferring_2007]] [OA](https://engine.scholarcy.com/oa_version?query=Soraker%2C%20Johnny%20Hartz%20Brey%2C%20Philip%20Ambient%20intelligence%20and%20problems%20with%20inferring%20desires%20from%20behaviour%2C%202007) [GScholar](https://scholar.google.co.uk/scholar?q=Soraker%2C%20Johnny%20Hartz%20Brey%2C%20Philip%20Ambient%20intelligence%20and%20problems%20with%20inferring%20desires%20from%20behaviour%2C%202007) [Scite](https://engine.scholarcy.com/scite_url?query=Soraker%2C%20Johnny%20Hartz%20Brey%2C%20Philip%20Ambient%20intelligence%20and%20problems%20with%20inferring%20desires%20from%20behaviour%2C%202007)

[^Sparrow_2017_a]: Sparrow, Robert & Howard, Mark, “When Human Beings are Like Drunk Robots: Driverless Vehicles, Ethics, and the Future of Transport,” Transportation Research Part C: Emerging Technologies, July 2017. [[Sparrow_WhenHumanBeingsLikeDrunk_2017]] [OA](https://engine.scholarcy.com/oa_version?query=Sparrow%2C%20Robert%20Howard%2C%20Mark%20When%20Human%20Beings%20are%20Like%20Drunk%20Robots%3A%20Driverless%20Vehicles%2C%20Ethics%2C%20and%20the%20Future%20of%20Transport%2C%202017-07) [GScholar](https://scholar.google.co.uk/scholar?q=Sparrow%2C%20Robert%20Howard%2C%20Mark%20When%20Human%20Beings%20are%20Like%20Drunk%20Robots%3A%20Driverless%20Vehicles%2C%20Ethics%2C%20and%20the%20Future%20of%20Transport%2C%202017-07) [Scite](https://engine.scholarcy.com/scite_url?query=Sparrow%2C%20Robert%20Howard%2C%20Mark%20When%20Human%20Beings%20are%20Like%20Drunk%20Robots%3A%20Driverless%20Vehicles%2C%20Ethics%2C%20and%20the%20Future%20of%20Transport%2C%202017-07)

[^Sparrow_2019_a]: Sparrow, Robert, “Robotics Has a Race Problem,” Science, Technology, & Human Values, 2019. [[Sparrow_RoboticsHasRaceProblem_2019]] [OA](https://engine.scholarcy.com/oa_version?query=Sparrow%2C%20Robert%20Robotics%20Has%20a%20Race%20Problem%2C%202019) [GScholar](https://scholar.google.co.uk/scholar?q=Sparrow%2C%20Robert%20Robotics%20Has%20a%20Race%20Problem%2C%202019) [Scite](https://engine.scholarcy.com/scite_url?query=Sparrow%2C%20Robert%20Robotics%20Has%20a%20Race%20Problem%2C%202019)

[^Sparrow_2016_a]: Sparrow, Robert, “Robots in aged care: a dystopian future?” AI & Society, Vol. 31, No. 4, 2016, pp. 445–454. [[Sparrow_RobotsAgedCareDystopianFuture_2016]] [OA](https://engine.scholarcy.com/oa_version?query=Sparrow%2C%20Robert%20Robots%20in%20aged%20care%3A%20a%20dystopian%20future%3F%202016) [GScholar](https://scholar.google.co.uk/scholar?q=Sparrow%2C%20Robert%20Robots%20in%20aged%20care%3A%20a%20dystopian%20future%3F%202016) [Scite](https://engine.scholarcy.com/scite_url?query=Sparrow%2C%20Robert%20Robots%20in%20aged%20care%3A%20a%20dystopian%20future%3F%202016)

[^Sparrow_2017_b]: Sparrow, Robert, “Robots, Rape, and Representation”, International of Journal of Social Robotics, May 2017. [[Sparrow_RobotsRapeRepresentation_2017]] [OA](https://engine.scholarcy.com/oa_version?query=Sparrow%2C%20Robert%20Robots%2C%20Rape%2C%20and%20Representation%202017-05) [GScholar](https://scholar.google.co.uk/scholar?q=Sparrow%2C%20Robert%20Robots%2C%20Rape%2C%20and%20Representation%202017-05) [Scite](https://engine.scholarcy.com/scite_url?query=Sparrow%2C%20Robert%20Robots%2C%20Rape%2C%20and%20Representation%202017-05)

[^Spilka_2019_a]: Spilka, Dmytro, “How AI Is Keeping Us Safe From Drivers Who Use Their Mobile Phones at the Wheel,” Datafloq, February 15, 2019. https://datafloq.com/read/aikeeping-safe-drivers-usingmobile-phones-wheel/6064. [[Spilka_AiIsKeepingUsSafe_2019]] [OA](https://datafloq.com/read/aikeeping-safe-drivers-usingmobile-phones-wheel/6064)  [Scite](https://engine.scholarcy.com/scite_url?query=Spilka%2C%20Dmytro%20How%20AI%20Is%20Keeping%20Us%20Safe%20From%20Drivers%20Who%20Use%20Their%20Mobile%20Phones%20at%20the%20Wheel%2C%202019-02-15)

[^Springer_et+al_2018_a]: Springer, A., Garcia-Gathright, J., and Cramer, H., ‘Assessing and Addressing Algorithmic Bias – But Before We Get There’, 2018 AAAI Spring Symposium Series, March 2018, pp. 450-454. https://www.aaai.org/ocs/index.php/SSS/SSS18/paper/viewPaper/17542. [[Springer_et+al_AssessingAddressingAlgorithmicBias_2018]] [OA](https://www.aaai.org/ocs/index.php/SSS/SSS18/paper/viewPaper/17542)  [Scite](https://engine.scholarcy.com/scite_url?query=Springer%2C%20A.%20Garcia-Gathright%2C%20J.%20Cramer%2C%20H.%20Assessing%20and%20Addressing%20Algorithmic%20Bias%20%E2%80%93%20But%20Before%20We%20Get%20There%202018-03)

[^Stahl_2006_a]: Stahl, B. C. (2006). Responsible computers? A case for ascribing quasi-responsibility to computers independent of personhood or agency. Ethics and Information Technology, 8(4), 205–213. [[Stahl_ResponsibleComputersACaseAscribing_2006]] [OA](https://engine.scholarcy.com/oa_version?query=Stahl%2C%20B.C.%20Responsible%20computers%3F%20A%20case%20for%20ascribing%20quasi-responsibility%20to%20computers%20independent%20of%20personhood%20or%20agency%202006) [GScholar](https://scholar.google.co.uk/scholar?q=Stahl%2C%20B.C.%20Responsible%20computers%3F%20A%20case%20for%20ascribing%20quasi-responsibility%20to%20computers%20independent%20of%20personhood%20or%20agency%202006) [Scite](https://engine.scholarcy.com/scite_url?query=Stahl%2C%20B.C.%20Responsible%20computers%3F%20A%20case%20for%20ascribing%20quasi-responsibility%20to%20computers%20independent%20of%20personhood%20or%20agency%202006)

[^Stahl_2016_a]: Stahl, Bernd Carsten & Coeckelbergh, Mark, “Ethics of Healthcare Robotics: Towards Responsible Research and Innovation”, Robotics and Autonomous Systems, 86, December 2016. [[Stahl_EthicsHealthcareRoboticsTowardsResponsible_2016]] [OA](https://engine.scholarcy.com/oa_version?query=Stahl%2C%20Bernd%20Carsten%20Coeckelbergh%2C%20Mark%20Ethics%20of%20Healthcare%20Robotics%3A%20Towards%20Responsible%20Research%20and%20Innovation%202016-12) [GScholar](https://scholar.google.co.uk/scholar?q=Stahl%2C%20Bernd%20Carsten%20Coeckelbergh%2C%20Mark%20Ethics%20of%20Healthcare%20Robotics%3A%20Towards%20Responsible%20Research%20and%20Innovation%202016-12) [Scite](https://engine.scholarcy.com/scite_url?query=Stahl%2C%20Bernd%20Carsten%20Coeckelbergh%2C%20Mark%20Ethics%20of%20Healthcare%20Robotics%3A%20Towards%20Responsible%20Research%20and%20Innovation%202016-12)

[^Stansbury_et+al_2014_a]: Stansbury, Richard, Olds, Joshua & Coyle, Eric, “Ethical Concerns of Unmanned and Autonomous Systems in Engineering Programs”, 121st ASEE Annual Conference & Exposition, June 2014. [[Stansbury_et+al_EthicalConcernsUnmannedAutonomousSystems_2014]] [OA](https://scholar.google.co.uk/scholar?q=Stansbury%2C%20Richard%20Olds%2C%20Joshua%20Coyle%2C%20Eric%20Ethical%20Concerns%20of%20Unmanned%20and%20Autonomous%20Systems%20in%20Engineering%20Programs%202014-06) [GScholar](https://scholar.google.co.uk/scholar?q=Stansbury%2C%20Richard%20Olds%2C%20Joshua%20Coyle%2C%20Eric%20Ethical%20Concerns%20of%20Unmanned%20and%20Autonomous%20Systems%20in%20Engineering%20Programs%202014-06) 

[^Steinfeld_2010_a]: Steinfeld, Aaron, “Ethics and Policy Implications for Inclusive Intelligent Transportation Systems,” Robotics Institute at Carnegie Mellon University, 2010. [[Steinfeld_EthicsPolicyImplicationsInclusiveIntelligent_2010]] [OA](https://scholar.google.co.uk/scholar?q=Steinfeld%2C%20Aaron%20Ethics%20and%20Policy%20Implications%20for%20Inclusive%20Intelligent%20Transportation%20Systems%2C%202010) [GScholar](https://scholar.google.co.uk/scholar?q=Steinfeld%2C%20Aaron%20Ethics%20and%20Policy%20Implications%20for%20Inclusive%20Intelligent%20Transportation%20Systems%2C%202010) 

[^Stephens_2012_a]: Stephens, Tim, “Robotics Project Aims to Develop Systems for Human-Robot Collaboration”, UC Santa Cruz Newscenter, December 2012. [[Stephens_RoboticsProjectAimsDevelopSystems_2012]] [OA](https://engine.scholarcy.com/oa_version?query=Stephens%2C%20Tim%20Robotics%20Project%20Aims%20to%20Develop%20Systems%20for%20Human-Robot%20Collaboration%202012-12) [GScholar](https://scholar.google.co.uk/scholar?q=Stephens%2C%20Tim%20Robotics%20Project%20Aims%20to%20Develop%20Systems%20for%20Human-Robot%20Collaboration%202012-12) [Scite](https://engine.scholarcy.com/scite_url?query=Stephens%2C%20Tim%20Robotics%20Project%20Aims%20to%20Develop%20Systems%20for%20Human-Robot%20Collaboration%202012-12)

[^Stobbs_et+al_2017_a]: Stobbs, Nigel, Bagaric, Mirko, and Hunter, Dan, “Can Sentencing Be Enhanced by the Use of Artificial Intelligence?,” Criminal Law Journal, Vol. 41, Issue 5, 2017, pp. 261–77. [[Stobbs_et+al_SentencingBeEnhancedUseArtificial_2017]] [OA](https://engine.scholarcy.com/oa_version?query=Stobbs%2C%20Nigel%20Bagaric%2C%20Mirko%20Hunter%2C%20Dan%20Can%20Sentencing%20Be%20Enhanced%20by%20the%20Use%20of%20Artificial%20Intelligence%3F%2C%202017) [GScholar](https://scholar.google.co.uk/scholar?q=Stobbs%2C%20Nigel%20Bagaric%2C%20Mirko%20Hunter%2C%20Dan%20Can%20Sentencing%20Be%20Enhanced%20by%20the%20Use%20of%20Artificial%20Intelligence%3F%2C%202017) [Scite](https://engine.scholarcy.com/scite_url?query=Stobbs%2C%20Nigel%20Bagaric%2C%20Mirko%20Hunter%2C%20Dan%20Can%20Sentencing%20Be%20Enhanced%20by%20the%20Use%20of%20Artificial%20Intelligence%3F%2C%202017)

[^Stocker_1997_a]: Stocker, Michael, “Abstract and concrete value: Plurality, conflict and maximization,” in Incommensurability,,,,Incomparability and Practical Reason, R. Chang, Ed., Cambridge, MA, USA: Harvard Univ. Press, 1997. [[Stocker_abstractConcreteValuePluralityConflict_1997]] [OA](https://scholar.google.co.uk/scholar?q=Stocker%2C%20Michael%20%E2%80%9CAbstract%20and%20concrete%20value%3A%20Plurality%2C%20conflict%20and%20maximization%2C%E2%80%9D%20in%20Incommensurability%2C%2C%2C%2C%20Incomparability%20and%20Practical%201997) [GScholar](https://scholar.google.co.uk/scholar?q=Stocker%2C%20Michael%20%E2%80%9CAbstract%20and%20concrete%20value%3A%20Plurality%2C%20conflict%20and%20maximization%2C%E2%80%9D%20in%20Incommensurability%2C%2C%2C%2C%20Incomparability%20and%20Practical%201997) 

[^Sullivan_2019_a]: Sullivan, Hannah R. and Schweikart, Scott J., “Are Current Tort Liability Doctrines Adequate for Addressing Injury Caused by AI?,” AMA Journal of Ethics, Vol. 21, No. 2, 2019. [[Sullivan_CurrentTortLiabilityDoctrinesAdequate_2019]] [OA](https://engine.scholarcy.com/oa_version?query=Sullivan%2C%20Hannah%20R.%20Schweikart%2C%20Scott%20J.%20Are%20Current%20Tort%20Liability%20Doctrines%20Adequate%20for%20Addressing%20Injury%20Caused%20by%20AI%3F%2C%202019) [GScholar](https://scholar.google.co.uk/scholar?q=Sullivan%2C%20Hannah%20R.%20Schweikart%2C%20Scott%20J.%20Are%20Current%20Tort%20Liability%20Doctrines%20Adequate%20for%20Addressing%20Injury%20Caused%20by%20AI%3F%2C%202019) [Scite](https://engine.scholarcy.com/scite_url?query=Sullivan%2C%20Hannah%20R.%20Schweikart%2C%20Scott%20J.%20Are%20Current%20Tort%20Liability%20Doctrines%20Adequate%20for%20Addressing%20Injury%20Caused%20by%20AI%3F%2C%202019)

[^Susskind_2015_a]: Susskind, Richard and Daniel Susskind, The Future of Professions: How Technology Will Transform the Work of Human Experts, Oxford University Press, Oxford, 2015. [[Susskind_FutureProfessionsTechnologyWillTransform_2015]] [OA](https://scholar.google.co.uk/scholar?q=Susskind%2C%20Richard%20Susskind%2C%20Daniel%20The%20Future%20of%20Professions%3A%20How%20Technology%20Will%20Transform%20the%20Work%20of%20Human%20Experts%202015) [GScholar](https://scholar.google.co.uk/scholar?q=Susskind%2C%20Richard%20Susskind%2C%20Daniel%20The%20Future%20of%20Professions%3A%20How%20Technology%20Will%20Transform%20the%20Work%20of%20Human%20Experts%202015) 

[^Suster_et+al_2017_a]: Suster, Simon, Stephan Tulkens, and Walter Daelemans, “A Short Review of Ethical Challenges in Clinical Natural Language Processing,” Proceedings of the First ACL Workshop on Ethics in Natural Language Processing, 2017. [[Suster_et+al_ShortReviewEthicalChallengesClinical_2017]] [OA](https://engine.scholarcy.com/oa_version?query=Suster%2C%20Simon%20Tulkens%2C%20Stephan%20Daelemans%2C%20Walter%20A%20Short%20Review%20of%20Ethical%20Challenges%20in%20Clinical%20Natural%20Language%20Processing%2C%202017) [GScholar](https://scholar.google.co.uk/scholar?q=Suster%2C%20Simon%20Tulkens%2C%20Stephan%20Daelemans%2C%20Walter%20A%20Short%20Review%20of%20Ethical%20Challenges%20in%20Clinical%20Natural%20Language%20Processing%2C%202017) [Scite](https://engine.scholarcy.com/scite_url?query=Suster%2C%20Simon%20Tulkens%2C%20Stephan%20Daelemans%2C%20Walter%20A%20Short%20Review%20of%20Ethical%20Challenges%20in%20Clinical%20Natural%20Language%20Processing%2C%202017)

[^Szegedy_et+al_2013_a]: Szegedy, Christian, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus, "Intriguing properties of neural networks," arXiv preprint arXiv:1312.6199, 2013. [[Szegedy_et+al_IntriguingPropertiesNeuralNetworks_2013]] [OA](https://export.arxiv.org/pdf/1312.6199)  

[^Tal_2016_b]: Tal Zarsky (2016) The trouble with algorithmic decisions an analytic road map to examine efficiency and fairness in automated and opaque decision making. Science, Technology & Human Values 41(1): 118–132. [[Tal_Zarsky_2016]] [OA](https://engine.scholarcy.com/oa_version?query=Tal%20Zarsky%202016%20The%20trouble%20with%20algorithmic%20decisions%20an%20analytic%20road%20map%20to%20examine%20efficiency%20and%20fairness%20in%20automated%20and%20opaque%20decision%20making%20Science%20Technology%20%20Human%20Values%20411%20118132) [GScholar](https://scholar.google.co.uk/scholar?q=Tal%20Zarsky%202016%20The%20trouble%20with%20algorithmic%20decisions%20an%20analytic%20road%20map%20to%20examine%20efficiency%20and%20fairness%20in%20automated%20and%20opaque%20decision%20making%20Science%20Technology%20%20Human%20Values%20411%20118132) [Scite](https://engine.scholarcy.com/scite_url?query=Tal%20Zarsky%202016%20The%20trouble%20with%20algorithmic%20decisions%20an%20analytic%20road%20map%20to%20examine%20efficiency%20and%20fairness%20in%20automated%20and%20opaque%20decision%20making%20Science%20Technology%20%20Human%20Values%20411%20118132)

[^Tamburrini_2009_a]: Tamburrini, Guglielmo. “Robot Ethics: A View from the Philosophy of Science”, Ethics and robotics, 2009. [[Tamburrini_RobotEthicsViewFromPhilosophy_2009]] [OA](https://engine.scholarcy.com/oa_version?query=Tamburrini%2C%20Guglielmo%20Robot%20Ethics%3A%20A%20View%20from%20the%20Philosophy%20of%20Science%202009) [GScholar](https://scholar.google.co.uk/scholar?q=Tamburrini%2C%20Guglielmo%20Robot%20Ethics%3A%20A%20View%20from%20the%20Philosophy%20of%20Science%202009) [Scite](https://engine.scholarcy.com/scite_url?query=Tamburrini%2C%20Guglielmo%20Robot%20Ethics%3A%20A%20View%20from%20the%20Philosophy%20of%20Science%202009)

[^Tarus_0000_a]: Tarus, Niu, and Mustafa, “Knowledge-Based Recommendation”. Tatman, Rachael, “Gender and Dialect Bias in YouTubes Automatic Captions,” Proceedings of the First [[Tarus_KnowledgebasedRecommendationTatmanRachaelgender_0000]] [OA](https://engine.scholarcy.com/oa_version?query=Tarus%2C%20Niu%20Mustafa%20Knowledge-Based%20Recommendation%E2%80%9D.%20Tatman%2C%20Rachael%2C%20%E2%80%9CGender%20and%20Dialect%20Bias%20in%20YouTubes%20Automatic%20Captions%2C) [GScholar](https://scholar.google.co.uk/scholar?q=Tarus%2C%20Niu%20Mustafa%20Knowledge-Based%20Recommendation%E2%80%9D.%20Tatman%2C%20Rachael%2C%20%E2%80%9CGender%20and%20Dialect%20Bias%20in%20YouTubes%20Automatic%20Captions%2C) [Scite](https://engine.scholarcy.com/scite_url?query=Tarus%2C%20Niu%20Mustafa%20Knowledge-Based%20Recommendation%E2%80%9D.%20Tatman%2C%20Rachael%2C%20%E2%80%9CGender%20and%20Dialect%20Bias%20in%20YouTubes%20Automatic%20Captions%2C)

[^ACL_2017_b]: ACL Workshop on Ethics in Natural Language Processing, 2017. [[ACL__2017]] [OA](https://engine.scholarcy.com/oa_version?query=ACL%20Workshop%20on%20Ethics%20in%20Natural%20Language%20Processing%202017) [GScholar](https://scholar.google.co.uk/scholar?q=ACL%20Workshop%20on%20Ethics%20in%20Natural%20Language%20Processing%202017) [Scite](https://engine.scholarcy.com/scite_url?query=ACL%20Workshop%20on%20Ethics%20in%20Natural%20Language%20Processing%202017)

[^RO-MAN_2009_a]: RO-MAN 2009 - The 18th IEEE International Symposium on Robot and Human Interactive Communication, 2009. doi: 10.1109/roman.2009.5326242 [[RO-MAN_Roman2009The18th_2009]] [OA](https://doi.org/10.1109/roman.2009.5326242)  [Scite](https://scite.ai/reports/10.1109/roman.2009.5326242)

[^Karppi_2018_a]: Ter Karppi, ‘“The Computer Said So”: On the Ethics, Effectiveness, and Cultural Techniques of Predictive Policing’, Social Media + Society, 2018, p. 1. [[Karppi_theComputerSaidSoEthics_2018]] [OA](https://engine.scholarcy.com/oa_version?query=Karppi%2C%20Ter%20%E2%80%9CThe%20Computer%20Said%20So%E2%80%9D%3A%20On%20the%20Ethics%2C%20Effectiveness%2C%20and%20Cultural%20Techniques%20of%20Predictive%20Policing%202018) [GScholar](https://scholar.google.co.uk/scholar?q=Karppi%2C%20Ter%20%E2%80%9CThe%20Computer%20Said%20So%E2%80%9D%3A%20On%20the%20Ethics%2C%20Effectiveness%2C%20and%20Cultural%20Techniques%20of%20Predictive%20Policing%202018) [Scite](https://engine.scholarcy.com/scite_url?query=Karppi%2C%20Ter%20%E2%80%9CThe%20Computer%20Said%20So%E2%80%9D%3A%20On%20the%20Ethics%2C%20Effectiveness%2C%20and%20Cultural%20Techniques%20of%20Predictive%20Policing%202018)

[^Tesla_2016_a]: Tesla, ‘A Tragic Loss’, June 30, 2016. https://www.tesla.com/blog/tragic-loss. Thaler, Richard H., and Cass R. Sunstein, Nudge: Improving Decisions about Health, Wealth, and Happiness, Penguin Books, New York, 2008. The Boston Consulting Group, “The Shifting Economics of Global Manufacturing”, February 2015. [[Tesla_TragicLoss_2016]] [OA](https://www.tesla.com/blog/tragic-loss)  [Scite](https://engine.scholarcy.com/scite_url?query=Tesla%20A%20Tragic%20Loss%202016-02-30)

[^Thieltges_et+al_2016_a]: Thieltges, Andree, Florian Schmidt, and Simon Hegelich, "The devil’s triangle: Ethical considerations on developing bot detection methods," 2016 AAAI Spring Symposium Series, 2016. [[Thieltges_et+al_DevilTriangleEthicalConsiderationsDeveloping_2016]] [OA](https://engine.scholarcy.com/oa_version?query=Thieltges%2C%20Andree%20Schmidt%2C%20Florian%20Hegelich%2C%20Simon%20The%20devil%E2%80%99s%20triangle%3A%20Ethical%20considerations%20on%20developing%20bot%20detection%20methods%2C%202016) [GScholar](https://scholar.google.co.uk/scholar?q=Thieltges%2C%20Andree%20Schmidt%2C%20Florian%20Hegelich%2C%20Simon%20The%20devil%E2%80%99s%20triangle%3A%20Ethical%20considerations%20on%20developing%20bot%20detection%20methods%2C%202016) [Scite](https://engine.scholarcy.com/scite_url?query=Thieltges%2C%20Andree%20Schmidt%2C%20Florian%20Hegelich%2C%20Simon%20The%20devil%E2%80%99s%20triangle%3A%20Ethical%20considerations%20on%20developing%20bot%20detection%20methods%2C%202016)

[^Thornton_et+al_2017_a]: Thornton, Sarah, Pan Selina, Erlien, Stephen & Gerdes, Christian, “Incorporating Ethical Considerations Into Automated Vehicle Control”, IEEE Transactions on Intelligent Transportation Systems 18(6), June 2017. [[Thornton_et+al_IncorporatingEthicalConsiderationsIntoAutomated_2017]] [OA](https://engine.scholarcy.com/oa_version?query=Thornton%2C%20Sarah%20Selina%2C%20Pan%20Erlien%2C%20Stephen%20Gerdes%2C%20Christian%20Incorporating%20Ethical%20Considerations%20Into%20Automated%20Vehicle%20Control%202017-06) [GScholar](https://scholar.google.co.uk/scholar?q=Thornton%2C%20Sarah%20Selina%2C%20Pan%20Erlien%2C%20Stephen%20Gerdes%2C%20Christian%20Incorporating%20Ethical%20Considerations%20Into%20Automated%20Vehicle%20Control%202017-06) [Scite](https://engine.scholarcy.com/scite_url?query=Thornton%2C%20Sarah%20Selina%2C%20Pan%20Erlien%2C%20Stephen%20Gerdes%2C%20Christian%20Incorporating%20Ethical%20Considerations%20Into%20Automated%20Vehicle%20Control%202017-06)

[^Tiffany_2019_a]: Tiffany, Kaitlyn, “A Timeline of High-Profile Tech Apologies.,” Vox, July 26, 2019. https://www.vox.com/the-goods/2019/7/26/8930765/tech-apologies-former-facebook-googletwitter-employees-list Todd, Sarah, “Inside the surprisingly sexist world of artificial intelligence,” Quartz, October 25, 2015. Retrieved at https://qz.com/531257/. Toiviainen, “Symbolic AI Versus Connectionism in Music Research”, 1. Toon Calders, Kamiran, F and Pechenizkiy, M (2009) Building classifiers with independency constraints. In: Data mining workshops, 2009. ICDMW’09; Cohen IG, Amarasingham R, Shah A, et al. (2014) The legal and ethical concerns that arise from using complex predictive analytics in health care. Health Affairs 33(7):1139–1147; and Anthony Danna and Gandy OH Jr (2002) All that glitters is not gold: Digging beneath the surface of data mining. Journal of Business Ethics 40(4):373–38. [[Tiffany_TimelineHighprofileTechApologies_2019]] [OA](https://www.vox.com/the-goods/2019/7/26/8930765/tech-apologies-former-facebook-googletwitter-employees-list)  [Scite](https://engine.scholarcy.com/scite_url?query=Tiffany%2C%20Kaitlyn%20A%20Timeline%20of%20High-Profile%20Tech%20Apologies.%2C%202019-07-26)

[^Torresen_2018_a]: Torresen, Jim, “A Review of Future and Ethical Perspectives of Robotics and AI”, Frontiers in Robotics and AI: Evolutionary Robots, January 2018. [[Torresen_ReviewFutureEthicalPerspectivesRobotics_2018]] [OA](https://engine.scholarcy.com/oa_version?query=Torresen%2C%20Jim%20A%20Review%20of%20Future%20and%20Ethical%20Perspectives%20of%20Robotics%20and%20AI%202018-01) [GScholar](https://scholar.google.co.uk/scholar?q=Torresen%2C%20Jim%20A%20Review%20of%20Future%20and%20Ethical%20Perspectives%20of%20Robotics%20and%20AI%202018-01) [Scite](https://engine.scholarcy.com/scite_url?query=Torresen%2C%20Jim%20A%20Review%20of%20Future%20and%20Ethical%20Perspectives%20of%20Robotics%20and%20AI%202018-01)

[^Trentelman_2009_a]: Trentelman, Kerry, Survey of Knowledge Representation and Reasoning Systems, Defence Science and Technology Organisation, Edinburgh, S. Aust., 2009. https://apps.dtic.mil/dtic/tr/fulltext/u2/a508761.pdf [[Trentelman_SurveyKnowledgeRepresentationReasoningSystems_2009]] [OA](https://apps.dtic.mil/dtic/tr/fulltext/u2/a508761.pdf)  [Scite](https://engine.scholarcy.com/scite_url?query=Trentelman%2C%20Kerry%20Survey%20of%20Knowledge%20Representation%20and%20Reasoning%20Systems%2C%20Defence%20Science%20and%20Technology%20Organisation%2C%20Edinburgh%2C%20S%202009)

[^Trentesaux_2017_a]: Trentesaux, Damien and Raphael Rault, “Designing Ethical Cyber-Physical Industrial Systems,” IFAC PapersOnLine, Vol. 50, No. 1, 2017. [[Trentesaux_DesigningEthicalCyberphysicalIndustrialSystems_2017]] [OA](https://engine.scholarcy.com/oa_version?query=Trentesaux%2C%20Damien%20Rault%2C%20Raphael%20Designing%20Ethical%20Cyber-Physical%20Industrial%20Systems%2C%202017) [GScholar](https://scholar.google.co.uk/scholar?q=Trentesaux%2C%20Damien%20Rault%2C%20Raphael%20Designing%20Ethical%20Cyber-Physical%20Industrial%20Systems%2C%202017) [Scite](https://engine.scholarcy.com/scite_url?query=Trentesaux%2C%20Damien%20Rault%2C%20Raphael%20Designing%20Ethical%20Cyber-Physical%20Industrial%20Systems%2C%202017)

[^Trewin_2018_a]: Trewin, S., “AI Fairness for People with Disabilities: Point of View,” arXiv preprint arXiv:1811.10670, 2018. [[Trewin_AiFairnessPeopleWithDisabilities_2018]] [OA](https://export.arxiv.org/pdf/1811.10670)  

[^Trimmer_et+al_2015_a]: Trimmer, Jelte, Pel, Bonno & Kool, Linda, et al., “5.3 Big Data”, Converging Roads: Linking Self-Driving Cars to Public Goals, February 2015. [[Trimmer_et+al_53BigDataConvergingRoads_2015]] [OA](https://scholar.google.co.uk/scholar?q=Trimmer%2C%20Jelte%20Pel%2C%20Bonno%20Kool%2C%20Linda%20%E2%80%9C5.3%20Big%20Data%E2%80%9D%2C%20Converging%20Roads%3A%20Linking%20Self-Driving%20Cars%20to%20Public%20Goals%202015-02) [GScholar](https://scholar.google.co.uk/scholar?q=Trimmer%2C%20Jelte%20Pel%2C%20Bonno%20Kool%2C%20Linda%20%E2%80%9C5.3%20Big%20Data%E2%80%9D%2C%20Converging%20Roads%3A%20Linking%20Self-Driving%20Cars%20to%20Public%20Goals%202015-02) 

[^Turkle_2007_a]: Turkle, Sherry, “Authenticity in the age of digital companions,” Interaction Studies, Vol. 8, No. 3, 2007, pp. 501–517. [[Turkle_AuthenticityDigitalCompanions_2007]] [OA](https://engine.scholarcy.com/oa_version?query=Turkle%2C%20Sherry%20Authenticity%20in%20the%20age%20of%20digital%20companions%2C%202007) [GScholar](https://scholar.google.co.uk/scholar?q=Turkle%2C%20Sherry%20Authenticity%20in%20the%20age%20of%20digital%20companions%2C%202007) [Scite](https://engine.scholarcy.com/scite_url?query=Turkle%2C%20Sherry%20Authenticity%20in%20the%20age%20of%20digital%20companions%2C%202007)

[^Turkle_2011_a]: Turkle, Sherry, Alone Together: Why We Expect More from Technology and Less from Each Other, Basic Books, 2011. [[Turkle_AloneTogetherWeExpectMore_2011]] [OA](https://scholar.google.co.uk/scholar?q=Turkle%2C%20Sherry%20Alone%20Together%3A%20Why%20We%20Expect%20More%20from%20Technology%20and%20Less%20from%20Each%202011) [GScholar](https://scholar.google.co.uk/scholar?q=Turkle%2C%20Sherry%20Alone%20Together%3A%20Why%20We%20Expect%20More%20from%20Technology%20and%20Less%20from%20Each%202011) 

[^Tutt_2016_a]: Tutt, Andrew, “An FDA for Algorithms,” SSRN Electronic Journal, 2016. [[Tutt_FdaAlgorithms_2016]] [OA](https://engine.scholarcy.com/oa_version?query=Tutt%2C%20Andrew%20An%20FDA%20for%20Algorithms%2C%202016) [GScholar](https://scholar.google.co.uk/scholar?q=Tutt%2C%20Andrew%20An%20FDA%20for%20Algorithms%2C%202016) [Scite](https://engine.scholarcy.com/scite_url?query=Tutt%2C%20Andrew%20An%20FDA%20for%20Algorithms%2C%202016)

[^Tzafestas_2018_a]: Tzafestas, Spyros, "Ethics and law in the internet of things world," Smart Cities, Vol. 1, no. 1, 2018, pp. 98-120., pp. 112-115 UCLA Smueli Newsroom, “UCLA Bioengineering Leads Development of Stingray-Inspired Soft Biobot”, January 2018. UK Government, “A guide to using artificial intelligence in the public sector”, 10 June 2019. [[Tzafestas_EthicsInternetThingsWorld_2018]] [OA](https://engine.scholarcy.com/oa_version?query=Tzafestas%2C%20Spyros%20Ethics%20and%20law%20in%20the%20internet%20of%20things%20world%2C%202018-01-10) [GScholar](https://scholar.google.co.uk/scholar?q=Tzafestas%2C%20Spyros%20Ethics%20and%20law%20in%20the%20internet%20of%20things%20world%2C%202018-01-10) [Scite](https://engine.scholarcy.com/scite_url?query=Tzafestas%2C%20Spyros%20Ethics%20and%20law%20in%20the%20internet%20of%20things%20world%2C%202018-01-10)

[^https://www.gov.uk/government/collections/a-guide-to-using-artificial-intelligence-in-the-publicsector_0000_b]: https://www.gov.uk/government/collections/a-guide-to-using-artificial-intelligence-in-the-publicsector UN General Assembly, "Universal declaration of human rights," UN General Assembly 302.2, 10 [[https://www.gov.uk/government/collections/a-guide-to-using-artificial-intelligence-in-the-publicsector_GeneralAssemblyuniversalDeclarationHuman_0000]] [OA](https://www.gov.uk/government/collections/a-guide-to-using-artificial-intelligence-in-the-publicsector)  [Scite](https://engine.scholarcy.com/scite_url?query=httpswwwgovukgovernmentcollectionsaguidetousingartificialintelligenceinthepublicsector%20UN%20General%20Assembly%20Universal%20declaration%20of%20human%20rights%20UN%20General%20Assembly%203022%2010)

[^U_2017_a]: UNESCO, “Section 3.4.4 Companion Robots”, Report of Comest on Robotics Ethics, September 2017. UNIDIR, “The Weaponization of Increasingly Autonomous Technologies: Autonomous Weapon Systems and Cyber Operations”, UNIDIR Resources, No. 7, 2017, p. 2. UNIDIR, op. cit., 2017 and UNIDIR, “The Weaponization of Increasingly Autonomous Technologies: Artificial Intelligence”, UNIDIR Resources, No. 8, 2018. Union of Concerned Scientists, “Industrial Agriculture” https://www.ucsusa.org/our-work/foodagriculture/our-failing-food-system/industrial-agriculture” Urbi, J., ‘Some Transgender Drivers Are Being Kicked Off Uber’s App’, CNBC, August 13, 2018.https://www.cnbc.com/2018/08/08/transgender-uber-driver-suspended-tech-oversight-facialrecognition.html [[U_section344CompanionRobotsReport_2017]] [OA](https://www.ucsusa.org/our-work/foodagriculture/our-failing-food-system/industrial-agriculture”)  

[^Uskov_2015_a]: Uskov, Vladimir L., Robert J. Howlett, and Lakhmi C. Jain, (eds.), Smart education and smart elearning. Vol. 41, Springer, 2015.; Utermohlen, “4 Ways AI Is Changing the Education Industry,” Medium, Towards Data Science, April 12, 2018. https://towardsdatascience.com/4ways-ai-is-changing-the-education-industry-b473c5d2c706 Van den [[Uskov_Utermohlen4WaysAiIs_2015]] [OA](https://towardsdatascience.com/4ways-ai-is-changing-the-education-industry-b473c5d2c706)  [Scite](https://engine.scholarcy.com/scite_url?query=Uskov%20Vladimir%20L%20Robert%20J%20Howlett%20and%20Lakhmi%20C%20Jain%20eds%20Smart%20education%20and%20smart%20elearning%20Vol%2041%20Springer%202015%20Utermohlen%204%20Ways%20AI%20Is%20Changing%20the%20Education%20Industry%20Medium%20Towards%20Data%20Science%20April%2012%202018%20httpstowardsdatasciencecom4waysaiischangingtheeducationindustryb473c5d2c706%20Van%20den)

[^Hoven_2008_a]: Hoven, J., & Rooksby, E. (2008). Distributive Justice and the Value of Information: A (Broadly) Rawlsian Approach. In J. van den Hoven, & J. Weckert (eds.), Information Technology and Moral Philosophy, Cambridge: Cambridge University Press, 376-396. Van den Hoven, J., Vermaas, P. & Van de Poel, I. (Eds.), Handbook of Ethics, Values, and Technological Design. Sources, Theory, Values and Application Domains. Dordrecht: Springer. Van Wynsberghe, Aimee & Donhauser, Justin, “The Dawning of the Ethics of Environmental Robots”, Science and Engineering Ethics 24(6), December 2018. Van Wynsberghe, Aimee, “Service Robots, Care Ethics, and Design”, Ethics and Information Technology 18(4), December 2016. Van Wynsberghe, Aimee, Healthcare Robots, 2015. Van Zoonen, Liesbet, "Privacy concerns in smart cities," Government Information Quarterly, Vol. 33, No. 3, 2016, pp. 472-480. [[Hoven_DistributiveJusticeValueInformationbroadly_2008]] [OA](https://engine.scholarcy.com/oa_version?query=Hoven%2C%20J.%20Rooksby%2C%20E.%20Distributive%20Justice%20and%20the%20Value%20of%20Information%3A%20A%20%28Broadly%29%20Rawlsian%20Approach%202008-12) [GScholar](https://scholar.google.co.uk/scholar?q=Hoven%2C%20J.%20Rooksby%2C%20E.%20Distributive%20Justice%20and%20the%20Value%20of%20Information%3A%20A%20%28Broadly%29%20Rawlsian%20Approach%202008-12) [Scite](https://engine.scholarcy.com/scite_url?query=Hoven%2C%20J.%20Rooksby%2C%20E.%20Distributive%20Justice%20and%20the%20Value%20of%20Information%3A%20A%20%28Broadly%29%20Rawlsian%20Approach%202008-12)

[^Vanderelst_2016_a]: Vanderelst, Dieter, and Alan Winfield, “The Dark Side of Ethical Robots,” Cornell University arXiv.org, 2016. https://arxiv.org/abs/1606.02583 Vardi, Moshe, “What the Industrial Revolution Really Tells Us About the Future of Automation and Work”, The Conversation, September 2017. [[Vanderelst_DarkSideEthicalRobots_2016]] [OA](https://arxiv.org/abs/1606.02583)  

[^Vassallo_et+al_2015_a]: Vassallo, T., E. Levy, M. Madansky, H. Mickell, B. Porter, M. Leas, and J. Oberweis, Elephant in the Valley, 2015. Retrieved at https://www.elephantinthevalley.com/. [[Vassallo_et+al_ElephantValley2015_2015]] [OA](https://www.elephantinthevalley.com/)  

[^Vayena_et+al_2015_a]: Vayena, Effy, et al., “Ethical Challenges of Big Data in Public Health”, PLoS Comput Biol., Vol. 11, No. 2, 2015. [[Vayena_et+al_EthicalChallengesBigDataPublic_2015]] [OA](https://engine.scholarcy.com/oa_version?query=Vayena%2C%20Effy%20Ethical%20Challenges%20of%20Big%20Data%20in%20Public%20Health%202015) [GScholar](https://scholar.google.co.uk/scholar?q=Vayena%2C%20Effy%20Ethical%20Challenges%20of%20Big%20Data%20in%20Public%20Health%202015) [Scite](https://engine.scholarcy.com/scite_url?query=Vayena%2C%20Effy%20Ethical%20Challenges%20of%20Big%20Data%20in%20Public%20Health%202015)

[^Veale_2017_a]: Veale, M., and Binns, R., ‘Fairer Machine Learning in the Real World: Mitigating Discrimination Without Collecting Sensitive Data’, Big Data & Society, Vol. 4, No. 2, July-December 2017. [[Veale_FairerMachineLearningRealWorld_2017]] [OA](https://engine.scholarcy.com/oa_version?query=Veale%2C%20M.%20Binns%2C%20R.%20Fairer%20Machine%20Learning%20in%20the%20Real%20World%3A%20Mitigating%20Discrimination%20Without%20Collecting%20Sensitive%20Data%202017-07) [GScholar](https://scholar.google.co.uk/scholar?q=Veale%2C%20M.%20Binns%2C%20R.%20Fairer%20Machine%20Learning%20in%20the%20Real%20World%3A%20Mitigating%20Discrimination%20Without%20Collecting%20Sensitive%20Data%202017-07) [Scite](https://engine.scholarcy.com/scite_url?query=Veale%2C%20M.%20Binns%2C%20R.%20Fairer%20Machine%20Learning%20in%20the%20Real%20World%3A%20Mitigating%20Discrimination%20Without%20Collecting%20Sensitive%20Data%202017-07)

[^Vega_2018_a]: Vega, Julio & Canas, Jose, “PiBot: An Open Low-Cost Robotic Platform with Camera for STEM Education”, Electronics, December 2018. [[Vega_PibotOpenLowcostRoboticPlatform_2018]] [OA](https://engine.scholarcy.com/oa_version?query=Vega%2C%20Julio%20Canas%2C%20Jose%20PiBot%3A%20An%20Open%20Low-Cost%20Robotic%20Platform%20with%20Camera%20for%20STEM%20Education%202018-12) [GScholar](https://scholar.google.co.uk/scholar?q=Vega%2C%20Julio%20Canas%2C%20Jose%20PiBot%3A%20An%20Open%20Low-Cost%20Robotic%20Platform%20with%20Camera%20for%20STEM%20Education%202018-12) [Scite](https://engine.scholarcy.com/scite_url?query=Vega%2C%20Julio%20Canas%2C%20Jose%20PiBot%3A%20An%20Open%20Low-Cost%20Robotic%20Platform%20with%20Camera%20for%20STEM%20Education%202018-12)

[^Maplecroft_2018_a]: Verisk Maplecroft, Human Rights Outlook 2018, July 2018. [[Maplecroft_HumanRightsOutlook_2018]] [OA](https://scholar.google.co.uk/scholar?q=Maplecroft%2C%20Verisk%20Human%20Rights%20Outlook%202018-07) [GScholar](https://scholar.google.co.uk/scholar?q=Maplecroft%2C%20Verisk%20Human%20Rights%20Outlook%202018-07) 

[^Veruggio_2008_a]: Veruggio, Gianmarco & Operto, Fiorella, “Roboethics: Social and Ethical Implications of Robotics”, Springer Handbook of Robotics, 2008. [[Veruggio_RoboethicsSocialEthicalImplicationsRobotics_2008]] [OA](https://scholar.google.co.uk/scholar?q=Veruggio%2C%20Gianmarco%20Operto%2C%20Fiorella%20Roboethics%3A%20Social%20and%20Ethical%20Implications%20of%20Robotics%202008) [GScholar](https://scholar.google.co.uk/scholar?q=Veruggio%2C%20Gianmarco%20Operto%2C%20Fiorella%20Roboethics%3A%20Social%20and%20Ethical%20Implications%20of%20Robotics%202008) 

[^Report:_2018_a]: Report: But the Impact Will Still be Significant, Increasing Societal Division Between the Rich and the Poor”, The Verge, April 2018. [[Report:_ReportImpactWillStillSignificant_2018]] [OA](https://engine.scholarcy.com/oa_version?query=Report%20But%20the%20Impact%20Will%20Still%20be%20Significant%20Increasing%20Societal%20Division%20Between%20the%20Rich%20and%20the%20Poor%20The%20Verge%20April%202018) [GScholar](https://scholar.google.co.uk/scholar?q=Report%20But%20the%20Impact%20Will%20Still%20be%20Significant%20Increasing%20Societal%20Division%20Between%20the%20Rich%20and%20the%20Poor%20The%20Verge%20April%202018) [Scite](https://engine.scholarcy.com/scite_url?query=Report%20But%20the%20Impact%20Will%20Still%20be%20Significant%20Increasing%20Societal%20Division%20Between%20the%20Rich%20and%20the%20Poor%20The%20Verge%20April%202018)

[^Vincent_2016_a]: Vincent, James, “First Click: Robots will make it even harder for poor countries to get rich,” The Verge, March 10, 2016. https://www.theverge.com/2016/5/10/11648062/first-click-robots-willmake-it-even-harder-for-poor-countries-to-get Vincent, James, “Hollywood Is Quietly Using AI to Help Decide Which Movies to Make,” The Verge, May 28, 2019.https://www.theverge.com/2019/5/28/18637135/hollywood-ai-film-decisionscript-analysis-data-machine-learning Vincent, James, “Robots and AI are going to make social inequality even worse, says new report,” The Verge, July 13, 2017.https://www.theverge.com/2017/7/13/15963710/robots-ai-inequalitysocial-mobility-study [[Vincent_FirstClickRobotsWillMake_2016]] [OA](https://www.theverge.com/2016/5/10/11648062/first-click-robots-willmake-it-even-harder-for-poor-countries-to-get)  [Scite](https://engine.scholarcy.com/scite_url?query=Vincent%2C%20James%20First%20Click%3A%20Robots%20will%20make%20it%20even%20harder%20for%20poor%20countries%20to%20get%20rich%2C%202016-03-10)

[^Vollmer_et+al_2018_a]: Vollmer, A., R. Read, D. Trippas, and T. Belpaeme, “Children conform, adults resist: A robot group induced peer pressure on normative social conformity,” Science Robotics, Vol. 3, No. 2, 2018. [[Vollmer_et+al_ChildrenConformAdultsResistRobot_2018]] [OA](https://engine.scholarcy.com/oa_version?query=Vollmer%2C%20A.%20Read%2C%20R.%20Trippas%2C%20D.%20Belpaeme%2C%20T.%20Children%20conform%2C%20adults%20resist%3A%20A%20robot%20group%20induced%20peer%20pressure%20on%20normative%20social%20conformity%2C%202018) [GScholar](https://scholar.google.co.uk/scholar?q=Vollmer%2C%20A.%20Read%2C%20R.%20Trippas%2C%20D.%20Belpaeme%2C%20T.%20Children%20conform%2C%20adults%20resist%3A%20A%20robot%20group%20induced%20peer%20pressure%20on%20normative%20social%20conformity%2C%202018) [Scite](https://engine.scholarcy.com/scite_url?query=Vollmer%2C%20A.%20Read%2C%20R.%20Trippas%2C%20D.%20Belpaeme%2C%20T.%20Children%20conform%2C%20adults%20resist%3A%20A%20robot%20group%20induced%20peer%20pressure%20on%20normative%20social%20conformity%2C%202018)

[^Wachsmuth_2018_a]: Wachsmuth, I., “Robots Like Me: Challenges and Ethical Issues in Aged Care,” Frontiers in Psychology, Vol. 9, 2018. doi: 10.3389/fpsyg.2018.00432 [[Wachsmuth_RobotsLikeMeChallengesEthical_2018]] [OA](https://doi.org/10.3389/fpsyg.2018.00432)  [Scite](https://scite.ai/reports/10.3389/fpsyg.2018.00432)

[^Wade_2018_a]: Wade, Michael, “Psychographics: the Behavioural Analysis That Helped Cambridge Analytica Know Voters' Minds,” The Conversation, March 21, 2018. http://theconversation.com/psychographicsthe-behavioural-analysis-that-helped-cambridge-analytica-know-voters-minds-93675. [[Wade_PsychographicsBehaviouralAnalysisThatHelped_2018]] [OA](http://theconversation.com/psychographicsthe-behavioural-analysis-that-helped-cambridge-analytica-know-voters-minds-93675)  [Scite](https://engine.scholarcy.com/scite_url?query=Wade%2C%20Michael%20Psychographics%3A%20the%20Behavioural%20Analysis%20That%20Helped%20Cambridge%20Analytica%20Know%20Voters%202018-03-21)

[^Wagner_et+al_2018_a]: Wagner, A. R., Borenstein, J., and Howard, A., ‘Overtrust in the Robotic Age’, Communications of the ACM, Vol. 61, No. 9, pp. 22-24, 2018. Wall-ye, “MYCE_Vigne”, accessed December 2018. wall-ye.com/index-2.html Wallach and Allen, “Android Ethics: Bottom-up and Top-down Approaches for Modeling Human Moral Faculties”, 149. [[Wagner_et+al_OvertrustRoboticAge_2018]] [OA](https://engine.scholarcy.com/oa_version?query=Wagner%2C%20A.R.%20Borenstein%2C%20J.%20Howard%2C%20A.%20Overtrust%20in%20the%20Robotic%20Age%202018) [GScholar](https://scholar.google.co.uk/scholar?q=Wagner%2C%20A.R.%20Borenstein%2C%20J.%20Howard%2C%20A.%20Overtrust%20in%20the%20Robotic%20Age%202018) [Scite](https://engine.scholarcy.com/scite_url?query=Wagner%2C%20A.R.%20Borenstein%2C%20J.%20Howard%2C%20A.%20Overtrust%20in%20the%20Robotic%20Age%202018)

[^Wamsley_2018_a]: Wamsley, Laurel, “Should Self-Driving Cars Have Ethics?”, Technology, October 2018. Webster-Wood, Victoria, “Biohybrid Robots Built from Living Tissue Start to Take Shape”, The Conversation, August 2016. Webster-Wood, Victoria, Akkus, Ozan & Gurkan, Umut et al. “Organismal Engineering: Toward a Robotic Taxonomic Key for Devices Using Organic Materials”, Science Robotics Review, November 2017. Weiser as cited in Spiekermann, Sarah, and Frank Pallas, "Technology paternalism–wider implications of ubiquitous computing," Poiesis & praxis, Vol. 4, No. 1, 2006, pp. 6-18., p. 7 Wellcome Trust and Future Advocacy, “Ethical, Social, and Political Challenges of Artificial Intelligence in Health”, Wellcome Trust, April 2018, pp. 12–13. [[Wamsley_ShouldSelfdrivingCarsHaveEthics_2018]] [OA](https://engine.scholarcy.com/oa_version?query=Wamsley%2C%20Laurel%20Should%20Self-Driving%20Cars%20Have%20Ethics%3F%202018-04-07) [GScholar](https://scholar.google.co.uk/scholar?q=Wamsley%2C%20Laurel%20Should%20Self-Driving%20Cars%20Have%20Ethics%3F%202018-04-07) [Scite](https://engine.scholarcy.com/scite_url?query=Wamsley%2C%20Laurel%20Should%20Self-Driving%20Cars%20Have%20Ethics%3F%202018-04-07)

[^West_2015_a]: West, Darrell M., “What Happens if Robots Take the Jobs? The Impact of Emerging Technologies on Employment and Public Policy”, Centre for Technology Innovation at Brookings, October 2015. [[West_WhatHappensRobotsTakeJobs_2015]] [OA](https://scholar.google.co.uk/scholar?q=West%2C%20Darrell%20M.%20What%20Happens%20if%20Robots%20Take%20the%20Jobs%3F%20The%20Impact%20of%20Emerging%20Technologies%20on%20Employment%20and%20Public%20Policy%202015-10) [GScholar](https://scholar.google.co.uk/scholar?q=West%2C%20Darrell%20M.%20What%20Happens%20if%20Robots%20Take%20the%20Jobs%3F%20The%20Impact%20of%20Emerging%20Technologies%20on%20Employment%20and%20Public%20Policy%202015-10) 

[^West_et+al_2019_a]: West, S. M., Whittaker, M., and Crawford, K., ‘Discriminating Systems: Gender, Race, and Power in AI’, 2019, p. 5. https://ainowinstitute.org/discriminatingsystems.html. [[West_et+al_DiscriminatingSystemsGenderRacePower_2019]] [OA](https://ainowinstitute.org/discriminatingsystems.html)  

[^Williams_et+al_0000_a]: Williams, Randi, Christian Vázquez Machado, Stefania Druga, Cynthia Breazeal, and Pattie Maes, “"My doll says it's ok": a study of children's conformity to a talking doll.” In Proceedings of the [[Williams_et+al_myDollSays_0000]] [OA](https://engine.scholarcy.com/oa_version?query=Williams%2C%20Randi%20Machado%2C%20Christian%20V%C3%A1zquez%20Druga%2C%20Stefania%20Breazeal%2C%20Cynthia%20%22My%20doll%20says%20it%27s%20ok%22%3A%20a%20study%20of%20children%27s%20conformity%20to%20a%20talking%20doll.) [GScholar](https://scholar.google.co.uk/scholar?q=Williams%2C%20Randi%20Machado%2C%20Christian%20V%C3%A1zquez%20Druga%2C%20Stefania%20Breazeal%2C%20Cynthia%20%22My%20doll%20says%20it%27s%20ok%22%3A%20a%20study%20of%20children%27s%20conformity%20to%20a%20talking%20doll.) [Scite](https://engine.scholarcy.com/scite_url?query=Williams%2C%20Randi%20Machado%2C%20Christian%20V%C3%A1zquez%20Druga%2C%20Stefania%20Breazeal%2C%20Cynthia%20%22My%20doll%20says%20it%27s%20ok%22%3A%20a%20study%20of%20children%27s%20conformity%20to%20a%20talking%20doll.)

[^Wilson_6256_a]: 17th ACM Conference on Interaction Design and Children (IDC '18). ACM, New York, NY, USA, 625631. Wilson, Dennis G, “The Ethics of Automated Behavioral Microtargeting,” AI Matters, Vol. 3, No. 3, 2017, pp. 56-64.; Jacobson, Jenna, Anatoliy Gruzd, and Ángel Hernández-García, “Social media marketing: Who is watching the watchers?,” Journal of Retailing and Consumer Services, available online March 20, 2019, in press. https://doi.org/10.1016/j.jretconser.2019.03.001. Wilson, Richard L., “Ethical Issues with Use of Drone Aircraft”, IEEE International Symposium on Ethics in Science, Technology and Engineering, May 2014. [[Wilson_AcmConferenceInteractionDesignChildren_6256]] [OA](https://doi.org/10.1016/j.jretconser.2019.03.001)  [Scite](https://scite.ai/reports/10.1016/j.jretconser.2019.03.001)

[^Wiseman_2016_a]: Wiseman, Sam, and Alexander M. Rush, “Sequence-to-Sequence Learning as Beam-Search Optimization,” Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, 2016. [[Wiseman_SequencesequenceLearningBeamsearchOptimization_2016]] [OA](https://engine.scholarcy.com/oa_version?query=Wiseman%2C%20Sam%20Rush%2C%20Alexander%20M.%20Sequence-to-Sequence%20Learning%20as%20Beam-Search%20Optimization%2C%202016) [GScholar](https://scholar.google.co.uk/scholar?q=Wiseman%2C%20Sam%20Rush%2C%20Alexander%20M.%20Sequence-to-Sequence%20Learning%20as%20Beam-Search%20Optimization%2C%202016) [Scite](https://engine.scholarcy.com/scite_url?query=Wiseman%2C%20Sam%20Rush%2C%20Alexander%20M.%20Sequence-to-Sequence%20Learning%20as%20Beam-Search%20Optimization%2C%202016)

[^Wolf_et+al_2017_a]: Wolf, M. J., Miller, K., and Grodzinsky, F. S., ‘Why We Should Have Seen That Coming: Comments on Microsoft’s Tay “Experiment”, and Wider Implications”, ACM SIGCAS Computers and Society, Vol. 47, No. 3, pp. 54-64, September 2017. World Economic Forum, The Global Gender Gap Report 2018. Retrieved at http://www3.weforum.org/docs/WEF_GGGR_2018.pdf. [[Wolf_et+al_WeShouldHaveSeenThat_2017]] [OA](http://www3.weforum.org/docs/WEF_GGGR_2018.pdf)  [Scite](https://engine.scholarcy.com/scite_url?query=Wolf%2C%20M.J.%20Miller%2C%20K.%20Grodzinsky%2C%20F.S.%20Why%20We%20Should%20Have%20Seen%20That%20Coming%3A%20Comments%20on%20Microsoft%E2%80%99s%20Tay%20%E2%80%9CExperiment%E2%80%9D%2C%20and%20Wider%20Implications%202017-09)

[^Forum_2018_a]: World Economic Forum, Towards a Reskilling Revolution. A Future of Jobs for All, 2018b, http://www3.weforum.org/docs/WEF_FOW_Reskilling_Revolution.pdf. [[Forum_TowardsReskillingRevolutionAFuture_2018]] [OA](http://www3.weforum.org/docs/WEF_FOW_Reskilling_Revolution.pdf)  

[^Worms_2006_a]: Worms, Frédéric, “The Two Concepts of Care. Life, Medicine, and Moral Relations.” Esprit, No. 1, Jan 2006, pp. 141-156. Wright Scott A., and Schultz, Ainslie E., “The Rising Tide of Artificial Intelligence and Business Automation: Developing an Ethical Framework,” Business Horizons, Vol. 61, 2018, p. 824. [[Worms_TwoConceptsCareLifeMedicine_2006]] [OA](https://engine.scholarcy.com/oa_version?query=Worms%2C%20Fr%C3%A9d%C3%A9ric%20The%20Two%20Concepts%20of%20Care.%20Life%2C%20Medicine%2C%20and%20Moral%20Relations.%202006-01) [GScholar](https://scholar.google.co.uk/scholar?q=Worms%2C%20Fr%C3%A9d%C3%A9ric%20The%20Two%20Concepts%20of%20Care.%20Life%2C%20Medicine%2C%20and%20Moral%20Relations.%202006-01) [Scite](https://engine.scholarcy.com/scite_url?query=Worms%2C%20Fr%C3%A9d%C3%A9ric%20The%20Two%20Concepts%20of%20Care.%20Life%2C%20Medicine%2C%20and%20Moral%20Relations.%202006-01)

[^Zerilli_et+al_2018_a]: Zerilli, J., Knott, A., Maclaurin, J., and Gavaghan, C., ‘Transparency in Algorithmic and Human Decision-Making: Is There a Double Standard?’, Philosophy & Technology, 2018, https://doi.org/10.1007/s13347-018-0330-6. [[Zerilli_et+al_TransparencyAlgorithmicHumanDecisionmakingThere_2018]] [OA](https://doi.org/10.1007/s13347-018-0330-6)  [Scite](https://scite.ai/reports/10.1007/s13347-018-0330-6)

[^Zerilli_2019_a]: Zerilli, John and Gavaghan, Colin, “Call for Independent Watchdog to Monitor NZ Government Use of Artificial Intelligence,” The Conversation, 27 May 2019. [[Zerilli_CallIndependentWatchdogMonitorNz_2019]] [OA](https://engine.scholarcy.com/oa_version?query=Zerilli%2C%20John%20Gavaghan%2C%20Colin%20Call%20for%20Independent%20Watchdog%20to%20Monitor%20NZ%20Government%20Use%20of%20Artificial%20Intelligence%2C%202019-05-27) [GScholar](https://scholar.google.co.uk/scholar?q=Zerilli%2C%20John%20Gavaghan%2C%20Colin%20Call%20for%20Independent%20Watchdog%20to%20Monitor%20NZ%20Government%20Use%20of%20Artificial%20Intelligence%2C%202019-05-27) [Scite](https://engine.scholarcy.com/scite_url?query=Zerilli%2C%20John%20Gavaghan%2C%20Colin%20Call%20for%20Independent%20Watchdog%20to%20Monitor%20NZ%20Government%20Use%20of%20Artificial%20Intelligence%2C%202019-05-27)

[^Zerilli_et+al_2018_b]: Zerilli, John, Alistair Knott, James Maclaurin, and Colin Gavaghan, "Transparency in Algorithmic and Human Decision-Making: Is There a Double Standard?," Philosophy & Technology, 2018, pp. 1-23. [[Zerilli_et+al_TransparencyAlgorithmicHumanDecisionmakingThere_2018]] [OA](https://engine.scholarcy.com/oa_version?query=Zerilli%2C%20John%20Knott%2C%20Alistair%20Maclaurin%2C%20James%20Gavaghan%2C%20Colin%20Transparency%20in%20Algorithmic%20and%20Human%20Decision-Making%3A%20Is%20There%20a%20Double%20Standard%3F%2C%202018) [GScholar](https://scholar.google.co.uk/scholar?q=Zerilli%2C%20John%20Knott%2C%20Alistair%20Maclaurin%2C%20James%20Gavaghan%2C%20Colin%20Transparency%20in%20Algorithmic%20and%20Human%20Decision-Making%3A%20Is%20There%20a%20Double%20Standard%3F%2C%202018) [Scite](https://engine.scholarcy.com/scite_url?query=Zerilli%2C%20John%20Knott%2C%20Alistair%20Maclaurin%2C%20James%20Gavaghan%2C%20Colin%20Transparency%20in%20Algorithmic%20and%20Human%20Decision-Making%3A%20Is%20There%20a%20Double%20Standard%3F%2C%202018)

[^Zhang_et+al_2018_a]: Zhang, Chuang, Wang, Wenxue & Xi, Ning, et al., “Development and Future Challenges of BioSyncretic Robots”, Research Robotics Review, August 2018. [[Zhang_et+al_DevelopmentFutureChallengesBiosyncreticRobots_2018]] [OA](https://engine.scholarcy.com/oa_version?query=Zhang%2C%20Chuang%20Wang%2C%20Wenxue%20Xi%2C%20Ning%20Development%20and%20Future%20Challenges%20of%20BioSyncretic%20Robots%202018-08) [GScholar](https://scholar.google.co.uk/scholar?q=Zhang%2C%20Chuang%20Wang%2C%20Wenxue%20Xi%2C%20Ning%20Development%20and%20Future%20Challenges%20of%20BioSyncretic%20Robots%202018-08) [Scite](https://engine.scholarcy.com/scite_url?query=Zhang%2C%20Chuang%20Wang%2C%20Wenxue%20Xi%2C%20Ning%20Development%20and%20Future%20Challenges%20of%20BioSyncretic%20Robots%202018-08)

[^Zheng_et+al_2019_a]: Zheng, Wujie, Wenyu Wang, Dian Liu, Changrong Zhang, Qinsong Zeng, Yuetang Deng, Wei Yang, Pinjia He, and Tao Xie, "Testing untestable neural machine translation: An industrial case," Proc. 41st International Conference on Software Engineering: Companion, Poster, 2019. [[Zheng_et+al_TestingUntestableNeuralMachineTranslation_2019]] [OA](https://engine.scholarcy.com/oa_version?query=Zheng%2C%20Wujie%20Wang%2C%20Wenyu%20Liu%2C%20Dian%20Zhang%2C%20Changrong%20Testing%20untestable%20neural%20machine%20translation%3A%20An%20industrial%20case%2C%202019) [GScholar](https://scholar.google.co.uk/scholar?q=Zheng%2C%20Wujie%20Wang%2C%20Wenyu%20Liu%2C%20Dian%20Zhang%2C%20Changrong%20Testing%20untestable%20neural%20machine%20translation%3A%20An%20industrial%20case%2C%202019) [Scite](https://engine.scholarcy.com/scite_url?query=Zheng%2C%20Wujie%20Wang%2C%20Wenyu%20Liu%2C%20Dian%20Zhang%2C%20Changrong%20Testing%20untestable%20neural%20machine%20translation%3A%20An%20industrial%20case%2C%202019)

